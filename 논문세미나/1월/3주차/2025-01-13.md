### 🚀 논문 요약: Deep Residual Learning for Image Recognition

---

### 핵심 내용

1. **문제 정의**
    
    - **깊은 신경망의 최적화 문제**:
        - 네트워크의 깊이가 증가할수록 정확도가 포화되거나 퇴화(Degradation)하는 현상 발생.
        - 이는 과적합 문제가 아닌, 최적화 과정에서 발생하는 어려움임.
2. **해결책: Residual Learning**
    
    - 네트워크의 출력 H(x)H(x)H(x)를 직접 학습하는 대신, 잔차 함수 F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x를 학습.
    - Shortcut Connection(바이패스 연결)을 사용하여 F(x)+xF(x) + xF(x)+x로 출력 계산.
    - **장점**:
        - 추가적인 매개변수 없이 최적화 문제 해결.
        - 매우 깊은 네트워크에서도 학습 및 성능 향상 가능.

---

### 실험 결과

#### 1. **ImageNet 결과**

- 최대 152층 깊이의 ResNet이 학습됨.
    
- VGG 네트워크보다 **복잡도는 낮지만 정확도는 높음**.
    
- **Top-5 오류율 비교** (단일 모델 기준):
-

| 모델         | Top-5 오류율 (%) |
|--------------|-----------------|
| VGG-16       | 9.33            |
| GoogLeNet    | 7.89            |
| ResNet-50    | 6.71            |
| ResNet-101   | 6.05            |
| ResNet-152   | 5.71            |

    
- **앙상블 성능**:
    
    - ResNet 앙상블은 ImageNet 테스트 데이터에서 **3.57%의 오류율**로 ILSVRC 2015 분류 과제에서 1위를 차지.

#### 2. **CIFAR-10 결과**

- 다양한 깊이의 네트워크(20, 32, 44, 56, 110층)에서 실험.
    
- 잔차 학습을 통해 깊이가 깊어질수록 테스트 오류율이 감소.
    
- **오류율 비교**:

| 모델        | 깊이 | 매개변수 수 | 테스트 오류율 (%) |
|-------------|------|------------|------------------|
| FitNet      | 19   | 2.5M       | 8.39             |
| Highway     | 32   | 1.25M      | 8.80             |
| ResNet-56   | 56   | 0.85M      | 6.97             |
| ResNet-110  | 110  | 1.7M       | 6.43             |

    
- **그래프**:
    
    - **Plain 네트워크**와 **Residual 네트워크** 비교:
        
        - 왼쪽: Plain 네트워크는 깊이가 증가할수록 최적화 실패.
        - 오른쪽: ResNet은 깊어질수록 학습 및 테스트 오류가 감소.

---

### 주요 분석

1. **ResNet의 효과**
    
    - 잔차 학습을 통해 깊은 네트워크에서도 안정적으로 학습 가능.
    - CIFAR-10과 ImageNet 모두에서 더 깊은 모델이 더 높은 성능을 발휘.
2. **잔차 함수와 출력**
    
    - 잔차 함수의 출력 크기가 일반적인 네트워크보다 작아 최적화 문제를 해결.
    - Layer Response 분석 결과:
        - ResNet의 각 층은 기존 네트워크보다 입력 신호를 더 작게 수정.

---


### 결론 및 의의

- **잔차 학습(Residual Learning)**은 깊은 네트워크의 최적화 문제를 해결하며, 더 많은 층을 쌓는 데 따른 성능 개선을 가능하게 함.
- ResNet은 ImageNet, CIFAR-10, COCO와 같은 다양한 비전 문제에서 최고 수준의 성능을 기록했으며, **딥러닝 모델 설계의 새로운 표준**을 제시함.


## 더 나아가기
### 문제 : 깊이에 따른 모델 효율성

- ResNet-152는 VGG-16/19보다 더 깊지만 복잡도는 낮고 성능은 높다. 하지만 CIFAR-10 실험에서는 1202층 모델이 오히려 110층 모델보다 테스트 성능이 낮아졌다. **잔차 학습 모델이 특정 깊이를 초과했을 때 과적합이나 성능 저하를 방지하기 위한 추가적인 기법**이 무엇이 있을까?


