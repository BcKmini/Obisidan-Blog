### 🚀 논문 요약: Deep Residual Learning for Image Recognition

---

### 핵심 내용

1. **문제 정의**
    
    - **깊은 신경망의 최적화 문제**:
        - 네트워크의 깊이가 증가할수록 정확도가 포화되거나 퇴화(Degradation)하는 현상 발생.
        - 이는 과적합 문제가 아닌, 최적화 과정에서 발생하는 어려움임.
2. **해결책: Residual Learning**
    
    - 네트워크의 출력 H(x)H(x)H(x)를 직접 학습하는 대신, 잔차 함수 F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x를 학습.
    - Shortcut Connection(바이패스 연결)을 사용하여 F(x)+xF(x) + xF(x)+x로 출력 계산.
    - **장점**:
        - 추가적인 매개변수 없이 최적화 문제 해결.
        - 매우 깊은 네트워크에서도 학습 및 성능 향상 가능.

---

### 실험 결과

#### 1. **ImageNet 결과**

- 최대 152층 깊이의 ResNet이 학습됨.
    
- VGG 네트워크보다 **복잡도는 낮지만 정확도는 높음**.
    
- **Top-5 오류율 비교** (단일 모델 기준):
-

| 모델         | Top-5 오류율 (%) |
|--------------|-----------------|
| VGG-16       | 9.33            |
| GoogLeNet    | 7.89            |
| ResNet-50    | 6.71            |
| ResNet-101   | 6.05            |
| ResNet-152   | 5.71            |

    
- **앙상블 성능**:
    
    - ResNet 앙상블은 ImageNet 테스트 데이터에서 **3.57%의 오류율**로 ILSVRC 2015 분류 과제에서 1위를 차지.

#### 2. **CIFAR-10 결과**

- 다양한 깊이의 네트워크(20, 32, 44, 56, 110층)에서 실험.
    
- 잔차 학습을 통해 깊이가 깊어질수록 테스트 오류율이 감소.
    
- **오류율 비교**:

| 모델        | 깊이 | 매개변수 수 | 테스트 오류율 (%) |
|-------------|------|------------|------------------|
| FitNet      | 19   | 2.5M       | 8.39             |
| Highway     | 32   | 1.25M      | 8.80             |
| ResNet-56   | 56   | 0.85M      | 6.97             |
| ResNet-110  | 110  | 1.7M       | 6.43             |

    
- **그래프**:
    
    - **Plain 네트워크**와 **Residual 네트워크** 비교:
        
        - 왼쪽: Plain 네트워크는 깊이가 증가할수록 최적화 실패.
        - 오른쪽: ResNet은 깊어질수록 학습 및 테스트 오류가 감소.

---

### 주요 분석

1. **ResNet의 효과**
    
    - 잔차 학습을 통해 깊은 네트워크에서도 안정적으로 학습 가능.
    - CIFAR-10과 ImageNet 모두에서 더 깊은 모델이 더 높은 성능을 발휘.
2. **잔차 함수와 출력**
    
    - 잔차 함수의 출력 크기가 일반적인 네트워크보다 작아 최적화 문제를 해결.
    - Layer Response 분석 결과:
        - ResNet의 각 층은 기존 네트워크보다 입력 신호를 더 작게 수정.

---


### 결론 및 의의

- **잔차 학습(Residual Learning)**은 깊은 네트워크의 최적화 문제를 해결하며, 더 많은 층을 쌓는 데 따른 성능 개선을 가능하게 함.
- ResNet은 ImageNet, CIFAR-10, COCO와 같은 다양한 비전 문제에서 최고 수준의 성능을 기록했으며, **딥러닝 모델 설계의 새로운 표준**을 제시함.


## 📌 더 나아가기
### 문제 : 깊이에 따른 모델 효율성

- ResNet-152는 VGG-16/19보다 더 깊지만 복잡도는 낮고 성능은 높다. 하지만 CIFAR-10 실험에서는 1202층 모델이 오히려 110층 모델보다 테스트 성능이 낮아졌다. **잔차 학습 모델이 특정 깊이를 초과했을 때 과적합이나 성능 저하를 방지하기 위한 추가적인 기법**이 무엇이 있을까?

### 🚀 논문 요약: DEEP LEARNING FOR VEGETATION IMAGE SEGMENTATION
IN LAI MEASUREMENT
## **연구 배경 및 목적**

- **LAI(Leaf Area Index)**는 농업과 환경 과학에서 중요한 지표로, 식물의 총 잎 면적과 단위 면적의 비율을 나타냅니다.
- DHP 방법은 **어안 카메라**를 사용해 촬영된 식물 이미지에서 잎과 배경을 분리하여 빛 투과도를 분석하는 방식입니다.
- 기존의 **Otsu** 및 **HSV 임계값 기반 방법**은 특정 환경에서 효과적이나, 강한 빛과 같은 복잡한 배경 조건에서는 성능이 저하됩니다.
- 이를 해결하기 위해 **Pix2pix 모델**을 개선해 DHP 이미지 분할 문제에 적용했습니다.

---

## **연구 방법**

1. **Pix2pix 모델 개선**:
    
    - 기존 **U-Net 구조**를 기반으로 하며, 입력과 출력의 유사성을 보존하기 위해 **skip connection**을 사용.
    - **Dense CRF**(Conditional Random Fields) 알고리즘을 추가해 경계 노이즈를 줄이고 세부 표현력을 향상.
    - **PatchGAN 구조**를 사용하여 지역 이미지의 고주파 정보를 잘 복원하도록 설계.
2. **데이터 준비 및 학습**:
    
    - 중국 여러 지역에서 촬영된 다양한 식물(벼, 옥수수, 차나무 등)의 어안 이미지를 수집.
    - 이미지를 256x256 크기로 분할하고, 회전 및 밝기 조절 등 데이터 증강을 수행.
    - 전체 데이터셋의 80%는 학습, 10%는 검증, 나머지 10%는 테스트에 사용.
3. **성능 평가**:
    
    - **Otsu**, **HSV 임계값 방법**과 개선된 Pix2pix 모델을 비교.
    - **정확도(Accuracy)**, **구조적 유사도(SSIM)**, **Precision**, **Recall** 등을 측정.

---

## **결과**

- **Pix2pix 모델의 성능**:
    - Otsu(0.8927), HSV(0.8142)와 비교해 Pix2pix 모델의 평균 정확도는 **0.9834**로 가장 우수.
    - SSIM 역시 Pix2pix 모델이 가장 높은 **0.9030**을 기록.
- **복잡한 환경에서도 우수한 성능**:
    - 강한 빛, 구름, 어두운 배경 등 복잡한 조건에서도 기존 방법보다 높은 정확도를 보임.
    - 개선된 Pix2pix 모델은 세부 표현 및 경계 처리에서 큰 개선 효과를 보임.
### **1. 주요 수식**

#### **1.1 Pix2pix 모델의 목적 함수**

Pix2pix는 조건부 생성적 적대 신경망(cGAN)을 기반으로 합니다. 목적 함수는 다음과 같습니다.

LcGAN(G,D)=Ex,y[log⁡D(x,y)]+Ex,z[log⁡(1−D(x,G(x,z)))]\mathcal{L}_{cGAN}(G, D) = \mathbb{E}_{x,y}[\log D(x, y)] + \mathbb{E}_{x,z}[\log(1 - D(x, G(x, z)))]LcGAN​(G,D)=Ex,y​[logD(x,y)]+Ex,z​[log(1−D(x,G(x,z)))]

여기서:

- GGG: 생성기 (Generator)
- DDD: 판별기 (Discriminator)
- xxx: 입력 이미지
- yyy: 실제 결과 이미지
- zzz: 조건 데이터

#### **1.2 L1 손실 추가**

입력 이미지와 출력 이미지 간의 유사성을 높이기 위해 L1 손실을 추가합니다.

LL1(G)=Ex,y,z[∣∣y−G(x,z)∣∣1]\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[||y - G(x, z)||_1]LL1​(G)=Ex,y,z​[∣∣y−G(x,z)∣∣1​]

#### **1.3 최종 손실 함수**

최종적으로 Pix2pix 모델의 손실 함수는 다음과 같이 정의됩니다.

G∗=arg⁡min⁡Gmax⁡DLcGAN(G,D)+λLL1(G)G^* = \arg \min_G \max_D \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G)G∗=argGmin​Dmax​LcGAN​(G,D)+λLL1​(G)

여기서 λ\lambdaλ는 L1 손실의 가중치 조정 파라미터입니다.

---

### **2. 실험 결과 요약**

#### **2.1 정확도 및 SSIM 비교**

Pix2pix 모델과 기존 방법(Otsu, HSV)의 성능을 정확도와 SSIM(구조적 유사도 지수)로 비교한 결과는 아래와 같습니다.

|**분할 방법**|**평균 정확도 (Accuracy)**|**평균 SSIM**|
|---|---|---|
|**Otsu**|0.8927|0.7724|
|**HSV Threshold**|0.8142|0.6342|
|**Improved Pix2pix**|**0.9834**|**0.9030**|

#### **2.2 Precision 및 Recall**

Pix2pix 모델의 잎과 배경에 대한 Precision 및 Recall은 아래 표와 같습니다.

|**항목**|**Precision**|**Recall**|
|---|---|---|
|**잎(Leaves)**|0.9874|0.9846|
|**배경(Background)**|0.9174|0.9283|
