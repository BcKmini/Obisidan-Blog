## Q
ax + b -> 인공신경망으로 그리기 

## A
#### 개념(Definition)

- 인공신경망의 한 층은 보통 “선형 변환 + 활성화 함수”로 표현됨:
$$

y=σ(Wx+b)
$$
ax+b는 선형식이므로, **활성화가 항등함수(Identity)** 라면 뉴런 1개로 정확히 표현 가능:
$$
y=1 ⋅⁣(ax+b)=ax+by 
$$
- 만약 **ReLU만** 사용해야 한다면, 다음 항등식으로 구현 가능:
$$
x=ReLU(x)−ReLU(−x)x 
$$

따라서
$$
ax+b=a [ReLU(x)−ReLU(−x)]+b
$$
→ ReLU 뉴런 2개와 최종 선형 결합으로 정확히 표현.

---
#### 수식

- 항등 활성화 사용:
$$
y=ax+b(σ=Id)
$$
- ReLU만 사용:
$$
\begin{bmatrix}
z1=ReLU(x \\ 
z2=ReLU(−x) \\
y=az1−az2+b 
\end{bmatrix}
$$


> Linear Activation으로 설명한 그림
![[인공신경망-함수.png]]
