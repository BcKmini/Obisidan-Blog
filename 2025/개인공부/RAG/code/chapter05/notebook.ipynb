{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x7b3Rpfgpk0"
      },
      "source": [
        "# 5. LangChain Expression Language(LCEL) 심층 해설\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "jvuKOwNbgpk1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m]    \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]    = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]= os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]    = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ4fwamMgpk1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-core==0.3.0\n",
            "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-openai==0.2.0\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-community==0.3.0\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pydantic==2.10.6 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (2.10.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-core==0.3.0) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core==0.3.0)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core==0.3.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-core==0.3.0) (24.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core==0.3.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-core==0.3.0) (4.12.2)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai==0.2.0)\n",
            "  Downloading openai-1.93.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
            "  Downloading tiktoken-0.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-community==0.3.0) (2.0.40)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.0)\n",
            "  Downloading aiohttp-3.12.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
            "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-community==0.3.0) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.0)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langchain-community==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from pydantic==2.10.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from pydantic==2.10.6) (2.27.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (25.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading frozenlist-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (6.1.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading propcache-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading yarl-1.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
            "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.28.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
            "  Downloading orjson-3.10.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.9.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
            "  Downloading jiter-0.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (1.0.0)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from requests<3,>=2->langchain-community==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from requests<3,>=2->langchain-community==0.3.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from requests<3,>=2->langchain-community==0.3.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from requests<3,>=2->langchain-community==0.3.0) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.0) (3.2.1)\n",
            "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai==0.2.0)\n",
            "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/mini/miniconda3/envs/mini/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.14.0)\n",
            "INFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0)\n",
            "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
            "Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "Downloading jiter-0.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading orjson-3.10.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "Downloading propcache-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (200 kB)\n",
            "Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading yarl-1.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: typing-inspection, tenacity, regex, propcache, orjson, mypy-extensions, marshmallow, jsonpatch, jiter, frozenlist, async-timeout, aiohappyeyeballs, yarl, typing-inspect, tiktoken, aiosignal, pydantic-settings, openai, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.26.0\n",
            "    Uninstalling openai-1.26.0:\n",
            "      Successfully uninstalled openai-1.26.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "eras 0.5.2 requires openai==1.26.0, but you have openai 1.93.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.7.0 jiter-0.10.0 jsonpatch-1.33 langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.0 langchain-openai-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.93.0 orjson-3.10.18 propcache-0.3.2 pydantic-settings-2.10.1 regex-2024.11.6 tenacity-8.5.0 tiktoken-0.9.0 typing-inspect-0.9.0 typing-inspection-0.4.1 yarl-1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0 pydantic==2.10.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkvQCCUQgpk1"
      },
      "source": [
        "## 5.1. Runnable과 RunnableSequence―LCEL의 가장 기본적인 구성 요소\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
          "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
          "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
          "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
        },
        "id": "iw47kajfgpk2"
      },
      "outputs": [
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      6\u001b[0m     [\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m사용자가 입력한 요리의 레시피를 생각해 주세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{dish}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n",
            "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/langchain_core/load/serializable.py:112\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:516\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    515\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
            "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/openai/_client.py:130\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    128\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
          "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
          "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
          "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
        },
        "id": "v1e4ytz_gpk2"
      },
      "outputs": [],
      "source": [
        "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
        "ai_message = model.invoke(prompt_value)\n",
        "output = output_parser.invoke(ai_message)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
          "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
          "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
          "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
        },
        "id": "Y1_uIxQ4gpk2"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
          "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
          "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
          "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
        },
        "id": "sidnFryBgpk2"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke({\"dish\": \"카레\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyCfUHUngpk2"
      },
      "source": [
        "### Runnable의 실행 방법―invoke・stream・batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
          "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
          "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
          "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
        },
        "id": "G44R6pakgpk3"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "for chunk in chain.stream({\"dish\": \"카레\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
          "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
          "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
          "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
        },
        "id": "2pol6DGIgpk3"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "outputs = chain.batch([{\"dish\": \"카레\"}, {\"dish\": \"우동\"}])\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2QALc2_gpk3"
      },
      "source": [
        "### LCEL의 \"|\"로 다양한 Runnable 연결하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
          "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
          "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
          "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
        },
        "id": "f3Djwd2Tgpk3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
          "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
          "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
          "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
        },
        "id": "dCC0nUongpk3"
      },
      "outputs": [],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자의 질문에 단계적으로 답변하세요.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
          "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
          "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
          "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
        },
        "id": "26AxQ2Pygpk3"
      },
      "outputs": [],
      "source": [
        "summarize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"단계적으로 생각한 답변에서 결론만 추출하세요.\"),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
          "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
          "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
          "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
        },
        "id": "VDRZWSxqgpk3"
      },
      "outputs": [],
      "source": [
        "cot_summarize_chain = cot_chain | summarize_chain\n",
        "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5nZTZFgpk3"
      },
      "source": [
        "## 5.2. RunnableLambda―임의의 함수를 Runnable로 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
          "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
          "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
          "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
        },
        "id": "POlYJCm4gpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
          "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
          "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
          "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
        },
        "id": "y8DJj3qvgpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfB51xjgpk4"
      },
      "source": [
        "### chain 데코레이터를 사용한 RunnableLambda 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
          "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
          "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
          "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
        },
        "id": "6A7mxm6ugpk4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJCe5l0Ngpk4"
      },
      "source": [
        "### RunnableLambda 자동 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
          "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
          "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
          "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
        },
        "id": "xTSxNdrFgpk4"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
          "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
          "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
          "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
        },
        "id": "kcZUxBzegpk4"
      },
      "outputs": [],
      "source": [
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEZdX1idgpk4"
      },
      "source": [
        "### Runnable의 입력 타입과 출력 타입에 주의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
          "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
          "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
          "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
        },
        "id": "BgXWvKxSgpk4"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | upper\n",
        "\n",
        "# 아래 코드를 실행하면 오류가 발생합니다\n",
        "output = chain.invoke({\"input\": \"Hello!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
          "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
          "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
          "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
        },
        "id": "maAuLYwXgpk4"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | StrOutputParser() | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zatE6tdcgpk5"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyx---Y6gpk5"
      },
      "source": [
        "### (칼럼) 사용자 함수를 stream에 대응시키는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:44.533838Z",
          "iopub.status.busy": "2024-06-28T02:33:44.533678Z",
          "iopub.status.idle": "2024-06-28T02:33:45.353621Z",
          "shell.execute_reply": "2024-06-28T02:33:45.353115Z"
        },
        "id": "uMj9etfGgpk5"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "\n",
        "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
        "    for text in input_stream:\n",
        "        yield text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | StrOutputParser() | upper\n",
        "\n",
        "for chunk in chain.stream({\"input\": \"Hello!\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BklIITh1gpk5"
      },
      "source": [
        "## 5.3. RunnableParallel―여러 Runnable을 병렬로 처리하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
          "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
          "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
          "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
        },
        "id": "q69b2WNfgpk5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
          "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
          "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
          "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
        },
        "id": "vDdo367ogpk5"
      },
      "outputs": [],
      "source": [
        "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "optimistic_chain = optimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
          "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
          "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
          "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
        },
        "id": "_96vanPzgpk5"
      },
      "outputs": [],
      "source": [
        "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "pessimistic_chain = pessimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
          "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
          "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
          "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
        },
        "id": "4jE7_Kg5gpk5"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        ")\n",
        "\n",
        "output = parallel_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_836fMgpk6"
      },
      "source": [
        "### RunnableParallel의 출력을 Runnable의 입력으로 연결하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
          "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
          "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
          "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
        },
        "id": "HnU_2vpvgpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 객관적 AI입니다. 두 가지 의견을 종합하세요.\"),\n",
        "        (\"human\", \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZf4LL47gpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"optimistic_opinion\": optimistic_chain,\n",
        "            \"pessimistic_opinion\": pessimistic_chain,\n",
        "        }\n",
        "    )\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COKJXWrsgpk6"
      },
      "source": [
        "### RunnableParallel 자동 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
          "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
          "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
          "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
        },
        "id": "TycHWzFegpk6"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
          "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
          "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
          "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
        },
        "id": "B2tXTmJegpk7"
      },
      "outputs": [],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNN-x47bgpk7"
      },
      "source": [
        "### RunnableLambda와의 조합―itemgetter를 사용한 예시\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
          "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
          "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
          "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
        },
        "id": "_0CLV_6sgpk7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "topic_getter = itemgetter(\"topic\")\n",
        "topic = topic_getter({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
          "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
          "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
          "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
        },
        "id": "mXj7XLL5gpk7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"당신은 객관적 AI입니다. {topic}에 대한 두 가지 의견을 종합하세요.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "        \"topic\": itemgetter(\"topic\"),\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIWPMfaHgpk7"
      },
      "source": [
        "## 5.4. RunnablePassthrough―입력을 그대로 출력하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZEBEbd9gpk7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mFb4dTHgpk7"
      },
      "outputs": [],
      "source": [
        "!pip install tavily-python==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkeaFNJtgpk7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "다음 문맥만을 고려해 질문에 답하세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGANXxDTgpk7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "\n",
        "retriever = TavilySearchAPIRetriever(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRwWTmr4gpk8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWMIB2fegpk8"
      },
      "source": [
        "### assign―RunnableParallel의 출력에 값 추가하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
          "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
          "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
          "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
        },
        "id": "kuEaBuX1gpk8"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever,\n",
        "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
        "\n",
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
          "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
          "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
          "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
        },
        "id": "M2TujE69gpk8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain = RunnableParallel(\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": retriever,\n",
        "    }\n",
        ").assign(answer=prompt | model | StrOutputParser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCCNWebgpk8"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFWT_Wq6gpk8"
      },
      "source": [
        "#### <보충:pick>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:27.725885Z",
          "iopub.status.busy": "2024-06-28T02:34:27.725708Z",
          "iopub.status.idle": "2024-06-28T02:34:34.101909Z",
          "shell.execute_reply": "2024-06-28T02:34:34.101439Z"
        },
        "id": "JKm5b4NRgpk8"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"context\": retriever,\n",
        "        }\n",
        "    )\n",
        "    .assign(answer=prompt | model | StrOutputParser())\n",
        "    .pick([\"context\", \"answer\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk5Xznb9gpk8"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHTA4a6gpk8"
      },
      "source": [
        "### (칼럼) astream_events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdHIHS7mgpk9"
      },
      "outputs": [],
      "source": [
        "# Google Colab에서는 다음 코드의 \"async\" 부분에 \"Use of \"async\" not allowed outside of async function\"이라고 표시되지만, 오류 없이 실행할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
          "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
          "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
          "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
        },
        "id": "c43P3RnWgpk9"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
        "    print(event, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xELg9Dj3gpk9"
      },
      "outputs": [],
      "source": [
        "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
        "    event_kind = event[\"event\"]\n",
        "\n",
        "    if event_kind == \"on_retriever_end\":\n",
        "        print(\"=== 검색 결과 ===\")\n",
        "        documents = event[\"data\"][\"output\"]\n",
        "        for document in documents:\n",
        "            print(document)\n",
        "\n",
        "    elif event_kind == \"on_parser_start\":\n",
        "        print(\"=== 최종 출력 ===\")\n",
        "\n",
        "    elif event_kind == \"on_parser_stream\":\n",
        "        chunk = event[\"data\"][\"chunk\"]\n",
        "        print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J1ZES31gpk9"
      },
      "source": [
        "### (칼럼) Chat history와 Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPqsChLigpk9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATzS7K7Sgpk9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "\n",
        "def respond(session_id: str, human_message: str) -> str:\n",
        "    chat_message_history = SQLChatMessageHistory(\n",
        "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
        "    )\n",
        "\n",
        "    ai_message = chain.invoke(\n",
        "        {\n",
        "            \"chat_history\": chat_message_history.get_messages(),\n",
        "            \"input\": human_message,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    chat_message_history.add_user_message(human_message)\n",
        "    chat_message_history.add_ai_message(ai_message)\n",
        "\n",
        "    return ai_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMFL6ZHagpk9"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "session_id = uuid4().hex\n",
        "\n",
        "output1 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"안녕하세요! 저는 존이라고 합니다!\",\n",
        ")\n",
        "print(output1)\n",
        "\n",
        "output2 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"제 이름을 알고 계신가요?\",\n",
        ")\n",
        "print(output2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
