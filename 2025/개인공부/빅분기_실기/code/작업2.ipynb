{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv()\n",
    "test = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "X_train['환불금액'] = X_train['환불금액'].fillna(0)\n",
    "X_test['환불금액'] = X_test['환불금액'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_columns = X_train.select_dtypes(exclude='object').columns\n",
    "\n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(175.08333333333334)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e8_p1_1.csv')\n",
    "# df.head(5)\n",
    "#df\n",
    "diff = df.groupby('대륙')['소비량'].mean()\n",
    "diff.sort_values(ascending=False).iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79607.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e8_p1_2.csv')\n",
    "#df.head(5)\n",
    "df['총방문객수'] = df['관광'] + df['방문'] + df['유학연수'] + df['취업'] + df['기타']\n",
    "#print(df['국가'].unique())\n",
    "df['관광비율'] = df['관광'] / df['총방문객수']\n",
    "result1 = df.sort_values(by='관광비율', ascending=False).iloc[1]\n",
    "#df['관광'].sort_values(ascendig=False).iloc[2]\n",
    "#print(result1)\n",
    "a = result1['취업']\n",
    "result2 = df.sort_values(by='취업', ascending=False).iloc[2]\n",
    "b = result2['방문']\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.266\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e8_p1_3.csv')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_CO = MinMaxScaler()\n",
    "scaler_NM = MinMaxScaler()\n",
    "df['CO(GT)'] = scaler_CO.fit_transform(df[['CO(GT)']])\n",
    "df['NMHC(GT)'] = scaler_NM.fit_transform(df[['NMHC(GT)']])\n",
    "\n",
    "a = np.std(df['CO(GT)'])\n",
    "b = np.std(df['NMHC(GT)'])\n",
    "print(round(abs(a-b),3)) # 0.266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.062467\n",
      "         Iterations 10\n",
      "Intercept    3.495639e-05\n",
      "X7           3.364761e-03\n",
      "X8           4.314143e+01\n",
      "X11          1.047526e+13\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e8_p3_1.csv')\n",
    "from statsmodels.api import Logit\n",
    "import numpy as np\n",
    "# formula = 'Y ~ X1 + X2 + X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15'\n",
    "# model = Logit.from_formula(formula, df).fit()\n",
    "# print(model.summary()) # 7, 8, 11 -> 3개  # 1번 12개\n",
    "\n",
    "formula = 'Y ~ X7+X8+X11'\n",
    "model = Logit.from_formula(formula, df).fit()\n",
    "#print(model.summary())\n",
    "# print(model.params.sum())\n",
    "result2 = model.params.sum() # 3.558\n",
    "# print(round(result2, 3))\n",
    "\n",
    "print(np.exp(5*model.params)) # 0.00336476092180229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OLS in module statsmodels.regression.linear_model:\n",
      "\n",
      "class OLS(WLS)\n",
      " |  OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |  \n",
      " |  Ordinary Least Squares\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      A 1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  hasconst : None or bool\n",
      " |      Indicates whether the RHS includes a user-supplied constant. If True,\n",
      " |      a constant is not checked for and k_constant is set to 1 and all\n",
      " |      result statistics are calculated as if a constant is present. If\n",
      " |      False, a constant is not checked for and k_constant is set to 0.\n",
      " |  **kwargs\n",
      " |      Extra arguments that are used to set model properties when using the\n",
      " |      formula interface.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  weights : scalar\n",
      " |      Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  WLS : Fit a linear model using Weighted Least Squares.\n",
      " |  GLS : Fit a linear model using Generalized Least Squares.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  No constant is added by the model unless you are using formulas.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import statsmodels.api as sm\n",
      " |  >>> import numpy as np\n",
      " |  >>> duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
      " |  >>> Y = duncan_prestige.data['income']\n",
      " |  >>> X = duncan_prestige.data['education']\n",
      " |  >>> X = sm.add_constant(X)\n",
      " |  >>> model = sm.OLS(Y,X)\n",
      " |  >>> results = model.fit()\n",
      " |  >>> results.params\n",
      " |  const        10.603498\n",
      " |  education     0.594859\n",
      " |  dtype: float64\n",
      " |  \n",
      " |  >>> results.tvalues\n",
      " |  const        2.039813\n",
      " |  education    6.892802\n",
      " |  dtype: float64\n",
      " |  \n",
      " |  >>> print(results.t_test([1, 0]))\n",
      " |                               Test for Constraints\n",
      " |  ==============================================================================\n",
      " |                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      " |  ------------------------------------------------------------------------------\n",
      " |  c0            10.6035      5.198      2.040      0.048       0.120      21.087\n",
      " |  ==============================================================================\n",
      " |  \n",
      " |  >>> print(results.f_test(np.identity(2)))\n",
      " |  <F test: F=array([[159.63031026]]), p=1.2607168903696672e-20,\n",
      " |   df_denom=43, df_num=2>\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OLS\n",
      " |      WLS\n",
      " |      RegressionModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_regularized(self, method='elastic_net', alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False, **kwargs)\n",
      " |      Return a regularized fit to a linear regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str\n",
      " |          Either 'elastic_net' or 'sqrt_lasso'.\n",
      " |      alpha : scalar or array_like\n",
      " |          The penalty weight.  If a scalar, the same penalty weight\n",
      " |          applies to all variables in the model.  If a vector, it\n",
      " |          must have the same length as `params`, and contains a\n",
      " |          penalty weight for each coefficient.\n",
      " |      L1_wt : scalar\n",
      " |          The fraction of the penalty given to the L1 penalty term.\n",
      " |          Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
      " |          ridge fit, if 1 it is a lasso fit.\n",
      " |      start_params : array_like\n",
      " |          Starting values for ``params``.\n",
      " |      profile_scale : bool\n",
      " |          If True the penalized fit is computed using the profile\n",
      " |          (concentrated) log-likelihood for the Gaussian model.\n",
      " |          Otherwise the fit uses the residual sum of squares.\n",
      " |      refit : bool\n",
      " |          If True, the model is refit using only the variables that\n",
      " |          have non-zero coefficients in the regularized fit.  The\n",
      " |          refitted model is not regularized.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      statsmodels.base.elastic_net.RegularizedResults\n",
      " |          The regularized results.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The elastic net uses a combination of L1 and L2 penalties.\n",
      " |      The implementation closely follows the glmnet package in R.\n",
      " |      \n",
      " |      The function that is minimized is:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          0.5*RSS/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
      " |      \n",
      " |      where RSS is the usual regression sum of squares, n is the\n",
      " |      sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
      " |      norms.\n",
      " |      \n",
      " |      For WLS and GLS, the RSS is calculated using the whitened endog and\n",
      " |      exog data.\n",
      " |      \n",
      " |      Post-estimation results are based on the same data used to\n",
      " |      select variables, hence may be subject to overfitting biases.\n",
      " |      \n",
      " |      The elastic_net method uses the following keyword arguments:\n",
      " |      \n",
      " |      maxiter : int\n",
      " |          Maximum number of iterations\n",
      " |      cnvrg_tol : float\n",
      " |          Convergence threshold for line searches\n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The square root lasso approach is a variation of the Lasso\n",
      " |      that is largely self-tuning (the optimal tuning parameter\n",
      " |      does not depend on the standard deviation of the regression\n",
      " |      errors).  If the errors are Gaussian, the tuning parameter\n",
      " |      can be taken to be\n",
      " |      \n",
      " |      alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p))\n",
      " |      \n",
      " |      where n is the sample size and p is the number of predictors.\n",
      " |      \n",
      " |      The square root lasso uses the following keyword arguments:\n",
      " |      \n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The cvxopt module is required to estimate model using the square root\n",
      " |      lasso.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
      " |         generalized linear models via coordinate descent.  Journal of\n",
      " |         Statistical Software 33(1), 1-22 Feb 2010.\n",
      " |      \n",
      " |      .. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso:\n",
      " |         pivotal recovery of sparse signals via conic programming.\n",
      " |         Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf\n",
      " |  \n",
      " |  hessian(self, params, scale=None)\n",
      " |      Evaluate the Hessian function at a given point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the Hessian is computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The Hessian matrix.\n",
      " |  \n",
      " |  hessian_factor(self, params, scale=None, observed=True)\n",
      " |      Calculate the weights for the Hessian.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The parameter at which Hessian is evaluated.\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is returned. If false then the\n",
      " |          expected information matrix is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          A 1d weight vector used in the calculation of the Hessian.\n",
      " |          The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
      " |  \n",
      " |  loglike(self, params, scale=None)\n",
      " |      The likelihood function for the OLS model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The coefficients with which to estimate the log-likelihood.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The likelihood function evaluated at params.\n",
      " |  \n",
      " |  score(self, params, scale=None)\n",
      " |      Evaluate the score function at a given point.\n",
      " |      \n",
      " |      The score corresponds to the profile (concentrated)\n",
      " |      log-likelihood in which the scale parameter has been profiled\n",
      " |      out.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the score function is\n",
      " |          computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The score vector.\n",
      " |  \n",
      " |  whiten(self, x)\n",
      " |      OLS model whitener does nothing.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          Data to be whitened.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          The input array unmodified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      OLS : Fit a linear model using Ordinary Least Squares.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RegressionModel:\n",
      " |  \n",
      " |  fit(self, method: \"Literal['pinv', 'qr']\" = 'pinv', cov_type: \"Literal['nonrobust', 'fixed scale', 'HC0', 'HC1', 'HC2', 'HC3', 'HAC', 'hac-panel', 'hac-groupsum', 'cluster']\" = 'nonrobust', cov_kwds=None, use_t: 'bool | None' = None, **kwargs)\n",
      " |      Full fit of the model.\n",
      " |      \n",
      " |      The results include an estimate of covariance matrix, (whitened)\n",
      " |      residuals and an estimate of scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str, optional\n",
      " |          Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
      " |          to solve the least squares problem. \"qr\" uses the QR\n",
      " |          factorization.\n",
      " |      cov_type : str, optional\n",
      " |          See `regression.linear_model.RegressionResults` for a description\n",
      " |          of the available covariance estimators.\n",
      " |      cov_kwds : list or None, optional\n",
      " |          See `linear_model.RegressionResults.get_robustcov_results` for a\n",
      " |          description required keywords for alternative covariance\n",
      " |          estimators.\n",
      " |      use_t : bool, optional\n",
      " |          Flag indicating to use the Student's t distribution when computing\n",
      " |          p-values.  Default behavior depends on cov_type. See\n",
      " |          `linear_model.RegressionResults.get_robustcov_results` for\n",
      " |          implementation details.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      RegressionResults\n",
      " |          The model estimation results.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      RegressionResults\n",
      " |          The results container.\n",
      " |      RegressionResults.get_robustcov_results\n",
      " |          A method to change the covariance estimator used when fitting the\n",
      " |          model.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The fit method uses the pseudoinverse of the design/exogenous variables\n",
      " |      to solve the least squares minimization.\n",
      " |  \n",
      " |  get_distribution(self, params, scale, exog=None, dist_class=None)\n",
      " |      Construct a random number generator for the predictive distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The model parameters (regression coefficients).\n",
      " |      scale : scalar\n",
      " |          The variance parameter.\n",
      " |      exog : array_like\n",
      " |          The predictor variable matrix.\n",
      " |      dist_class : class\n",
      " |          A random number generator class.  Must take 'loc' and 'scale'\n",
      " |          as arguments and return a random number generator implementing\n",
      " |          an ``rvs`` method for simulating random values. Defaults to normal.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      gen\n",
      " |          Frozen random number generator object with mean and variance\n",
      " |          determined by the fitted linear model.  Use the ``rvs`` method\n",
      " |          to generate random values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to the behavior of ``scipy.stats.distributions objects``,\n",
      " |      the returned random number generator must be called with\n",
      " |      ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
      " |      the data set used to fit the model.  If any other value is\n",
      " |      used for ``n``, misleading results will be produced.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize model components.\n",
      " |  \n",
      " |  predict(self, params, exog=None)\n",
      " |      Return linear predicted values from a design matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Parameters of a linear model.\n",
      " |      exog : array_like, optional\n",
      " |          Design / exogenous data. Model exog is used if None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          An array of fitted values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the model has not yet been fit, params is not optional.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RegressionModel:\n",
      " |  \n",
      " |  df_model\n",
      " |      The model degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the rank of the regressor matrix minus 1 if a\n",
      " |      constant is included.\n",
      " |  \n",
      " |  df_resid\n",
      " |      The residual degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the number of observations minus the rank of\n",
      " |      the regressor matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.api import OLS\n",
    "print(help(OLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    PIQ   R-squared:                       0.295\n",
      "Model:                            OLS   Adj. R-squared:                  0.233\n",
      "Method:                 Least Squares   F-statistic:                     4.741\n",
      "Date:                Tue, 17 Jun 2025   Prob (F-statistic):            0.00722\n",
      "Time:                        17:28:23   Log-Likelihood:                -165.25\n",
      "No. Observations:                  38   AIC:                             338.5\n",
      "Df Residuals:                      34   BIC:                             345.1\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    111.3536     62.971      1.768      0.086     -16.619     239.326\n",
      "Brain          2.0604      0.563      3.657      0.001       0.915       3.205\n",
      "Height        -2.7319      1.229     -2.222      0.033      -5.230      -0.233\n",
      "Weight         0.0006      0.197      0.003      0.998      -0.400       0.401\n",
      "==============================================================================\n",
      "Omnibus:                        1.379   Durbin-Watson:                   1.827\n",
      "Prob(Omnibus):                  0.502   Jarque-Bera (JB):                1.088\n",
      "Skew:                           0.409   Prob(JB):                        0.580\n",
      "Kurtosis:                       2.859   Cond. No.                     3.73e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.73e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e8_p3_2.csv')\n",
    "#df.head()\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "formula = 'PIQ ~ Brain + Height + Weight'\n",
    "model = OLS.from_formula(formula, df).fit()\n",
    "print(model.summary())\n",
    "#print(model.params['Brain']) # 2.06036679789495\n",
    "#  0.2949392104739402\n",
    "ex = pd.DataFrame({'Brain' : [90], 'Height' : [70], 'Weight' : [150] })\n",
    "pred = model.predict(ex)\n",
    "#\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.713855688712825\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e7_p1.csv')\n",
    "# print(df.isnull().sum()) #국어\n",
    "# 각 과목별 응시 인원 (NaN 제외)\n",
    "counts = df[['국어', '수학', '영어', '과학']].count()\n",
    "\n",
    "scores = df['국어'].dropna()\n",
    "mean = df['국어'].mean()\n",
    "std = np.std(df['국어'])\n",
    "\n",
    "result = (scores - mean) / std\n",
    "result.max()\n",
    "print(result.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06289356546077182\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e7_p2_.csv')\n",
    "#df.head(5)\n",
    "result = df.corr()['var_11'].abs().sort_values().index[-2]\n",
    "a = df[result].mean()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e7_p3.csv')\n",
    "\n",
    "data = df['var_6']\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outlier_count = data[(data < lower_bound) | (data > upper_bound)].count()\n",
    "\n",
    "print(outlier_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.343347\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:                  210\n",
      "Model:                          Logit   Df Residuals:                      197\n",
      "Method:                           MLE   Df Model:                           12\n",
      "Date:                Tue, 17 Jun 2025   Pseudo R-squ.:                  0.5014\n",
      "Time:                        17:46:47   Log-Likelihood:                -72.103\n",
      "converged:                       True   LL-Null:                       -144.61\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.821e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "age           -0.0448      0.027     -1.688      0.091      -0.097       0.007\n",
      "sex            1.1075      0.596      1.857      0.063      -0.061       2.277\n",
      "cp             0.5500      0.227      2.421      0.015       0.105       0.995\n",
      "trestbps       0.0176      0.012      1.478      0.139      -0.006       0.041\n",
      "chol           0.0043      0.004      0.969      0.332      -0.004       0.013\n",
      "fbs           -0.8977      0.632     -1.420      0.156      -2.137       0.341\n",
      "restecg        0.3694      0.228      1.620      0.105      -0.078       0.816\n",
      "thalach       -0.0470      0.011     -4.389      0.000      -0.068      -0.026\n",
      "exang          0.8792      0.478      1.841      0.066      -0.057       1.815\n",
      "oldpeak        0.3512      0.243      1.443      0.149      -0.126       0.828\n",
      "slope          0.2034      0.406      0.501      0.616      -0.592       0.999\n",
      "ca             0.9572      0.296      3.233      0.001       0.377       1.537\n",
      "thal           0.3285      0.121      2.718      0.007       0.092       0.565\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e7_p3_t.csv')\n",
    "df.head()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "train = df.iloc[:210].reset_index(drop=True)\n",
    "test = df.iloc[210:].reset_index(drop=True)\n",
    "\n",
    "# 종속변수와 독립변수 설정\n",
    "X = train.drop('target', axis=1)\n",
    "y = train['target']\n",
    "\n",
    "# 로지스틱 회귀모형 적합\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "\n",
    "# age의 weight 오즈비 계산\n",
    "odds_ratios = np.exp(model.params['age'])\n",
    "odds_ratios\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소방서명: 중랑소방서\n",
      "평균 소요시간(초): -2.8524631147540982e-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e6_p1_1.csv')\n",
    "\n",
    "# 출동시각, 신고시각 컬럼을 datetime으로 변환\n",
    "df['출동시각'] = pd.to_datetime(df['출동시각'])\n",
    "df['신고시각'] = pd.to_datetime(df['신고시각'])\n",
    "\n",
    "# 소요시간 (초 단위)\n",
    "df['소요시간'] = (df['출동시각'] - df['신고시각']).dt.total_seconds()\n",
    "\n",
    "# 소방서명 별 평균 소요시간\n",
    "result = df.groupby('소방서명')['소요시간'].mean().sort_values() \n",
    "\n",
    "# 3번째 값\n",
    "third_name = result.index[2]\n",
    "third_value = result.iloc[2]\n",
    "\n",
    "print('소방서명:', third_name)\n",
    "print('평균 소요시간(초):', third_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>학교명</th>\n",
       "      <th>시도</th>\n",
       "      <th>학교세부유형</th>\n",
       "      <th>일반학급_학생수_계</th>\n",
       "      <th>교원수_총계_계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울대학교사범대학부설고등학교</td>\n",
       "      <td>서울</td>\n",
       "      <td>일반고등학교</td>\n",
       "      <td>689</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>가락고등학교</td>\n",
       "      <td>서울</td>\n",
       "      <td>일반고등학교</td>\n",
       "      <td>765</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개포고등학교</td>\n",
       "      <td>서울</td>\n",
       "      <td>일반고등학교</td>\n",
       "      <td>684</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경기고등학교</td>\n",
       "      <td>서울</td>\n",
       "      <td>일반고등학교</td>\n",
       "      <td>1047</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경동고등학교</td>\n",
       "      <td>서울</td>\n",
       "      <td>일반고등학교</td>\n",
       "      <td>529</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               학교명  시도  학교세부유형  일반학급_학생수_계  교원수_총계_계\n",
       "0  서울대학교사범대학부설고등학교  서울  일반고등학교         689        73\n",
       "1           가락고등학교  서울  일반고등학교         765        70\n",
       "2           개포고등학교  서울  일반고등학교         684        69\n",
       "3           경기고등학교  서울  일반고등학교        1047        96\n",
       "4           경동고등학교  서울  일반고등학교         529        58"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e6_p1_2.csv')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Gender   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.6344 -26.724\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../csv/Titanic.csv\")\n",
    "\n",
    "q1 = df['Fare'].quantile(0.25)\n",
    "q3 = df['Fare'].quantile(0.75)\n",
    "\n",
    "iqr = q3 - q1\n",
    "\n",
    "down = q1 - 1.5 * iqr\n",
    "up = q3 + 1.5 * iqr\n",
    "\n",
    "out1 = df[df['Fare'] < down]\n",
    "out2 = df[df['Fare'] > up]\n",
    "\n",
    "print(up, down)\n",
    "len(out1), len(out2)\n",
    "\n",
    "print(sum(out2['Gender'] == 'female'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.459658\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               Survived   No. Observations:                  891\n",
      "Model:                          Logit   Df Residuals:                      886\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Wed, 18 Jun 2025   Pseudo R-squ.:                  0.3097\n",
      "Time:                        14:49:59   Log-Likelihood:                -409.56\n",
      "converged:                       True   LL-Null:                       -593.33\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.854e-78\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept          3.4596      0.311     11.109      0.000       2.849       4.070\n",
      "Gender[T.male]    -2.7624      0.195    -14.141      0.000      -3.145      -2.379\n",
      "Pclass            -0.9392      0.107     -8.815      0.000      -1.148      -0.730\n",
      "SibSp             -0.2340      0.099     -2.359      0.018      -0.428      -0.040\n",
      "Parch             -0.0503      0.110     -0.455      0.649      -0.267       0.166\n",
      "==================================================================================\n",
      "-0.05026012943957689\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../csv/Titanic.csv\")\n",
    "\n",
    "from statsmodels.api import Logit, OLS # from scipy import stats\n",
    "formula = 'Survived ~ Pclass + Gender + SibSp + Parch'\n",
    "model = Logit.from_formula(formula, df).fit()\n",
    "print(model.summary())\n",
    "print(model.params['Parch'])# 0.05026012943957689"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', 'Covariance', 'DegenerateDataWarning', 'FitError', 'MonteCarloMethod', 'NearConstantInputWarning', 'PermutationMethod', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_ansari_swilk_statistics', '_axis_nan_policy', '_biasedurn', '_binned_statistic', '_binomtest', '_boost', '_bws_test', '_censored_data', '_common', '_constants', '_continuous_distns', '_covariance', '_crosstab', '_discrete_distns', '_distn_infrastructure', '_distr_params', '_entropy', '_fit', '_hypotests', '_kde', '_ksstats', '_levy_stable', '_mannwhitneyu', '_morestats', '_mstats_basic', '_mstats_extras', '_multicomp', '_multivariate', '_mvn', '_odds_ratio', '_page_trend_test', '_qmc', '_qmc_cy', '_qmvnt', '_rcont', '_relative_risk', '_resampling', '_rvs_sampling', '_sampling', '_sensitivity_analysis', '_sobol', '_stats', '_stats_mstats_common', '_stats_py', '_stats_pythran', '_survival', '_tukeylambda_stats', '_unuran', '_variation', '_warnings_errors', '_wilcoxon', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betanbinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'bws_test', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'directional_stats', 'dirichlet', 'dirichlet_multinomial', 'distributions', 'dlaplace', 'dunnett', 'dweibull', 'ecdf', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expectile', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'false_discovery_control', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gmean', 'gompertz', 'goodness_of_fit', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'jf_skew_t', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logrank', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'poisson_means_test', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'quantile_test', 'randint', 'random_correlation', 'random_table', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'rel_breitwigner', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'sampling', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'sobol_indices', 'somersd', 'spearmanr', 'special_ortho_group', 'stats', 'studentized_range', 't', 'test', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncpareto', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'uniform_direction', 'unitary_group', 'variation', 'vonmises', 'vonmises_fisher', 'vonmises_line', 'wald', 'wasserstein_distance', 'wasserstein_distance_nd', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ttest_rel in module scipy.stats._stats_py:\n",
      "\n",
      "ttest_rel(a, b, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
      "    Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
      "    \n",
      "    This is a test for the null hypothesis that two related or\n",
      "    repeated samples have identical average (expected) values.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a, b : array_like\n",
      "        The arrays must have the same shape.\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "        \n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "        Defines the alternative hypothesis.\n",
      "        The following options are available (default is 'two-sided'):\n",
      "        \n",
      "        * 'two-sided': the means of the distributions underlying the samples\n",
      "          are unequal.\n",
      "        * 'less': the mean of the distribution underlying the first sample\n",
      "          is less than the mean of the distribution underlying the second\n",
      "          sample.\n",
      "        * 'greater': the mean of the distribution underlying the first\n",
      "          sample is greater than the mean of the distribution underlying\n",
      "          the second sample.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : `~scipy.stats._result_classes.TtestResult`\n",
      "        An object with the following attributes:\n",
      "        \n",
      "        statistic : float or array\n",
      "            The t-statistic.\n",
      "        pvalue : float or array\n",
      "            The p-value associated with the given alternative.\n",
      "        df : float or array\n",
      "            The number of degrees of freedom used in calculation of the\n",
      "            t-statistic; this is one less than the size of the sample\n",
      "            (``a.shape[axis]``).\n",
      "        \n",
      "            .. versionadded:: 1.10.0\n",
      "        \n",
      "        The object also has the following method:\n",
      "        \n",
      "        confidence_interval(confidence_level=0.95)\n",
      "            Computes a confidence interval around the difference in\n",
      "            population means for the given confidence level.\n",
      "            The confidence interval is returned in a ``namedtuple`` with\n",
      "            fields `low` and `high`.\n",
      "        \n",
      "            .. versionadded:: 1.10.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Examples for use are scores of the same set of student in\n",
      "    different exams, or repeated sampling from the same units. The\n",
      "    test measures whether the average score differs significantly\n",
      "    across samples (e.g. exams). If we observe a large p-value, for\n",
      "    example greater than 0.05 or 0.1 then we cannot reject the null\n",
      "    hypothesis of identical average scores. If the p-value is smaller\n",
      "    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n",
      "    hypothesis of equal averages. Small p-values are associated with\n",
      "    large t-statistics.\n",
      "    \n",
      "    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n",
      "    standard error. Therefore, the t-statistic will be positive when the sample\n",
      "    mean of ``a - b`` is greater than zero and negative when the sample mean of\n",
      "    ``a - b`` is less than zero.\n",
      "    \n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "    \n",
      "    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "    >>> stats.ttest_rel(rvs1, rvs2)\n",
      "    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n",
      "    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n",
      "    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "    >>> stats.ttest_rel(rvs1, rvs3)\n",
      "    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "print(help(scipy.stats.ttest_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n\u001b[0;32m----> 2\u001b[0m table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcrosstab(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(chi2_contingency(table))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n",
      "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/mini/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "table = pd.crosstab(df['1'], df['2'])\n",
    "#print(chi2_contingency(table))\n",
    "\n",
    "from scipy import stats\n",
    "model = stats.ttest_rel(df['1']. df['2'],alternative=greater / less )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../csv/mtcars.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24283687943262405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[['mpg']] = scaler.fit_transform(df[['mpg']])\n",
    "reuslt = df['mpg'] > 0.5\n",
    "#print(sum(reuslt))\n",
    "result2 = df.groupby('gear')['mpg'].mean()\n",
    "a = abs(result2.loc[3] - reuslt.loc[4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "q1 = df['mpg'].quantile(0.25)\n",
    "q3 = df['mpg'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "down = q1 - 1.5 * iqr\n",
    "up = q3 + 1.5 * iqr\n",
    "\n",
    "out = df[(df['mpg'] < down) | (df['mpg'] > up)]\n",
    "print(out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "q1 = df['mpg'].quantile(0.25)\n",
    "q3 = df['mpg'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "do = q1 - 1.5 * iqr\n",
    "up = q3 + 1.5 * iqr\n",
    "\n",
    "out = df[(df['mpg'] < do) | (df['mpg'] > up)]\n",
    "print(out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', 'Covariance', 'DegenerateDataWarning', 'FitError', 'MonteCarloMethod', 'NearConstantInputWarning', 'PermutationMethod', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_ansari_swilk_statistics', '_axis_nan_policy', '_biasedurn', '_binned_statistic', '_binomtest', '_boost', '_bws_test', '_censored_data', '_common', '_constants', '_continuous_distns', '_covariance', '_crosstab', '_discrete_distns', '_distn_infrastructure', '_distr_params', '_entropy', '_fit', '_hypotests', '_kde', '_ksstats', '_levy_stable', '_mannwhitneyu', '_morestats', '_mstats_basic', '_mstats_extras', '_multicomp', '_multivariate', '_mvn', '_odds_ratio', '_page_trend_test', '_qmc', '_qmc_cy', '_qmvnt', '_rcont', '_relative_risk', '_resampling', '_rvs_sampling', '_sampling', '_sensitivity_analysis', '_sobol', '_stats', '_stats_mstats_common', '_stats_py', '_stats_pythran', '_survival', '_tukeylambda_stats', '_unuran', '_variation', '_warnings_errors', '_wilcoxon', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betanbinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'bws_test', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'directional_stats', 'dirichlet', 'dirichlet_multinomial', 'distributions', 'dlaplace', 'dunnett', 'dweibull', 'ecdf', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expectile', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'false_discovery_control', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gmean', 'gompertz', 'goodness_of_fit', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'jf_skew_t', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logrank', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'poisson_means_test', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'quantile_test', 'randint', 'random_correlation', 'random_table', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'rel_breitwigner', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'sampling', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'sobol_indices', 'somersd', 'spearmanr', 'special_ortho_group', 'stats', 'studentized_range', 't', 'test', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncpareto', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'uniform_direction', 'unitary_group', 'variation', 'vonmises', 'vonmises_fisher', 'vonmises_line', 'wald', 'wasserstein_distance', 'wasserstein_distance_nd', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "print(dir(scipy.stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', 'Covariance', 'DegenerateDataWarning', 'FitError', 'MonteCarloMethod', 'NearConstantInputWarning', 'PermutationMethod', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_ansari_swilk_statistics', '_axis_nan_policy', '_biasedurn', '_binned_statistic', '_binomtest', '_boost', '_bws_test', '_censored_data', '_common', '_constants', '_continuous_distns', '_covariance', '_crosstab', '_discrete_distns', '_distn_infrastructure', '_distr_params', '_entropy', '_fit', '_hypotests', '_kde', '_ksstats', '_levy_stable', '_mannwhitneyu', '_morestats', '_mstats_basic', '_mstats_extras', '_multicomp', '_multivariate', '_mvn', '_odds_ratio', '_page_trend_test', '_qmc', '_qmc_cy', '_qmvnt', '_rcont', '_relative_risk', '_resampling', '_rvs_sampling', '_sampling', '_sensitivity_analysis', '_sobol', '_stats', '_stats_mstats_common', '_stats_py', '_stats_pythran', '_survival', '_tukeylambda_stats', '_unuran', '_variation', '_warnings_errors', '_wilcoxon', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betanbinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'bws_test', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'directional_stats', 'dirichlet', 'dirichlet_multinomial', 'distributions', 'dlaplace', 'dunnett', 'dweibull', 'ecdf', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expectile', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'false_discovery_control', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gmean', 'gompertz', 'goodness_of_fit', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'jf_skew_t', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logrank', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'poisson_means_test', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'quantile_test', 'randint', 'random_correlation', 'random_table', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'rel_breitwigner', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'sampling', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'sobol_indices', 'somersd', 'spearmanr', 'special_ortho_group', 'stats', 'studentized_range', 't', 'test', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncpareto', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'uniform_direction', 'unitary_group', 'variation', 'vonmises', 'vonmises_fisher', 'vonmises_line', 'wald', 'wasserstein_distance', 'wasserstein_distance_nd', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
