 모델에 일반화에 한계적
-> 많은 데이터셋에 처리가 필요했음. 

Gradient Vanishing

---
Tash함수는 Sigmoid보다 개선된 전통적인 비선형 함수지만, ReLU에 비해 학습속도와 성능에서 불리해서 저자들은 ReLU를 선택함
#### 1. 포화 비선형 함수 (Saturating Non-linear Functions)
- 정의: 입력값이 특정 범위(예: 매우 크거나 매우 작은 값)를 벗어날 때, 함수의 출력값이 특정 값(최대값 또는 최소값)에 수렴하여 변화율(기울기)이 거의 0에 가까워지는 함수입니다. 즉, "포화" 상태에 이른다는 의미입니다.
특징:
- 기울기 소실(Vanishing Gradient): 입력값이 포화 영역에 들어서면 기울기가 0에 가까워지기 때문에, 역전파(backpropagation) 과정에서 오차 기울기가 이전 계층으로 잘 전달되지 않아 가중치 업데이트가 거의 이루어지지 않습니다. 이는 특히 깊은 신경망에서 학습 속도를 매우 느리게 만들거나 학습을 멈추게 하는 기울기 소실 문제(vanishing gradient problem)를 야기합니다.
- 계산 복잡성: 지수 함수(exponential function) 등을 포함하여 계산 비용이 상대적으로 높을 수 있습니다.
```bash
예시:
- 시그모이드 (Sigmoid): f(x)=1/(1+e^x)
- 출력 범위: 0 ~ 1
- 양수 또는 음수의 큰 입력값에 대해 출력값이 1 또는 0에 포화됩니다.

하이퍼볼릭 탄젠트 (Tanh): f(x)=tanh(x)
- 출력 범위: -1 ~ 1.
- 양수 또는 음수의 큰 입력값에 대해 출력값이 1 또는 -1에 포화됩니다. 시그모이드보다 중심이 0에 맞춰져 있어 학습에 유리한 측면이 있지만, 여전히 포화 문제는 동일
```
#### 2. 비포화 비선형 함수 (Non-saturating Non-linear Functions)
- 정의: 입력값이 커질수록 함수의 출력값이 계속해서 증가하거나, 적어도 기울기가 0으로 수렴하지 않는 함수입니다.
**특징**:
- **기울기 소실 완화**: 포화 영역이 없거나 제한적이어서 기울기가 0으로 수렴하는 문제가 적습니다. 따라서 역전파 시 기울기가 효과적으로 전달되어 학습 속도가 빠릅니다.
```bash
ReLU (Rectified Linear Unit): f(x)=max(0,x)
- 출력 범위: 0 ~ 무한대.
- 양수 입력값에 대해서는 기울기가 항상 1 
음수 입력값에 대해서는 기울기가 0이지만, 최소한 양수 부분에서는 기울기 소실 문제가 발생하지 않는다.
- 저자들이 AlexNet에서 ReLU를 사용한 주된 이유가 tanh 유닛과 비교하여 훈련 속도가 몇 배 더 빠르기 때문이다. 
```
---

측면 억제란 어떤 뉴런이 활성화되면, 그 신경세포는 자신과 인접한 주변 신경세포들의 활성화를 억제하는 현상
-> 그럼 왜? 측면 억제가 발생하는가? 
- 어떻게 적용시켰나?


---
전통적인 pooling layer는 stride S와 window Z를 동일하게 설정했는데
저자들은 strid < window 조건을 도입했다.
저자들이 이렇게 도입한 이유에 대해서 어떻게 생각하나
-> 오버피팅이 있었지만 실험적으로는 오히려 안정적 일반화 성능을 보이나 
>중첩 풀링: `s < z` (겹치는 풀링)
- 예시: `z=3, s=2` (3x3 윈도우, 2칸씩 이동)
- 처음에 돋보기는 이미지의 왼쪽 위 (1,2,3,6,7,8,11,12,13)를 봅니다. (최대값 13)
- 돋보기가 오른쪽으로 2칸 점프합니다.
    - 이때 새로운 돋보기 영역은 (3,4,5,8,9,10,13,14,15)가 됩니다.
    - 여기서 중요한 것은 숫자 3, 8, 13이 이전 돋보기와 현재 돋보기 모두에 포함된다는 것입니다! (겹침)
```
[초기 위치] [오른쪽 2칸 이동 (겹침 발생!)] 
+---+---+---+---+---+      +---+---+---+---+------+ 
|***1|***2|***3| 4 | 5 |   | 1 | 2 |***3|***4|***5| 
|***6|***7|***8| 9 | 10|   | 6 | 7 |***8|***9|**10| 
|**11|**12|**13| 14| 15|   | 11| 12|**13|**14|**15| 
+---+---+---+---+---+      +---+---+---+---+------+ 
| 16| 17| 18| 19| 20|      | 16| 17| 18| 19| 20| 
+---+---+---+---+---+      +---+---+---+---+------+ 
| 21| 22| 23| 24| 25|      | 21| 22| 23| 24| 25| 
+---+---+---+---+---+      +---+---+---+---+------+
```



더크고 깊은 CNN이 어려운 데이터셋에도 높은 성능을 달성할 수 있음
Depths는 성능 향상에 핵심 역할
전체 파라미터의 1%도 안 되는 layer라도 성능에 큰 영향 -> 깊이의 중요성
네트워크 크기 증가 및 학습 시간 증가에 따라 성능은 지속적으로 향상
GPU와 데이터셋의 발전이 이루어진다면 추가적으로 성능 향상

Resnet 논문
알렉스넷 논문 


globallabs12
globalLabs12
- transform논문