교수님 상담

- 학부연구생 연구 할당
- 2025 과학기술대전에 포스터 게시/ Labeling Tool 개발 관련
- 졸업작품 주제/테마 피드백

- 야생동물 종구분 및 행동 구분을 위한 데이터 수집을 위한 라벨링 툴 


국립센터원 

개 / 고양이 행동분류 논문 확인
초당 몇프레임을 잘라서 확인해야할지
행동 분류에 반경을 어디까지 해야할지

[Label Box참고](https://labelbox.com/product/annotate/video/)
스틸컷으로 할거냐 
동영상/이미지 행동 분류에 관해 프레임에서 무엇을 하는지 확인


2개/ 3개 구분 해서 이미지 넣기 

---
# 야생동물 행동 분류 관련 연구 검토

야생 환경에서 촬영된 동물 영상 데이터를 활용하여 **동물의 행동을 자동 분류**하는 연구들을 조사하였다. 특히 카메라 트랩이나 모션 센서로 촬영된 영상에서 **프레임 샘플링(FPS)**, **행동 판단에 사용되는 공간적 맥락(반경)**, **스틸컷 vs 연속 프레임 기반 방식**, **행동 클래스의 구성(클래스 수)**에 주목하였다. 또한 LabelBox 등 어노테이션 도구 기반 워크플로우를 염두에 두어, **프레임 단위 정보 추출과 행동 분류 방법**을 설명하는 사례들을 선별하였다. 아래에서는 주요 연구들의 **Motivation(연구 동기)**, **Methodology(방법)**, **Results(결과)**를 정리하고, 끝에 각 연구의 FPS, 공간 반경, 분석 방식, 클래스 수를 표로 요약하였다.


---
## Schindler 등 (2021, 2024) – 사슴류 영상에서 객체 인스턴스 및 행동 인식

**Motivation:** Schindler와 Steinhage는 **야생동물(사슴류)의 영상에서 개체를 식별하고 행동을 인식**하는 **엔드투엔드 파이프라인**을 제안하였다​

[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=Action%20Detection%20for%20Wildlife%20Monitoring,Google)

. 사슴과 동물은 영상에서 여러 동작(예: 먹이 섭취, 이동, 경계 등)을 보이지만, 기존에는 행동별로 주석된 데이터가 부족하고 행동 인식 연구도 드물었다. 연구자들은 **고해상도(1728×1296) 카메라 트랩 영상**을 대량 확보하여, 딥러닝을 통해 **동물 검출과 행동 분류를 통합**하려는 동기를 가졌다​

[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=actions%20are%20present%20in%20the,21)

---

**Methodology:** 이 연구는 **두 단계의 딥러닝 모듈**로 구성되었다. 첫째, **인스턴스 세그멘테이션 및 추적 모듈**(예: Mask R-CNN 기반)이 영상 내 개별 동물을 프레임별로 분할(mask)하고 동일 개체의 연속 프레임을 **트랙(**track**)**으로 연결한다. 둘째, **행동 인식 모듈**(MAROON)은 추적된 개체의 영상 조각을 입력으로 받아 해당 개체의 행동 클래스를 분류한다. 행동 인식은 **연속된 프레임(sequence) 기반**으로 수행되며, 빠른 행동도 포착하기 위해 원본 영상을 **높은 FPS (30fps)**로 사용한다​

[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=ID%20and%20an%20action%20label,evaluated%20the%20effectiveness%20of%20our)

---

. 다만 FPS가 낮은 야간 영상의 경우에는 **8장의 프레임으로 구성된 짧은 클립**만을 사용하도록 제한하여 학습하였다. 어노테이션은 **VGG Image Annotator (VIA)** 등을 활용해 **프레임 단위로 마스크, 바운딩 박스, 개체 ID, 행동 레이블**을 부여하는 방식으로 진행되었다. 이때 행동 인식 모듈은 **세그멘테이션 마스크로 추출된 개체 영역**만을 사용하여, **개체 주변의 한정된 공간 영역** 내 정보를 바탕으로 행동을 파악한다.

**Results:** 자체 구축한 **야생 사슴 영상 데이터셋** 3종으로 실험한 결과, 제안한 시스템은 **다중 개체 상황에서도 개별 동물의 11가지 행동**을 성공적으로 인식해냈다​

[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=These%20videos%20are%20all%20captured,21)

---

. 예를 들어, Wildpark Rolandseck 공원에서 촬영된 주간 영상(30fps) 33개에는 **foraging (먹이 섭취)**, **grooming (털 손질)**, **running (달리기)** 등 **11개 행동 클래스**가 정의되었고, Bavarian Forest 국립공원의 주간/야간 영상(15fps/8fps)에는 **7개 행동 클래스**를 정의하여 모델을 평가하였다​

[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=These%20videos%20are%20all%20captured,21)

---

. 낮은 FPS의 야간 영상에서도 본 기법이 행동 인식에 성공하여, **색상 정보나 FPS가 낮아도 행동 분류가 가능함**을 보였다. 다만 희귀 행동의 데이터 부족으로 일부 클래스에 성능 편차가 있었고, **프레임 수를 늘리면 성능 향상이 가능**함을 언급하였다.

---
## Chan 등 (2025) – YOLO-Behaviour: 다양한 동물 행동의 자동 분류 프레임워크

**Motivation:** Chan 등은 **다양한 종의 동물 행동을 최소한의 수작업으로 자동 정량화**하기 위해 **YOLO-Behaviour 프레임워크**를 개발하였다. 행동 비디오를 일일이 사람이 주석하는 것은 많은 시간과 주관적 편차의 문제가 있으므로, **범용적인 객체 검출기(YOLO)를 행동 분류에 활용**하여 연구자 정의 행동을 빠르게 검출하는 것이 목표였다. 이들은 새 둥지에서의 먹이주기, 설치류의 먹이 섭취, 인간의 식사 행동, 비둘기 및 초식동물(얼룩말, 기린 등의 KABR 데이터) 행동 등 **다섯 가지 사례 연구**를 통해 **다양한 환경**에 적용 가능한 **유연한 행동 인식 파이프라인**을 제시하고자 했다.

**Methodology:** YOLO-Behaviour는 기본적으로 **YOLOv8 객체 검출 신경망**을 **행동 검출기로 변형**하여 사용한다. 구체적으로, **영상의 매 프레임마다 동물의 바운딩 박스**를 검출하면서 동시에 해당 프레임의 **행동 클래스**를 예측한다. 이를 위해 행동을 일종의 객체 클래스로 간주하여 YOLO 모델을 학습시키며, 비교적 **짧은 구간(예: 3초 길이)**의 영상을 처리한다. YOLO 검출은 **프레임별(스틸컷 기반)으로 이루어지며**, 별도의 순차적 메모리(RNN 등)를 사용하지 않고도 연속 프레임에서 행동을 포착할 수 있도록 하였다. 대신 후처리로 **연속 프레임에서 공간적으로 근접한 행동 감지 결과를 하나의 행동 이벤트로 묶는 알고리즘**을 적용하여​
, **동일 개체의 지속적인 행동**은 하나의 이벤트로 식별한다. 이렇게 하면 개체가 움직여도 **일정 반경 내에서 같은 행동이면 하나의 행동으로 간주**하고, 일시적인 오검출은 걸러내도록 설계되었다. 전체 프레임 시퀀스 대신 **프레임별 검출 기반**이므로, LabelBox 등의 도구에서 **프레임 단위로 바운딩 박스와 행동 레이블을 붙인 데이터**로도 학습이 가능하여 실용성을 높였다.

---

**Results:** 제안된 YOLO-Behaviour 프레임워크는 **여러 종에 걸쳐 안정적인 행동 검출 성능**을 보였다. 5가지 시나리오 각각에서 **정밀도-재현율 조화 평균(F1)**이 **0.62에서 0.94** 사이로 나타나 수작업 주석과 유사한 정확도를 달성했다. 예를 들어 **참새 둥지 새끼 먹이주기**의 경우 부모 개체의 방문 횟수를 자동 산출했고, **시베리아어치의 먹이 먹기**와 **사람의 식사 행동** 사례에서는 **“먹음” vs “안 먹음”**의 **이진 분류**를 높은 신뢰도로 수행하였다. 한편 **얼룩말·기린 행동 데이터셋(KABR)**에 대해서는 **걷기, 뜯어먹기, 뛰기 등 7가지 행동**에 대한 프레임별 예측을 실시하여, DeepEthogram 등의 영상 시퀀스 기반 모델과 유사한 수준의 정확도를 보였다. 흥미롭게도 YOLO-Behaviour는 **DeepEthogram 대비 추론 속도가 빨라**(초당 52.4프레임 vs 39.5프레임) 실시간 응용에 유리하였다. 이는 **YOLO 기반이 순차 처리 없이도 행동을 포착**한 덕분이며, 다만 복잡한 행동이 동시 발생하는 경우에는 프레임 단위 예측의 한계가 있을 수 있음을 지적하였다.


---
## Gabeff 등 (2025) – MammAlps: 멀티뷰 야생동물 행동 데이터셋 및 벤치마크

**Motivation:** Gabeff 등은 **알프스 산악지대의 야생 포유류에 대한 종합적인 행동 분석 데이터셋**인 **MammAlps**를 구축하였다​

[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=Monitoring%20wildlife%20is%20essential%20for,5%20hours)

. **9대의 카메라 트랩**을 이용해 6주간 수집한 장시간 영상을 통해, **다양한 종(사슴, 여우, 늑대, 산토끼 등)**의 행동 양상을 대규모로 기록하였다​

[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=The%20curated%20tracks%20include%20five,because%20individuals%20were%20too%20small)

. 기존에는 야생에서 촬영된 장시간 멀티뷰 영상에 행동 레이블을 붙인 자료가 없어 **딥러닝 기반 행동 분석 연구에 제약**이 있었기 때문에, 이러한 데이터를 제공하고 **멀티모달(영상+오디오) 행동 인식 벤치마크**를 제안하는 것이 주요 동기였다​

.

**Methodology:** MammAlps에서는 우선 **카메라 트랩 설정**을 표준화하여 **주간 1–2분, 야간 20초 고정 길이 영상**을 얻었다​

---

. LabelBox와 유사한 도구인 **CVAT**을 활용하여, 수집된 영상을 **이벤트 단위로 잘라내고 개체별 트랙을 구축**하였다​
. 구체적으로, **딥러닝 동물탐지기(MegaDetector)**를 사용해 **매 5프레임마다** 동물을 확인하고, 동물이 포착된 구간만 추출한 뒤 **모든 프레임**으로 검출을 촘촘히 수행하여 **연속적인 개체 바운딩 박스 시퀀스(트랙렛)**를 얻었다​

---
. 트래킹에는 ByteTrack 알고리즘을 개선한 기법을 적용하여, 연속 프레임에서 **바운딩 박스가 겹치지 않아도 일정 거리 이내(GIoU 기준)면 동일 개체로 간주**하도록 하였다​
. 이후 트랙마다 주변 배경을 패딩하여 **개체 중심의 정사각형 영상 클립**을 만들고, 이를 **VideoMAE 비디오 변환기 모델** 등 최신 비디오 인식 모델로 학습시켜 **행동 분류**를 수행하였다​
. VideoMAE 기반 벤치마크 모델은 **클립 당 16프레임**을 샘플링하여 학습하도록 설정되어, 프레임 시퀀스 기반으로 동작을 인식한다​

---

. 행동 레이블 체계는 전문가의 **에소그램(ethogram)**을 참고하여 정의되었으며, **계층적**으로 행동을 구분하였다 (상위 **활동 종류**와 하위 **행동 동작**).

**Results:** MammAlps 데이터셋에는 **5종의 야생동물**에 대해 **약 8.5시간 분량의 트랙렛 영상**이 **프레임 단위로 밀도 있게 행동 주석**되었다​
. 정의된 **행동 클래스는 10가지 이상**으로, 예를 들어 **Foraging(먹이 탐색)**, **Resting(휴식)**, **Escaping(도망)**, **Playing(장난)**, **Vigilance(경계)** 등 다양한 범주의 행동이 포함되었다​
. 벤치마크 실험에서 **멀티모달 비디오 인식 모델**은 종과 행동을 함께 예측하는 복합 과제를 수행하여 **평균 mAP 40% 수준**의 성능을 보였고​

---

, 영상+오디오 결합 시 정확도가 향상됨을 확인하였다. 이는 야생동물 행동 인식이 여전히 어려운 과제이지만, **기존 모델로도 어느 정도 일반화 성능을 기대할 수 있음**을 보여준다. 또한 멀티뷰 카메라를 활용한 **장기간 이벤트 요약** 등 생태학적 활용을 위한 2차 벤치마크를 제안하여, **향후 딥러닝 모델이 개별 행동 검출을 넘어 사건 수준의 요약까지 발전**할 수 있음을 논의하였다.

---
## 주요 연구 비교 요약

위에서 소개한 주요 연구들을 FPS, 공간 맥락, 프레임 활용 방식, 클래스 구성 측면에서 표로 요약하면 다음과 같다:

|연구 (출처)|사용 FPS 및 샘플링|공간 반경 고려 방식|분석 방식 (스틸컷 vs 시퀀스)|행동 클래스 수|
|---|---|---|---|---|
|Schindler 등 (2021​<br><br>[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=ID%20and%20an%20action%20label,evaluated%20the%20effectiveness%20of%20our)<br><br>, 2024)|- 원본 FPS 사용 (데이터셋별 30fps, 15fps, 8fps)<br/>- 낮은 FPS 야간영상은 8프레임 클립으로 제한|- **개체 세그멘테이션 마스크** 및 바운딩 박스로 개체 주변 국소영역 초점<br/>- 다수 개체 영상에서도 **개체별로 분리**하여 행동 파악|- **연속 프레임 기반** (예: 몇 프레임 시퀀스로 행동 인식)<br/>- 마스크된 개체 영상들을 시간 순서로 입력|- 데이터셋별 **7–11개 클래스** 정의​<br><br>[mdpi.com](https://www.mdpi.com/2076-3417/14/2/514#:~:text=These%20videos%20are%20all%20captured,21)<br><br><br/>- (예: 걷기, 달리기, 먹기, 휴식 등 다양)|
|Chan 등 (2025) YOLO-Behaviour|- 원본 FPS (대부분 약 30fps) 영상 사용<br/>- 3초 등 짧은 클립 단위로 처리 (90프레임)|- **바운딩 박스** 간 거리 기반 **이벤트 묶기** 적용​<br><br>[besjournals.onlinelibrary.wiley.com](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14502#:~:text=,event%2C%20and%20short%20incorrect)<br><br><br/>- 인접 프레임에서 **위치가 가까운 동일 행동**은 하나로 간주|- **스틸컷 기반** 프레임별 검출 (YOLO)<br/>- 후처리로 연속성 고려 (하지만 모델 자체는 순차 정보 미사용)|- 사례별 **2개에서 최대 7개** 클래스<br/>- (예: 먹는 vs 안먹는 이진 분류부터<br/>      얼룩말 행동 7종 분류 등)|
|Gabeff 등 (2025) MammAlps|- 카메라 트랩 원본 FPS (약 30fps 추정)<br/>- 전처리로 **5프레임 간격** 검출 후 관심 구간은 **전 프레임 사용**​<br><br>[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=We%20used%20MegaDetector%C2%A0,obtain%20dense%20animal%20detection%20predictions)|- **트래킹** 단계에서 GIoU 기반 **바운딩 박스 연결**​<br><br>[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=ByteTrack%C2%A0,area%20difference%20matching%20cost%20to)<br><br><br/>- 트랙 생성 후 **개체 주변 패딩**하여 맥락 유지|- **연속 프레임 기반** (클립당 16프레임 샘플)​<br><br>[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=several%20modifications%20so%20that%20the,It%20needs%20to%20be%20noted)<br><br><br/>- 멀티모달 입력 (영상+오디오+환경정보) 지원|- **10종 이상의 세분화된 행동** 정의​<br><br>[arxiv.org](https://arxiv.org/html/2503.18223v1#:~:text=Resting%20standing%20head%20up%2C%20bathing%2C,an%20animal%20or%20a%20group)<br><br><br/>- (예: Foraging, Escaping, Playing, Vigilance 등)|

각 연구는 **FPS 조절**이나 **프레임 샘플링 전략**, **공간적 컨텍스트 활용** 방식에서 차이가 있으나, 전반적으로 **행동 분류 정확도 향상**을 위해 적절한 프레임 수를 사용하고(너무 낮은 FPS 보완, 충분한 연속프레임 활용 등), **개체 중심의 지역 정보**에 초점을 맞추면서도 **연속적인 시간 정보**를 고려하려는 추세를 보인다. 또한 **행동 클래스 구성**은 연구 목적에 따라 달랐는데, **이진 분류에서부터 10여 개의 세분 행동 분류까지** 다양하게 설정되었다. 이러한 최신 연구 동향은 **LabelBox** 등으로 프레임 단위 주석을 달아 구축한 데이터도 효과적으로 활용 가능함을 보여주며, 향후 야생동물 행동 연구에 **표준화된 워크플로우**와 **딥러닝 모델**을 적용하는 데에 중요한 참고가 된다.