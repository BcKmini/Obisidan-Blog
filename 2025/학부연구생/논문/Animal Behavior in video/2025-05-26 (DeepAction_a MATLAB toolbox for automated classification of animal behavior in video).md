#### [ê¹ƒí—ˆë¸Œ ì°¸ê³ ](https://github.com/carlwharris/DeepAction?tab=readme-ov-file)

### ğŸ“„ ì›ë¬¸ ì œëª©

**DeepAction: a MATLAB toolbox for automated classification of animal behavior in video**

---

### 1. ì´ˆë¡ (Abstract)

> The identification of animal behavior in video is a critical but time-consuming task in many areas of research.  
> **ë¹„ë””ì˜¤ì—ì„œ ë™ë¬¼ì˜ í–‰ë™ì„ ì‹ë³„í•˜ëŠ” ê²ƒì€ ë§ì€ ì—°êµ¬ ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•˜ì§€ë§Œ ì‹œê°„ ì†Œëª¨ì ì¸ ì‘ì—…ì´ë‹¤.**

> Here, we introduce DeepAction, a deep learning-based toolbox for automatically annotating animal behavior in video.  
> **ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¹„ë””ì˜¤ì—ì„œ ë™ë¬¼ í–‰ë™ì„ ìë™ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ íˆ´ë°•ìŠ¤ì¸ DeepActionì„ ì†Œê°œí•œë‹¤.**

> Our approach uses features extracted from raw video frames by a pretrained convolutional neural network to train a recurrent neural network classifier.  
> **ì´ ë°©ë²•ì€ ì‚¬ì „ í•™ìŠµëœ í•©ì„±ê³± ì‹ ê²½ë§(CNN)ìœ¼ë¡œë¶€í„° ì›ì‹œ ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì‹ ê²½ë§(RNN) ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.**

> We evaluate the classifier on two benchmark rodent datasets and one octopus dataset.  
> **ì´ ë¶„ë¥˜ê¸°ëŠ” ë‘ ê°œì˜ ì„¤ì¹˜ë¥˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ê³¼ í•˜ë‚˜ì˜ ë¬¸ì–´ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë˜ì—ˆë‹¤.**

> We show that it achieves high accuracy, requires little training data, and surpasses both human agreement and most comparable existing methods.  
> **ê·¸ ê²°ê³¼ ì´ ëª¨ë¸ì€ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì ì€ í•™ìŠµ ë°ì´í„°ë¡œë„ ì¸ê°„ ê°„ ì¼ì¹˜ë„ ë° ê¸°ì¡´ ëŒ€ë¶€ë¶„ì˜ ê¸°ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

> We also create a confidence score for classifier output, and show that our method provides an accurate estimate of classifier performance and reduces the time required by human annotators to review and correct automatically-produced annotations.  
> **ë˜í•œ ë¶„ë¥˜ê¸°ì˜ ì¶œë ¥ì— ëŒ€í•´ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë„ì…í•˜ì˜€ìœ¼ë©°, ì´ ë°©ë²•ì€ ë¶„ë¥˜ê¸° ì„±ëŠ¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆê³ , ì‚¬ëŒì´ ìë™ ì£¼ì„ì„ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¤„ì—¬ì¤€ë‹¤.**

> We release our system and accompanying annotation interface as an open-source MATLAB toolbox.  
> **ì´ ì‹œìŠ¤í…œê³¼ í•¨ê»˜ ì œê³µë˜ëŠ” ì£¼ì„ ì¸í„°í˜ì´ìŠ¤ëŠ” ì˜¤í”ˆì†ŒìŠ¤ MATLAB íˆ´ë°•ìŠ¤ë¡œ ì œê³µëœë‹¤.**

---

### 2. ì„œë¡  (Introduction)

> The classification and analysis of animal behavior in video is a ubiquitous but often laborious process in life sciences research.  
> **ë¹„ë””ì˜¤ì—ì„œ ë™ë¬¼ì˜ í–‰ë™ì„ ë¶„ë¥˜í•˜ê³  ë¶„ì„í•˜ëŠ” ì¼ì€ ìƒëª…ê³¼í•™ ì—°êµ¬ì—ì„œ í”í•˜ì§€ë§Œ, ëŒ€ê°œëŠ” ë§¤ìš° í˜ë“  ê³¼ì •ì´ë‹¤.**

> Traditionally, such analyses have been performed manually.  
> **ì „í†µì ìœ¼ë¡œ ì´ëŸ¬í•œ ë¶„ì„ì€ ìˆ˜ì‘ì—…ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ ì™”ë‹¤.**

> This approach, however, suffers from several limitations.  
> **í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ì‹ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ í•œê³„ê°€ ì¡´ì¬í•œë‹¤.**

> Most obvious is that it requires researchers to allocate much of their time to the tedious work of behavioral annotation, limiting or slowing the progress of downstream analyses.  
> **ê°€ì¥ ëª…í™•í•œ ë¬¸ì œëŠ” ì—°êµ¬ìë“¤ì´ í–‰ë™ ì£¼ì„ì´ë¼ëŠ” ì§€ë£¨í•œ ì‘ì—…ì— ìƒë‹¹í•œ ì‹œê°„ì„ íˆ¬ì…í•´ì•¼ í•˜ë©°, ì´ëŠ” ì´í›„ ë¶„ì„ì˜ ì§„í–‰ì„ ì§€ì—°ì‹œí‚¤ê±°ë‚˜ ë°©í•´í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.**

> Particularly for labs without research assistants or paid annotators, the opportunity cost of annotating video can be quite high.  
> **íŠ¹íˆ ì—°êµ¬ ë³´ì¡°ì›ì´ë‚˜ ìœ ê¸‰ ì£¼ì„ìê°€ ì—†ëŠ” ì—°êµ¬ì‹¤ì—ì„œëŠ” ì˜ìƒ ì£¼ì„ ì‘ì—…ì˜ ê¸°íšŒë¹„ìš©ì´ ë§¤ìš° ë†’ë‹¤.**

> Manual annotation also suffers from relatively poor reproducibility and reliability, largely due to the limited attentional capacity of human annotators.  
> **ìˆ˜ì‘ì—… ì£¼ì„ì€ ì‚¬ëŒì˜ ì£¼ì˜ë ¥ í•œê³„ë¡œ ì¸í•´ ì¬í˜„ì„±ê³¼ ì‹ ë¢°ì„±ì´ ë‚®ë‹¤ëŠ” ë¬¸ì œë„ ìˆë‹¤.**

> This issue is particularly salient in studies involving rodents.  
> **ì´ ë¬¸ì œëŠ” ì„¤ì¹˜ë¥˜ë¥¼ ë‹¤ë£¨ëŠ” ì—°êµ¬ì—ì„œ íŠ¹íˆ ë‘ë“œëŸ¬ì§„ë‹¤.**

> Due to their nocturnal nature, rodents are preferably studied under dimmed or infrared light, which makes the identification of behaviors more difficult due to more limited light and color cues.  
> **ì„¤ì¹˜ë¥˜ëŠ” ì•¼í–‰ì„± ë™ë¬¼ì´ê¸° ë•Œë¬¸ì— ì–´ë‘ìš´ í™˜ê²½ì´ë‚˜ ì ì™¸ì„  ì¡°ëª… ì•„ë˜ì—ì„œ ê´€ì°°ë˜ëŠ”ë°, ì´ë¡œ ì¸í•´ ë¹›ê³¼ ìƒ‰ì— ëŒ€í•œ ì •ë³´ê°€ ë¶€ì¡±í•´ í–‰ë™ì„ ì‹ë³„í•˜ê¸°ê°€ ë” ì–´ë ¤ì›Œì§„ë‹¤.**

> This, in turn, increases annotatorsâ€™ fatigue and reduces their capacity to pay attention for extended periods, introducing variation in annotation quality, thereby decreasing the quality of behavioral data.  
> **ì´ë¡œ ì¸í•´ ì£¼ì„ìì˜ í”¼ë¡œë„ê°€ ë†’ì•„ì§€ê³  ì¥ì‹œê°„ ì§‘ì¤‘í•˜ëŠ” ëŠ¥ë ¥ì´ ë–¨ì–´ì§€ë©°, ì£¼ì„ í’ˆì§ˆì— ë³€ë™ì„±ì´ ìƒê²¨ ê²°êµ­ í–‰ë™ ë°ì´í„°ì˜ í’ˆì§ˆë„ ì €í•˜ëœë‹¤.**

> Given the time and accuracy limitations of manual annotation, increasing work has focused on creating methods to automate the annotation process.  
> **ì´ëŸ¬í•œ ì‹œê°„ì Â·ì •í™•ì„± í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ìµœê·¼ì—ëŠ” ì£¼ì„ ê³¼ì •ì„ ìë™í™”í•˜ëŠ” ë°©ë²• ê°œë°œì— ë§ì€ ë…¸ë ¥ì´ ê¸°ìš¸ì—¬ì§€ê³  ìˆë‹¤.**

> Many such methods rely on tracking animalsâ€™ bodies or body parts, from which higher-level features (e.g., velocity, acceleration, and posture) are extrapolated and used to classify behavior.  
> **ì´ëŸ¬í•œ ë°©ë²• ì¤‘ ë‹¤ìˆ˜ëŠ” ë™ë¬¼ì˜ ì „ì²´ ëª¸ì´ë‚˜ ì‹ ì²´ ë¶€ìœ„ë¥¼ ì¶”ì í•˜ê³ , ì´ë¡œë¶€í„° ì†ë„, ê°€ì†ë„, ìì„¸ ë“±ì˜ ê³ ì°¨ì› íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ í–‰ë™ì„ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ë‹¤.**

> However, approaches using these â€œhand-crafted featuresâ€ are limited in several ways.  
> **í•˜ì§€ë§Œ ì´ëŸ¬í•œ â€˜ìˆ˜ì‘ì—… ì„¤ê³„ëœ íŠ¹ì§•(hand-crafted features)â€™ ê¸°ë°˜ ì ‘ê·¼ì€ ì—¬ëŸ¬ í•œê³„ë¥¼ ê°€ì§„ë‹¤.**

> First, they require that researchers identify sets of features that both encompass a given animalâ€™s entire behavioral repertoire and can distinguish between visually similar behaviors.  
> **ì²« ë²ˆì§¸ë¡œ, ì—°êµ¬ìëŠ” í•´ë‹¹ ë™ë¬¼ì˜ ì „ì²´ í–‰ë™ ë ˆí¼í† ë¦¬ë¥¼ í¬ê´„í•˜ë©° ë™ì‹œì— ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ í–‰ë™ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” íŠ¹ì§• ì§‘í•©ì„ ìˆ˜ë™ìœ¼ë¡œ ì •ì˜í•´ì•¼ í•œë‹¤.**

> For example, â€œeatingâ€ and â€œgrooming snoutâ€ behaviors in rodents do not have a well-defined difference in posture or movement, making crafting features to differentiate them difficult.  
> **ì˜ˆë¥¼ ë“¤ì–´ ì„¤ì¹˜ë¥˜ì˜ â€˜ë¨¹ê¸°â€™ì™€ â€˜ì½” ì†ì§ˆ(grooming snout)â€™ í–‰ë™ì€ ìì„¸ë‚˜ ì›€ì§ì„ ë©´ì—ì„œ ëª…í™•í•œ ì°¨ì´ê°€ ì—†ê¸° ë•Œë¬¸ì— ì´ë“¤ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ì„ ì •ì˜í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤.**

> Second, after features have been selected, detecting and tracking them is difficult and imperfect.  
> **ë‘ ë²ˆì§¸ë¡œ, ì¼ë‹¨ íŠ¹ì§•ì´ ì •í•´ì§€ê³  ë‚˜ë©´ ê·¸ê²ƒì„ ë¹„ë””ì˜¤ì—ì„œ ì •í™•í•˜ê²Œ ê²€ì¶œí•˜ê³  ì¶”ì í•˜ëŠ” ê²ƒ ìì²´ë„ ì–´ë µê³  ì˜¤ë¥˜ê°€ ìƒê¸°ê¸° ì‰½ë‹¤.**

> Subtle changes in video illumination, animal movement, and environment can result in inaccurate keypoint detection, decreasing the fidelity of extracted features.  
> **ì˜ìƒì˜ ë¯¸ì„¸í•œ ì¡°ëª… ë³€í™”, ë™ë¬¼ì˜ ì›€ì§ì„, í™˜ê²½ ì¡°ê±´ì˜ ë³€í™” ë“±ìœ¼ë¡œ ì¸í•´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì •í™•ë„ê°€ ë–¨ì–´ì§€ê³ , ì´ì— ë”°ë¼ ì¶”ì¶œëœ íŠ¹ì§•ì˜ ì •í™•ì„±ë„ ì €í•˜ëœë‹¤.**

> And third, selected feature sets are often experiment-specific.  
> **ì…‹ì§¸, ì •ì˜ëœ íŠ¹ì§• ì§‘í•©ì€ ì¢…ì¢… íŠ¹ì • ì‹¤í—˜ì— ì¢…ì†ì ì´ë¼ëŠ” ì ì´ë‹¤.**

> Those optimal for a singly housed rodent study, for example, likely differ from those optimal for a social rodent study.  
> **ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ì¼ ì„¤ì¹˜ë¥˜ í–‰ë™ ë¶„ì„ì— ì í•©í•œ íŠ¹ì§• ì§‘í•©ì€ ì‚¬íšŒì  ì„¤ì¹˜ë¥˜ í–‰ë™ ì—°êµ¬ì—ëŠ” ë§ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.**

> This increases the complexity of the feature-selection task, impeding experimental progress and annotation accuracy.  
> **ì´ë¡œ ì¸í•´ íŠ¹ì§• ì„ íƒ ì‘ì—…ì˜ ë³µì¡ì„±ì´ ë†’ì•„ì§€ê³ , ê²°ê³¼ì ìœ¼ë¡œ ì‹¤í—˜ì˜ ì§„í–‰ê³¼ ì£¼ì„ì˜ ì •í™•ì„±ì„ ì €í•´í•œë‹¤.**

---
### 3. ê¸°ì¡´ ë°©ë²•ì˜ í•œê³„ì™€ DeepActionì˜ ì œì•ˆ

> To address these limitations, Bohnslav et al. proposed an alternative to hand-crafted approaches, instead using hidden two-stream networks and temporal gaussian mixture networks, and achieved high classification accuracy on a diverse collection of animal behavior datasets.  
> **ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Bohnslav ì™¸ ì—°êµ¬ì§„ì€ ìˆ˜ì‘ì—… ê¸°ë°˜ ì ‘ê·¼ ëŒ€ì‹  ìˆ¨ê²¨ì§„ ì´ì¤‘ ìŠ¤íŠ¸ë¦¼ ë„¤íŠ¸ì›Œí¬(hidden two-stream networks)ì™€ ì‹œê°„ ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(temporal Gaussian mixture networks)ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë™ë¬¼ í–‰ë™ ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œ ë°” ìˆë‹¤.**

> Here, we expand on this work by introducing DeepAction, a MATLAB toolbox for the automated annotation of animal behavior in video.  
> **ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ì¡´ ì—°êµ¬ë¥¼ í™•ì¥í•˜ì—¬, ë¹„ë””ì˜¤ì—ì„œ ë™ë¬¼ í–‰ë™ì„ ìë™ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” MATLAB ê¸°ë°˜ íˆ´ë°•ìŠ¤ì¸ DeepActionì„ ì œì•ˆí•œë‹¤.**

> Our approach utilizes a two-stream convolutional and recurrent neural network architecture to generate behavioral labels from raw video frames.  
> **DeepActionì€ ì›ì‹œ ë¹„ë””ì˜¤ í”„ë ˆì„ìœ¼ë¡œë¶€í„° í–‰ë™ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì´ì¤‘ ìŠ¤íŠ¸ë¦¼(two-stream) ë°©ì‹ì˜ CNN ë° RNN ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•œë‹¤.**

---

## 4. ë°©ë²•ë¡  ê°œìš” (Method Overview)

> We use convolutional neural networks (CNNs) and dense optical flow to extract spatial and temporal features from video, which are then used to train a long short-term memory network classifier to predict behavior.  
> **CNNê³¼ ë°€ì§‘ ì˜µí‹°ì»¬ í”Œë¡œìš°(Dense Optical Flow)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ë¡œë¶€í„° ê³µê°„ì (spatial) ë° ì‹œê°„ì (temporal) íŠ¹ì§•ì„ ì¶”ì¶œí•œ í›„, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LSTM(Long Short-Term Memory) ê¸°ë°˜ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œì¼œ í–‰ë™ì„ ì˜ˆì¸¡í•œë‹¤.**

> We evaluate our approach on two benchmark datasets of laboratory mouse video and one dataset of octopus video.  
> **ì´ ì ‘ê·¼ë²•ì€ ì‹¤í—˜ì‹¤ ìƒì¥ ë¹„ë””ì˜¤ë¡œ êµ¬ì„±ëœ ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ê³¼ ë¬¸ì–´ ë¹„ë””ì˜¤ë¡œ êµ¬ì„±ëœ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë˜ì—ˆë‹¤.**

> We show that it outperforms existing methods and reaches human-level performance with little training data.  
> **ë³¸ ëª¨ë¸ì€ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ë©°, ì ì€ ì–‘ì˜ í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œë„ ì¸ê°„ ìˆ˜ì¤€ì˜ ì •í™•ë„ì— ë„ë‹¬í•¨ì„ ë³´ì¸ë‹¤.**

---

> In addition to outputting behavior labels for each video frame, we also introduce a classification confidence system that generates a measure of how â€œconfidentâ€ the classifier is about each label.  
> **í”„ë ˆì„ ë‹¨ìœ„ë¡œ í–‰ë™ ë ˆì´ë¸”ì„ ì¶œë ¥í•˜ëŠ” ê²ƒ ì™¸ì—ë„, ê° ì˜ˆì¸¡ì— ëŒ€í•´ ë¶„ë¥˜ê¸°ì˜ â€œí™•ì‹ ë„(confidence)â€ë¥¼ ì¸¡ì •í•˜ëŠ” ë¶„ë¥˜ ì‹ ë¢°ë„ ì‹œìŠ¤í…œì„ ë„ì…í•˜ì˜€ë‹¤.**

> This allows researchers to estimate the quality of automatically-produced annotations without having to review them, and reduces the time required to review annotations by allowing users to selectively correct ambiguous ones, while omitting those that the classifier produced with high confidence.  
> **ì´ ì‹œìŠ¤í…œì„ í†µí•´ ì—°êµ¬ìë“¤ì€ ìë™ ìƒì„±ëœ ì£¼ì„ì˜ í’ˆì§ˆì„ ì§ì ‘ ê²€í† í•˜ì§€ ì•Šê³ ë„ ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì‹ ë¢°ë„ê°€ ë‚®ì€ ì£¼ì„ë§Œ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³ , ì‹ ë¢°ë„ê°€ ë†’ì€ ì£¼ì„ì€ ìƒëµí•¨ìœ¼ë¡œì¨ ì „ì²´ ê²€í†  ì‹œê°„ë„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.**

> We show that this confidence score accurately differentiates low quality annotations from high quality ones and improves the efficiency of reviewing and correcting video.  
> **ì´ ì‹ ë¢°ë„ ì ìˆ˜ëŠ” ë‚®ì€ í’ˆì§ˆì˜ ì£¼ì„ê³¼ ë†’ì€ í’ˆì§ˆì˜ ì£¼ì„ì„ ì •í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìœ¼ë©°, ë¹„ë””ì˜¤ ì£¼ì„ ê²€í†  ë° êµì •ì˜ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•œë‹¤.**

> Finally, we release the code and annotation GUI as an open-source MATLAB project.  
> **ë§ˆì§€ë§‰ìœ¼ë¡œ, ë³¸ ì½”ë“œ ë° ì£¼ì„ ì¸í„°í˜ì´ìŠ¤ GUIëŠ” ì˜¤í”ˆì†ŒìŠ¤ MATLAB í”„ë¡œì íŠ¸ë¡œ ê³µê°œëœë‹¤.**

---

## 5. DeepAction ì „ì²´ ì›Œí¬í”Œë¡œìš° ìš”ì•½ (p.3)

ë…¼ë¬¸ ê·¸ë¦¼ 1A~1Eë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **[A] í”„ë¡œì íŠ¸ íë¦„**:
    
    1. í”„ë¡œì íŠ¸ ìƒì„±
        
    2. ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ (ê³µê°„ + ì‹œê°„ í”„ë ˆì„)
        
    3. CNN ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
        
    4. í›ˆë ¨ í´ë¦½ ì„ íƒ
        
    5. í´ë¦½ ìˆ˜ë™ ì£¼ì„
        
    6. ë¶„ë¥˜ê¸° í›ˆë ¨
        
    7. í‰ê°€
        
    8. ì „ì²´ ìë™ ì£¼ì„
        
    9. ì‹ ë¢°ë„ ê¸°ë°˜ ê²€í† 
        
    10. ìµœì¢… ì£¼ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        
- **[B] í´ë¦½ ë¶„í•  ê³¼ì •**:  
    ì „ì²´ ì˜ìƒì„ ì¼ì • ê¸¸ì´ì˜ í´ë¦½ìœ¼ë¡œ ë¶„í• í•˜ê³  ì¼ë¶€ëŠ” ì£¼ì„ì„ ìœ„í•´ ë¬´ì‘ìœ„ë¡œ ì„ íƒ  
    â†’ ìˆ˜ë™ ì£¼ì„ â†’ í•´ë‹¹ ì£¼ì„ê³¼ CNN íŠ¹ì§•ì„ ì—°ê²°í•˜ì—¬ ë¶„ë¥˜ê¸° í•™ìŠµ
    
- **[C] í•™ìŠµ ë°ì´í„° êµ¬ì„±**:  
    ìˆ˜ë™ ì£¼ì„ëœ ë°ì´í„°ëŠ” í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ë¡œ ë‚˜ë‰˜ë©° í›ˆë ¨ì€ ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ ìˆ˜í–‰
    
- **[D] ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ì„¤ê³„**:  
    ì „ì²´ ë°ì´í„°ì…‹ì„ ë¬´ì‘ìœ„ë¡œ ì¼ì • ë¹„ìœ¨ë§Œ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•´ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    
- **[E] íŠ¹ì§• ì¶”ì¶œ êµ¬ì¡°**:
    
    - Raw video â†’ Optical flowë¡œ ì‹œê°„ í”„ë ˆì„ ìƒì„±
        
    - ResNet18ì„ í™œìš©í•´ ê³µê°„ì /ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        
    - íŠ¹ì§• ê²°í•© í›„ ì°¨ì› ì¶•ì†Œ (1024 â†’ 512)

### 6. ì‹¤í—˜ ë°ì´í„°ì…‹ (Datasets)

> In our primary analyses, we evaluate our approach on two publicly available â€œbenchmarkâ€ datasets of mice in a laboratory setting.  
> **ë³¸ ë…¼ë¬¸ì˜ ì£¼ìš” ì‹¤í—˜ì—ì„œëŠ” ê³µê°œëœ ì„¤ì¹˜ë¥˜ í–‰ë™ ë°ì´í„°ì…‹ ë‘ ê°€ì§€ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.**

> The first dataset, referred to as the â€œhome-cage dataset,â€ was collected by Jhuang et al., and features 12 videos (10.5 h total) of singly housed mice in their home cages performing eight stereotypical, mutually-exclusive behaviors recorded from the side of the cage.  
> **ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì€ â€œhome-cage ë°ì´í„°ì…‹â€ìœ¼ë¡œ Jhuang ì™¸ ì—°êµ¬ì§„ì´ ìˆ˜ì§‘í•œ ìë£Œì´ë©°, ë‹¨ë… ì‚¬ìœ¡ëœ ìƒì¥ê°€ ì¼€ì´ì§€ ì•ˆì—ì„œ ìˆ˜í–‰í•œ 8ê°€ì§€ ìƒí˜¸ ë°°íƒ€ì  í–‰ë™ì„ ì¸¡ë©´ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•œ 12ê°œ ì˜ìƒ(ì´ 10.5ì‹œê°„)ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.**

> The second dataset, called â€œCRIM13,â€ consists of 237 pairs of videos, recorded with synchronized side and top views, of pairs of mice engaging in social behavior, categorized into 13 distinct, mutually-exclusive actions.  
> **ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ì€ â€œCRIM13â€ìœ¼ë¡œ, ìƒì¥ 2ë§ˆë¦¬ì˜ ì‚¬íšŒì  í–‰ë™ì„ ë™ê¸°í™”ëœ ì¸¡ë©´ ë° ìƒë‹¨ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•œ 237ìŒì˜ ë¹„ë””ì˜¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ 13ê°œì˜ ìƒí˜¸ ë°°íƒ€ì  í–‰ë™ ë²”ì£¼ë¡œ ì£¼ì„ë˜ì–´ ìˆë‹¤.**

> In addition to these benchmark datasets, we challenge the classifier by evaluating it on an â€œexploratoryâ€ unpublished dataset of octopus bimaculoides behavior during a habituation task.  
> **ì´ ì™¸ì—ë„, ë¬¸ì–´(bimaculoides)ì˜ ìŠµê´€í™” í–‰ë™(habituation task)ì„ ê¸°ë¡í•œ ë¯¸ê³µê°œ íƒìƒ‰ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë³¸ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¶”ê°€ë¡œ í‰ê°€í•˜ì˜€ë‹¤.**

---

### 7. í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ê³¼ ì •í™•ë„ (Performance vs. Training Size)

> We first evaluate the performance of the classifier (i.e., accuracy and F1) with varying amounts of training data, and show that it requires remarkably little manual annotation to achieve high accuracy.  
> **ë¶„ë¥˜ê¸°ì˜ ì •í™•ë„ì™€ F1 ì ìˆ˜ë¥¼ í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¼ ì¸¡ì •í•œ ê²°ê³¼, ì ì€ ì–‘ì˜ ìˆ˜ë™ ì£¼ì„ë§Œìœ¼ë¡œë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì´ ë‚˜íƒ€ë‚¬ë‹¤.**

- Home-cage ë°ì´í„°ì…‹ì˜ ê²½ìš°:
    
    - **ë‹¨ 10~20% ì •ë„ì˜ ì£¼ì„ ë°ì´í„°**ë§Œ ì‚¬ìš©í•´ë„ **ì •í™•ë„ 75~79%** ë„ë‹¬
        
    - ì¸ê°„ ì£¼ì„ì ê°„ ì¼ì¹˜ë„ëŠ” ì•½ 71.6% ìˆ˜ì¤€
        

> For a given proportion labeled, a corresponding proportion of project clips are randomly selected from all the clips in the dataset and used to train the classifier.  
> **ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ì£¼ì–´ì§„ ë¹„ìœ¨ë§Œí¼ì˜ í´ë¦½ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•´ ì£¼ì„í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œì¼°ë‹¤.**

> On the home-cage dataset, in addition to showing higher accuracy than the agreement between human annotators, our classifier outperforms existing commercial options and methods using hand-crafted features or 3D convolutional networks.  
> **home-cage ë°ì´í„°ì…‹ì—ì„œëŠ”, ë³¸ ë¶„ë¥˜ê¸°ëŠ” ì¸ê°„ ì£¼ì„ì ê°„ ì¼ì¹˜ë„ë¥¼ ë„˜ì—ˆì„ ë¿ ì•„ë‹ˆë¼, ìƒìš© ì†Œí”„íŠ¸ì›¨ì–´ ë° ìˆ˜ì‘ì—… í”¼ì²˜ ê¸°ë°˜ ë˜ëŠ” 3D CNN ê¸°ë°˜ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ë„ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤.**

> On the CRIM13 dataset, it also surpasses prior methods and even performs better than human agreement.  
> **CRIM13 ë°ì´í„°ì…‹ì—ì„œë„ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ì˜€ê³ , ì¸ê°„ ì£¼ì„ì ê°„ ì¼ì¹˜ë„(69.7%)ë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„(73.9%)ë¥¼ ê¸°ë¡í•˜ì˜€ë‹¤.**

### 8. ê¸°ì¡´ ë°©ë²•ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ

ë…¼ë¬¸ Table 1ì— ì •ë¦¬ëœ ì •í™•ë„ ë¹„êµí‘œ (ìš”ì•½):

| ëª¨ë¸                | ì •í™•ë„ (%)  |
| ----------------- | -------- |
| Human (home-cage) | 71.6     |
| CleverSys ìƒìš© ì‹œìŠ¤í…œ  | 61.0     |
| Jhuang et al.     | 78.3     |
| DeepAction (ours) | **79.5** |
| Jiang HMM (ìµœê³ )    | 81.5     |
| CRIM13 Human      | 69.7     |
| Burgos-Artizzu    | 62.6     |
| DeepAction (ours) | **73.9** |

### 9. í´ë¦½ ê¸¸ì´ ë° ì…ë ¥ ë°©ì‹ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

> We hypothesized that our classifier shows superior performance when it is trained using relatively short clips rather than longer ones, given equal annotation time.  
> **ê°™ì€ ì‹œê°„ ë™ì•ˆ ì£¼ì„ì´ ì´ë£¨ì–´ì¡Œì„ ë•Œ, ì§§ì€ í´ë¦½ ë‹¨ìœ„ë¡œ í•™ìŠµí•œ ë¶„ë¥˜ê¸°ê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ë¼ ê°€ì •í•˜ì˜€ë‹¤.**

> This is indeed the case.  
> **ì‹¤ì œë¡œ ì‹¤í—˜ ê²°ê³¼ë„ ì´ë¥¼ ë’·ë°›ì¹¨í•˜ì˜€ë‹¤.**

- **ì§§ì€ í´ë¦½(1~2ë¶„)** ë‹¨ìœ„ë¡œ í•™ìŠµí• ìˆ˜ë¡ ë” ë¹ ë¥´ê²Œ ì •í™•ë„ê°€ ìƒìŠ¹
    
- ê¸´ í´ë¦½ì€ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë‚®ê³ , ë³€í™” ê°ì§€ì— ì·¨ì•½
    

> The CRIM13 dataset is recorded using synchronized top and side cameras.  
> **CRIM13ì€ ìƒë‹¨/ì¸¡ë©´ ë‘ ê°œì˜ ë™ê¸°í™”ëœ ì¹´ë©”ë¼ë¡œ ì´¬ì˜ë˜ì—ˆë‹¤.**

> The classifier trained using features from both views demonstrates superior performance.  
> **ë‘ ì‹œì ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ ê²°í•©í•˜ì—¬ í•™ìŠµí•œ ë¶„ë¥˜ê¸°ê°€ ë‹¨ì¼ ì‹œì ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ë‹¤.**

---

### 10. í–‰ë™ í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì°¨ì´ (Behavior-wise Performance)

> In the home-cage dataset, except for the â€œdrinkâ€ behavior (0.26% of labels), the classifier shows consistently high performance.  
> **home-cage ë°ì´í„°ì…‹ì—ì„œëŠ” â€˜ë§ˆì‹œê¸°(drink)â€™ í–‰ë™ì„ ì œì™¸í•˜ë©´ ëª¨ë“  í–‰ë™ì—ì„œ ì¼ê´€ë˜ê²Œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

- â€˜rest(íœ´ì‹)â€™: ë§¤ìš° ë†’ì€ ì •í™•ë„ (ì •ë°€ë„ 0.98, ì¬í˜„ìœ¨ 0.95)
    
- â€˜drinkâ€™: ë§¤ìš° ë‚®ì€ ìƒ˜í”Œ ìˆ˜ë¡œ ì¸í•´ ë¶ˆì•ˆì •
    

> In the CRIM13 dataset, â€œotherâ€ class dominates the dataset and causes imbalance.  
> **CRIM13ì—ì„œëŠ” 'other'(ë¹„ì‚¬íšŒì  í–‰ë™) í´ë˜ìŠ¤ê°€ ê³¼ë„í•˜ê²Œ ë§ì•„ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì¡´ì¬í•œë‹¤.**
> This causes the classifier to overpredict â€œotherâ€ and mislabel some social behaviors.  
> **ì´ë¡œ ì¸í•´ ë¶„ë¥˜ê¸°ê°€ 'other'ë¡œ ê³¼ë‹¤ ì˜ˆì¸¡í•˜ì—¬ ì¼ë¶€ ì‚¬íšŒì  í–‰ë™ì„ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí–ˆë‹¤.**

---

### 11. ë¬¸ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ (Exploratory Octopus Dataset)

> On the exploratory dataset, we evaluated the classifier on a six-behavior dataset of seven octopus bimaculoides behavior videos collected in-house.  
> **íƒìƒ‰ì  ì‹¤í—˜ì—ì„œëŠ”, ì‹¤í—˜ì‹¤ì—ì„œ ìˆ˜ì§‘í•œ 7ê°œì˜ ë¬¸ì–´(bimaculoides) í–‰ë™ ë¹„ë””ì˜¤(ì´ 6ê°€ì§€ í–‰ë™ ë²”ì£¼)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤.**

> Overall, the classifier performs relatively well, with an accuracy of 73.1 percent.  
> **ì „ì²´ì ìœ¼ë¡œ ë¶„ë¥˜ê¸°ëŠ” ì•½ 73.1%ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©° ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

> This is much lower than human-level performance, however, given that manual annotators reached an agreement of 88.7 percent on the same, independently annotated video.  
> **í•˜ì§€ë§Œ ì´ ì„±ëŠ¥ì€ ì¸ê°„ ì£¼ì„ì ê°„ ì¼ì¹˜ë„(88.7%)ë³´ë‹¤ëŠ” ë‚®ì€ ìˆ˜ì¹˜ì´ë©°, ë™ì¼í•œ ì˜ìƒì„ ë‘ ë…ë¦½ ì£¼ì„ìê°€ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì£¼ì„í•œ ê²°ê³¼ì™€ ë¹„êµí•œ ê²ƒì´ë‹¤.**

> In terms of behavior-level performance, the classifier performs well on crawling, none (indicating behavior of interest) and fixed pattern, but poorly on relaxation, jetting, and expanding.  
> **í–‰ë™ ë²”ì£¼ë³„ë¡œ ë³´ë©´, â€˜ê¸°ì–´ê°€ê¸°(crawling)â€™, â€˜none(ê´€ì‹¬ í–‰ë™ ì—†ìŒ)â€™, â€˜fixed pattern(ê³ ì • íŒ¨í„´ ì´ë™)â€™ì—ì„œëŠ” ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, â€˜relaxation(ì´ì™„)â€™, â€˜jetting(ê¸‰ê°€ì† íšŒí”¼)â€™, â€˜expanding(íŒ”ë‹¤ë¦¬ ë²Œë¦¬ê¸°)â€™ì—ì„œëŠ” ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

> The poor performance on these behaviors is likely due to their infrequency, particularly in the case of jetting and expanding.  
> **ì´ëŸ¬í•œ ë‚®ì€ ì„±ëŠ¥ì€ í•´ë‹¹ í–‰ë™ì´ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ë§¤ìš° ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚¬ê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì´ë©°, íŠ¹íˆ â€˜jettingâ€™ê³¼ â€˜expandingâ€™ì€ ë§¤ìš° ë‚®ì€ ë¹ˆë„ë¡œ ë“±ì¥í•˜ì˜€ë‹¤.**

---

### 12. ë¶„ë¥˜ê¸° í–‰ë™ë³„ ì„±ëŠ¥ (Behavior-wise Metrics â€“ Home-Cage ê¸°ì¤€)

> To examine classifier performance as a function of the amount of data used to train it, we calculate the precision, recall, and F1 score for each behavior with varying labeled data proportions.  
> **í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„° ì–‘ì— ë”°ë¥¸ ë¶„ë¥˜ê¸° ì„±ëŠ¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ë¼ë²¨ë§ ë¹„ìœ¨ì— ëŒ€í•´ ê° í–‰ë™ë³„ ì •ë°€ë„(Precision), ì¬í˜„ìœ¨(Recall), F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì˜€ë‹¤.**

- **ì •ë°€ë„(Precision)**: ì˜ˆì¸¡í•œ í–‰ë™ ì¤‘ ì‹¤ì œë¡œ ë§ì€ ë¹„ìœ¨
    
- **ì¬í˜„ìœ¨(Recall)**: ì‹¤ì œ í–‰ë™ ì¤‘ ì˜ˆì¸¡ì´ ë§ì€ ë¹„ìœ¨
    
- **F1 ì ìˆ˜**: ë‘ ê°’ì˜ ì¡°í™” í‰ê· 
    

> In the home-cage dataset, for non-drinking behaviors, we observe a similar pattern in behavior-level improvement as we do to overall accuracyâ€”a rapid increase at low training data proportions, followed by a more gradual one.  
> **home-cage ë°ì´í„°ì—ì„œëŠ” â€˜drinkâ€™ë¥¼ ì œì™¸í•œ í–‰ë™ë“¤ì— ëŒ€í•´, ì „ì²´ ì •í™•ë„ì™€ ìœ ì‚¬í•œ íŒ¨í„´ì´ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ˆê¸° í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ì´ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ í–¥ìƒë˜ë‹¤ê°€ ì´í›„ ì™„ë§Œí•´ì§€ëŠ” ê²½í–¥ì„ ë³´ì˜€ë‹¤.**

> For drinking behavior, however, due to its exceptionally low incidence, we observe a more inconsistent, non-gradual improvement in performance across training set proportions.  
> **ë°˜ë©´ â€˜drinkingâ€™ í–‰ë™ì€ ë§¤ìš° ë‚®ì€ ì¶œí˜„ìœ¨ë¡œ ì¸í•´, í•™ìŠµ ë¹„ìœ¨ì´ ì¦ê°€í•¨ì— ë”°ë¼ ì„±ëŠ¥ í–¥ìƒì´ ì¼ì •í•˜ì§€ ì•Šê³  ë¶ˆê·œì¹™í•˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤.**

---

### 13. CRIM13ì—ì„œë„ ìœ ì‚¬í•œ ê²½í–¥ ì¡´ì¬

> This pattern generally applies in the CRIM13 dataset as well.  
> **ì´ëŸ¬í•œ íŒ¨í„´ì€ CRIM13 ë°ì´í„°ì…‹ì—ì„œë„ ì „ë°˜ì ìœ¼ë¡œ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤.**

> For most behaviors we observe a rapid increase in recall, precision, and F1, followed by a relative slowdown in improvement as a function of training proportion.  
> **ëŒ€ë¶€ë¶„ì˜ í–‰ë™ì—ì„œ í•™ìŠµ ë¹„ìœ¨ì´ ë‚®ì„ ë•ŒëŠ” ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ê°€ ê¸‰ê²©íˆ ìƒìŠ¹í–ˆìœ¼ë©°, ì´í›„ ì ì§„ì ìœ¼ë¡œ ì™„í™”ë˜ëŠ” ì–‘ìƒì„ ë³´ì˜€ë‹¤.**

> We observe that â€œeat,â€ â€œcircle,â€ and â€œdrinkâ€ show sporadic improvements in recall, precision, and F1.  
> **ê·¸ëŸ¬ë‚˜ â€˜eatâ€™, â€˜circleâ€™, â€˜drinkâ€™ ê°™ì€ í–‰ë™ì€ ë¶ˆê·œì¹™í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ëŠ”ë°, ì´ëŠ” ì´ë“¤ í–‰ë™ì´ ë§¤ìš° ë“œë¬¼ê¸° ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„ëœë‹¤.**

---

### 14. ì‹ ë¢°ë„ ì ìˆ˜ì™€ ë¶„ë¥˜ ì •í™•ë„ ê°„ì˜ ê´€ê³„ (Confidence Scores Predict Accuracy)

> We generate a confidence score for each clip that represents the classifierâ€™s prediction of the accuracy of its predicted labels.  
> **ë¶„ë¥˜ê¸°ê°€ ê° í´ë¦½ì— ëŒ€í•´ ìƒì„±í•œ í–‰ë™ ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°’ì„ â€˜ì‹ ë¢°ë„ ì ìˆ˜(confidence score)â€™ë¡œ ì •ì˜í•˜ì˜€ë‹¤.**

> We demonstrate that there is a strong correlation between confidence score and actual accuracy.  
> **ì‹ ë¢°ë„ ì ìˆ˜ì™€ ì‹¤ì œ ì •í™•ë„ ê°„ì—ëŠ” ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆìŒì„ í™•ì¸í•˜ì˜€ë‹¤.**

- ìƒê´€ë„ R2â‰ˆ0.45R^2 \approx 0.45R2â‰ˆ0.45 ìˆ˜ì¤€ â†’ ë¹„êµì  ì¼ê´€ëœ ì˜ˆì¸¡ ê°€ëŠ¥ì„±
    

> Confidence scores based on temperature scaling perform better than those using softmax probability.  
> **Softmax í™•ë¥  ê¸°ë°˜ ì‹ ë¢°ë„ ì ìˆ˜ë³´ë‹¤, Temperature Scaling ê¸°ë²•ìœ¼ë¡œ ê³„ì‚°ëœ ì‹ ë¢°ë„ ì ìˆ˜ê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

> While the softmax score consistently overestimates accuracy by 6â€“8%, temperature scaling is overconfident by only 1â€“2%.  
> **Softmax ë°©ì‹ì€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ 6~~8% ê³¼ëŒ€ ì¶”ì •í•˜ëŠ” ë°˜ë©´, Temperature Scalingì€ 1~~2% ì •ë„ë§Œ ê³¼ëŒ€ ì¶”ì •í•˜ì˜€ë‹¤.**

---

### 15. ì‹ ë¢°ë„ ì ìˆ˜ì˜ ì ˆëŒ€ì˜¤ì°¨(MAE) ë° í‰ê· í¸ì°¨(MSD)

> The MAE expresses the amount by which a randomly selected clipâ€™s confidence score differs from its actual accuracy.  
> **MAE(í‰ê·  ì ˆëŒ€ ì˜¤ì°¨)ëŠ” ì„ì˜ë¡œ ì„ íƒëœ í´ë¦½ì˜ ì‹ ë¢°ë„ ì ìˆ˜ê°€ ì‹¤ì œ ì •í™•ë„ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.**

> The MSD expresses systematic over- or under-confidence.  
> **MSD(í‰ê·  ë¶€í˜¸ ìˆëŠ” ì°¨ì´)ëŠ” ì‹œìŠ¤í…œì ìœ¼ë¡œ ê³¼ì‹ (ê³¼ëŒ€ ì˜ˆì¸¡) ë˜ëŠ” ê³¼ì†Œ ì˜ˆì¸¡ì´ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.**

- ê²°ê³¼:
    
    - Temperature Scalingì€ **ë‚®ì€ MAE**, **ë‚®ì€ MSD** â†’ ê°€ì¥ ì•ˆì •ì 

### 16. ì‹ ë¢°ë„ ê¸°ë°˜ ë¦¬ë·°ëŠ” ê²€í†  ì‹œê°„ì„ ì¤„ì¸ë‹¤ (Uncertaintyâ€‘based review reduces correction time)

> Having established the high correspondence between clip confidence score and clip accuracy, we investigate how well our confidence-based review system leverages those confidence scores to reduce the time it takes to review and correct classifier-produced labels.  
> **í´ë¦½ì˜ ì‹ ë¢°ë„ ì ìˆ˜ì™€ ì‹¤ì œ ì •í™•ë„ ê°„ì˜ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•œ í›„, ìš°ë¦¬ëŠ” ì´ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ìë™ ìƒì„±ëœ ì£¼ì„ì„ ê²€í† í•˜ê³  êµì •í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì–¼ë§ˆë‚˜ ì¤„ì¼ ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ì˜€ë‹¤.**

> A viable confidence measure would allow clips with a lower confidence score to be preferentially reviewed over those with a higher confidence score, decreasing the manual review time required to obtain acceptably high-quality annotations.  
> **ìœ íš¨í•œ ì‹ ë¢°ë„ ì§€í‘œëŠ” ì‹ ë¢°ë„ê°€ ë‚®ì€ í´ë¦½ì„ ìš°ì„ ì ìœ¼ë¡œ ê²€í† í•˜ê²Œ í•´ ì¤Œìœ¼ë¡œì¨, ì¼ì • ìˆ˜ì¤€ ì´ìƒì˜ í’ˆì§ˆì„ í™•ë³´í•˜ê¸° ìœ„í•œ ìˆ˜ì‘ì—… ê²€í†  ì‹œê°„ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆê²Œ í•œë‹¤.**

> Rather than reviewing all the classifier-produced labels, the user could instead review only a portion with the lowest accuracy.  
> **ì‚¬ìš©ìëŠ” ì „ì²´ ì£¼ì„ì„ ê²€í† í•˜ëŠ” ëŒ€ì‹ , ì •í™•ë„ê°€ ê°€ì¥ ë‚®ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì¼ë¶€ í´ë¦½ë§Œ ê²€í† í•¨ìœ¼ë¡œì¨ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.**

> We provide an example of this process in practice in Fig. 6G, which simulates the relationship between the proportion of test video reviewed and the overall accuracy of the labels in the test set.  
> **Fig. 6Gì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì¤‘ ë¦¬ë·°ëœ ë¹„ìœ¨ê³¼ ì „ì²´ ì£¼ì„ ì •í™•ë„ ê°„ì˜ ê´€ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ë³´ì—¬ì¤€ë‹¤.**

- ì‹ ë¢°ë„ ì •ë ¬ì„ ì´ìš©í•´ ë‚®ì€ ì ìˆ˜ë¶€í„° ê²€í† í•˜ë©´, **ì ì€ ê²€í† ëŸ‰ìœ¼ë¡œë„ ë†’ì€ ì •í™•ë„ í™•ë³´ ê°€ëŠ¥**
    
- ë¬´ì‘ìœ„ ê²€í†  ëŒ€ë¹„ íš¨ìœ¨ì„± ìš°ìˆ˜
    

---

### 17. ë¦¬ë·° íš¨ìœ¨ì„± ì •ëŸ‰í™” (Review Efficiency Metric)

> To compare the performance of the confidence-based review across labeled data proportions, we calculate a metric called â€œreview efficiency.â€  
> **í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ë³„ë¡œ ì‹ ë¢°ë„ ê¸°ë°˜ ë¦¬ë·°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ 'ë¦¬ë·° íš¨ìœ¨ì„±(review efficiency)'ì´ë¼ëŠ” ì§€í‘œë¥¼ ì •ì˜í•˜ì˜€ë‹¤.**

- **ì •ì˜**:
    $$
    ë¦¬ë·°Â íš¨ìœ¨ì„±= \frac{ì‹ ë¢°ë„Â ê¸°ë°˜Â ë¦¬ë·°Â ê°œì„ }{ì´ë¡ ì ìœ¼ë¡œÂ ê°€ëŠ¥í•œÂ ìµœëŒ€Â ê°œì„ ëŸ‰}
$$
> As shown in Fig. 6H, as the proportion of data labeled increases, both confidence scores become closer to optimal in sorting videos for review.  
> **Fig. 6H, 6Iì— ë”°ë¥´ë©´, í•™ìŠµëœ ë°ì´í„° ë¹„ìœ¨ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬ì´ ì´ìƒì ì¸ ì •ë ¬(ì‹¤ì œ ì •í™•ë„ ê¸°ì¤€)ê³¼ ê°€ê¹Œì›Œì§„ë‹¤.**

> The softmax- and temperature scaling-based scores perform approximately the same.  
> **Softmax ë°©ì‹ê³¼ Temperature Scaling ë°©ì‹ ëª¨ë‘ ë¦¬ë·° íš¨ìœ¨ì„±ì—ì„œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**

---

### 18. MATLAB GUIë¥¼ í†µí•œ ë¦¬ë·° ë° ì£¼ì„ ì‹œìŠ¤í…œ (Annotation GUI improves annotation and review)

> While we evaluate our method here using fully annotated datasets, the central purpose of this work is to improve the annotation of behavior in experimental settings.  
> **ë³¸ ì—°êµ¬ëŠ” ì™„ì „íˆ ì£¼ì„ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë˜ì—ˆì§€ë§Œ, ê¶ê·¹ì ì¸ ëª©ì ì€ ì‹¤ì œ ì‹¤í—˜ í™˜ê²½ì—ì„œì˜ í–‰ë™ ì£¼ì„ ê³¼ì •ì„ ê°œì„ í•˜ëŠ” ë° ìˆë‹¤.**

> For this reason, we release the entire system as a MATLAB toolbox as a GitHub repository that includes example projects and GUI interfaces.  
> **ì´ë¥¼ ìœ„í•´ ì „ì²´ ì‹œìŠ¤í…œì„ MATLAB ê¸°ë°˜ íˆ´ë°•ìŠ¤ë¡œ êµ¬í˜„í•˜ì—¬ GitHubì— ê³µê°œí•˜ì˜€ê³ , ì˜ˆì œ í”„ë¡œì íŠ¸ ë° GUIë„ í•¨ê»˜ ì œê³µí•œë‹¤.**

> We integrate clip-wise annotation by pre-dividing project videos into clips and presenting clips, rather than videos, for users to annotate.  
> **ì „ì²´ ë¹„ë””ì˜¤ê°€ ì•„ë‹Œ, í´ë¦½ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì£¼ì„í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ë‹¤.**

> In addition, we incorporate the confidence-based review process into the GUI: incomplete (i.e., unreviewed annotations) are shown in a table, with low-confidence clips appearing at the top.  
> **GUI ìƒì—ì„œ ì‹ ë¢°ë„ ê¸°ë°˜ ë¦¬ë·°ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ê²€í† ë˜ì§€ ì•Šì€ í´ë¦½ì€ í‘œë¡œ ì •ë¦¬ë˜ì–´ ë‚˜íƒ€ë‚˜ë©°, ì‹ ë¢°ë„ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìƒë‹¨ì— í‘œì‹œëœë‹¤.**

> Users can easily load videos, annotate them using the keyboard, add or remove behaviors, and export the results entirely within the GUI.  
> **ì‚¬ìš©ìëŠ” GUIì—ì„œ ë¹„ë””ì˜¤ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , í‚¤ë³´ë“œë¡œ í–‰ë™ ì£¼ì„ì„ ì¶”ê°€Â·ì‚­ì œí•˜ë©°, ì „ì²´ ê²°ê³¼ë¥¼ GUI ë‚´ì—ì„œ ì§ì ‘ ë‚´ë³´ë‚¼ ìˆ˜ ìˆë‹¤.**

---

## âœ… ë…¼ë¬¸ ê²°ë¡  ë° í† ë¡  (Discussion)

### 19. ìš”ì•½ ë° ì„±ê³¼ ì •ë¦¬

> Here we present a method for the automatic annotation of laboratory animal behavior from video.  
> **ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ì‹¤ ë™ë¬¼ì˜ ë¹„ë””ì˜¤ í–‰ë™ì„ ìë™ìœ¼ë¡œ ì£¼ì„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤.**

> Our classifier produces high accuracy annotations, rivaling or surpassing human-level agreement, while requiring relatively little human annotation time.  
> **ë³¸ ë¶„ë¥˜ê¸°ëŠ” ë¹„êµì  ì ì€ ìˆ˜ì˜ ìˆ˜ì‘ì—… ì£¼ì„ë§Œìœ¼ë¡œë„ ì¸ê°„ ìˆ˜ì¤€ ì´ìƒì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.**

> Our confidence scores accurately predict accuracy and are useful in reducing the time required for human annotators.  
> **ë„ì…ëœ ì‹ ë¢°ë„ ì ìˆ˜ëŠ” ì‹¤ì œ ì •í™•ë„ë¥¼ ì˜ ì˜ˆì¸¡í•˜ë©°, ì‚¬ëŒì˜ ë¦¬ë·° ì‹œê°„ë„ ì¤„ì´ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.**

---

### 20. ì¥ì  ìš”ì•½

- í–‰ë™ í‚¤í¬ì¸íŠ¸ë‚˜ ìˆ˜ì‘ì—… íŠ¹ì§• ì—†ì´ë„ ì›ì‹œ ì˜ìƒ í”„ë ˆì„ë§Œìœ¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥
    
- ì„¤ì¹˜ë¥˜ ì™¸ ë¬¸ì–´ì™€ ê°™ì€ ë¹„ì„¤ì¹˜ë¥˜ ëª¨ë¸ì—ì„œë„ ì¼ë°˜í™” ê°€ëŠ¥
    
- GUI ê¸°ë°˜ í™˜ê²½ ì œê³µ: ì‹¤ì œ ì‹¤í—˜ í™˜ê²½ì— ì‰½ê²Œ ì ìš© ê°€ëŠ¥
    
- ìˆ˜ì‘ì—… ì‹œê°„ **75~82% ì ˆê°** ê°€ëŠ¥ (Home-cage: 264ì‹œê°„ â†’ 47ì‹œê°„, CRIM13: 350ì‹œê°„ â†’ 88ì‹œê°„)
    

---

### 21. í–¥í›„ ê°œì„  ë°©í–¥

> While our clip selection process demonstrates superior performance to whole-video annotation, we currently select clips randomly.  
> **í˜„ì¬ëŠ” í´ë¦½ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì§€ë§Œ, ë” ë‚˜ì€ ì „ëµ(ì˜ˆ: í´ëŸ¬ìŠ¤í„°ë§, ëŒ€í‘œì„± ê¸°ë°˜ ì„ íƒ ë“±)ì´ ìˆì„ ìˆ˜ ìˆë‹¤.**

> It is possible that alternate architectures would demonstrate superior performance.  
> **BiLSTM ì™¸ì—ë„ ë‹¤ë¥¸ RNN êµ¬ì¡°ë‚˜ attention ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•˜ë©´ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.**

> The classifier described here assumes behaviors are mutually exclusive, but could be extended for co-occurring behaviors.  
> **í˜„ì¬ëŠ” ìƒí˜¸ ë°°íƒ€ì  í–‰ë™ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ì§€ë§Œ, ë³µìˆ˜ í–‰ë™ ë™ì‹œ ë°œìƒì„ ë‹¤ë£¨ë„ë¡ í™•ì¥í•  ìˆ˜ ìˆë‹¤.**

> Future versions might use density-based metrics or Bayesian dropout for better uncertainty estimation.  
> **ì•ìœ¼ë¡œëŠ” ë°€ë„ ê¸°ë°˜ ì§€í‘œë‚˜ Bayesian dropoutì„ í†µí•œ ë” ì •í™•í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.**

---
## 22. ë°ì´í„°ì…‹ (Datasets)

> Given that rodents are widely used in behavioral research, and mice are the most studied rodents, we chose two publicly-available datasets featuring mice engaging in a range of behaviors in our main analysis.  
> **í–‰ë™ ì—°êµ¬ì—ì„œ ì„¤ì¹˜ë¥˜ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ìƒì¥ê°€ ê°€ì¥ ë§ì´ ì—°êµ¬ë˜ëŠ” ì„¤ì¹˜ë¥˜ì´ê¸° ë•Œë¬¸ì—, ë³¸ ì—°êµ¬ì—ì„œëŠ” ìƒì¥ê°€ ë‹¤ì–‘í•œ í–‰ë™ì„ ìˆ˜í–‰í•˜ëŠ” ë‘ ê°€ì§€ ê³µê°œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.**

- **Home-cage dataset**:
    
    - Jhuang et al. ìˆ˜ì§‘
        
    - ë‹¨ì¼ ìƒì¥, ì¸¡ë©´ ì‹œì ì—ì„œ ì´¬ì˜
        
    - 12ê°œ ë¹„ë””ì˜¤ (10.5ì‹œê°„, 113ë§Œ í”„ë ˆì„)
        
    - 8ê°€ì§€ í–‰ë™ìœ¼ë¡œ ì£¼ì„ë¨
        
- **CRIM13 dataset**:
    
    - Burgos-Artizzu et al. ìˆ˜ì§‘
        
    - 237ìŒ ë¹„ë””ì˜¤ (ìƒë‹¨/ì¸¡ë©´ ë™ê¸°í™”)
        
    - 13ê°œ í–‰ë™ (12ê°œ ì‚¬íšŒ í–‰ë™ + ê¸°íƒ€)
        
- **Octopus dataset**:
    
    - ì—°êµ¬íŒ€ì´ ìì²´ ìˆ˜ì§‘
        
    - 7ê°œ ë¹„ë””ì˜¤ (ì´ 6.75ì‹œê°„, 6.15ì‹œê°„ ì£¼ì„ ì™„ë£Œ)
        
    - 6ê°œ í–‰ë™ (crawling, fixed pattern, jetting ë“±)
        

---

## 23. ì¸ê°„ ì£¼ì„ì ê°„ ì¼ì¹˜ë„ (Inter-observer Reliability)

> Both datasets include a set of annotations performed by two groups of annotators.  
> **ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ë‘ ëª…ì˜ ë…ë¦½ëœ ì£¼ì„ì ê·¸ë£¹ì´ ìˆ˜í–‰í•œ ì£¼ì„ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤.**

> The agreement between these two sets is used as a reference to benchmark classifier performance.  
> **ì´ë“¤ ê°„ì˜ ì¼ì¹˜ë„ëŠ” ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê¸°ì¤€ì„ ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.**

- **Home-cage**: 1.6ì‹œê°„ ë¶„ëŸ‰ì—ì„œ ì¸ê°„ ê°„ ì¼ì¹˜ë„ 78.3%
    
- **CRIM13**: 12ê°œ ë¹„ë””ì˜¤ì— ëŒ€í•´ 69.7%
    

---

## 24. ë¼ë²¨ë§ ì‹œë®¬ë ˆì´ì…˜ (Simulating Labeled Data)

> We simulate our approachâ€™s performance with varying amounts of training data.  
> **í•™ìŠµ ë°ì´í„° ì–‘ì´ ë‹¬ë¼ì§ˆ ë•Œ ì„±ëŠ¥ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í—˜ì„ ì„¤ê³„í•˜ì˜€ë‹¤.**

- í•™ìŠµ ë°ì´í„° ë¹„ìœ¨:
    
    proplabeled=[0.02:0.02:0.2,0.2:0.05:0.9]\text{proplabeled} = [0.02 : 0.02 : 0.2, 0.2 : 0.05 : 0.9]proplabeled=[0.02:0.02:0.2,0.2:0.05:0.9]
- ì˜ˆ: 2% ~ 90%ê¹Œì§€ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° 10íšŒ ë°˜ë³µ í‰ê°€ ìˆ˜í–‰
    

---

## 25. ê¸°ì¡´ ë°©ë²•ê³¼ì˜ ë¹„êµ ë°©ì‹ (Comparison with Existing Methods)

> For home-cage dataset, existing methods use â€œleave-one-outâ€ cross-validation.  
> **home-cage ë°ì´í„°ì…‹ì—ì„œëŠ” ê¸°ì¡´ ì—°êµ¬ë“¤ì´ í•œ ë¹„ë””ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‘ê³  ë‚˜ë¨¸ì§€ë¥¼ í•™ìŠµì— ì“°ëŠ” leave-one-out ë°©ì‹ ì‚¬ìš©**

- DeepActionì—ì„œëŠ” í´ë¦½ ë‹¨ìœ„ë¡œ 12-fold êµì°¨ê²€ì¦ ìˆ˜í–‰
    
- CRIM13ì—ì„œëŠ” ê¸°ì¡´ ì—°êµ¬ì—ì„œ 44% í•™ìŠµ, 56% í…ŒìŠ¤íŠ¸ â†’ DeepActionë„ 2-fold êµì°¨ê²€ì¦ìœ¼ë¡œ ë§ì¶¤
    

---

## 26. í”„ë ˆì„ ì¶”ì¶œ (Frame Extraction)

> To generate spatial frames, we extract raw video frames from each video file.  
> **ê³µê°„ í”„ë ˆì„ ìƒì„±ì„ ìœ„í•´ ë¹„ë””ì˜¤ì˜ ì›ì‹œ í”„ë ˆì„ì„ ì¶”ì¶œí•œë‹¤.**

> For temporal frames, we compute dense optical flow between adjacent frames using the TV-L1 algorithm.  
> **ì‹œê°„ í”„ë ˆì„ì€ TV-L1 ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ì¸ì ‘ í”„ë ˆì„ ê°„ ë°€ì§‘ ì˜µí‹°ì»¬ í”Œë¡œìš°ë¥¼ ê³„ì‚°í•´ ìƒì„±í•œë‹¤.**

- Optical flowëŠ” ìƒ‰ìœ¼ë¡œ ë°©í–¥/ì†ë„ ì •ë³´ í‘œí˜„
    
- TV-L1ì€ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ê³ ì „ ì•Œê³ ë¦¬ì¦˜
    

---

## 27. íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)

> We utilize the pretrained ResNet18 CNN to extract high-level features from spatial and temporal video frames.  
> **ê³µê°„/ì‹œê°„ í”„ë ˆì„ì—ì„œ ê³ ì°¨ì› ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ResNet18 CNNì„ ì‚¬ìš©í•˜ì˜€ë‹¤.**

- Spatial frame â†’ [224, 224, 3] â†’ CNN ì¶”ë¡ 
    
- Temporal frame â†’ ì£¼ë³€ 11ê°œ í”„ë ˆì„ ìŠ¤íƒ â†’ [224, 224, 33] ì…ë ¥
    
- CNNì˜ ë§ˆì§€ë§‰ global average pooling ì¸µì—ì„œ 512ì°¨ì› ë²¡í„° ì¶”ì¶œ
    

> For multiple cameras, we concatenate features across cameras as well.  
> **ë‹¤ì¤‘ ì¹´ë©”ë¼(ì˜ˆ: CRIM13)ì—ì„œëŠ” ì¹´ë©”ë¼ë³„ íŠ¹ì§•ì„ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•¨**

---

## 28. ì°¨ì› ì¶•ì†Œ (Dimensionality Reduction)

> We use reconstruction independent component analysis (ICA) to reduce the initial 1024/2048 features to 512.  
> **ì´ˆê¸° spatiotemporal íŠ¹ì§• ë²¡í„°(1024 ë˜ëŠ” 2048ì°¨ì›)ë¥¼ ì¬êµ¬ì„± ê°€ëŠ¥í•œ ë…ë¦½ ì„±ë¶„ ë¶„ì„(ICA)ìœ¼ë¡œ 512ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì˜€ë‹¤.**

- ì„±ëŠ¥ í–¥ìƒ ë° í•™ìŠµ íš¨ìœ¨ ê°œì„  ëª©ì 
    

---

## 29. ë¶„ë¥˜ê¸° êµ¬ì¡° (Classifier Architecture)

> We use a long short-term memory (LSTM) network with bidirectional layers (BiLSTM).  
> **Bidirectional LSTMì„ í™œìš©í•œ ìˆœí™˜ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.**

- ì…ë ¥: [n, 512] ì‹œí€€ìŠ¤ (í”„ë ˆì„ ìˆ˜ n, í”¼ì²˜ 512)
    
- êµ¬ì¡°:
    
    - BiLSTM â†’ Dropout â†’ BiLSTM â†’ Dropout
        
    - Fully connected â†’ Softmax â†’ Sequence classification
        

> To reduce overfitting, we apply dropout (rate: 50%) after each BiLSTM layer.  
> **ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ê° BiLSTM ì¸µ ë’¤ì— 50% ë“œë¡­ì•„ì›ƒ ì ìš©**


## 30. ë¶„ë¥˜ê¸° í•™ìŠµ (Classifier Training)

> Validation set is 20% of labeled clips.  
> **ë¼ë²¨ë§ëœ í´ë¦½ì˜ 20%ëŠ” ê²€ì¦(validation) ì„¸íŠ¸ë¡œ ì‚¬ìš©**

> Training stops if validation loss does not improve for two epochs.  
> **ê²€ì¦ ì†ì‹¤ì´ ë‘ epoch ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ**

| í•˜ì´í¼íŒŒë¼ë¯¸í„°   | ê°’             |
| --------- | ------------- |
| ìµœëŒ€ ì—í­     | 16            |
| ì´ˆê¸° í•™ìŠµë¥     | 0.001         |
| ë°°ì¹˜ í¬ê¸°     | 8             |
| í•™ìŠµë¥  ê°ì†Œ ì£¼ê¸° | 4 ì—í­ë§ˆë‹¤ 10ë°° ê°ì†Œ |
---
## 31. ë¶„ë¥˜ê¸° í‰ê°€ ë°©ì‹ (Classifier Evaluation)

> We evaluate the classifier on the test set by comparing its predicted labels to the ground-truth labels for each frame.  
> **í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê° í”„ë ˆì„ì— ëŒ€í•œ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ ë¶„ë¥˜ê¸°ë¥¼ í‰ê°€í•˜ì˜€ë‹¤.**

- **ì •í™•ë„(Accuracy)** = ì •í™•íˆ ì˜ˆì¸¡í•œ í”„ë ˆì„ ìˆ˜ / ì „ì²´ í”„ë ˆì„ ìˆ˜
    
- **ì •ë°€ë„(Precision)** = TP / (TP + FP)
    
- **ì¬í˜„ìœ¨(Recall)** = TP / (TP + FN)
    
- **F1 ì ìˆ˜** = 2 * (ì •ë°€ë„ * ì¬í˜„ìœ¨) / (ì •ë°€ë„ + ì¬í˜„ìœ¨)
    

> We calculate the average F1 score across all behavior classes.  
> **ëª¨ë“  í–‰ë™ í´ë˜ìŠ¤ì— ëŒ€í•´ F1 ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ê³ , ì´ë“¤ì˜ í‰ê· ê°’ì„ ì‚¬ìš©í•œë‹¤.**

---

## 32. ì‹ ë¢°ë„ ì ìˆ˜ ì •ì˜ (Confidence Score Definition)

> For each frame, we estimate the probability that the predicted label is correct.  
> **ê° í”„ë ˆì„ì— ëŒ€í•´ ì˜ˆì¸¡ ë ˆì´ë¸”ì´ ì •ë‹µì¼ í™•ë¥ ì„ ì‚°ì •í•œë‹¤.**

- í”„ë ˆì„ë³„ í™•ë¥ :
$$
p_j=P(\hat y_j = y_j)
$$    
- í´ë¦½ ì‹ ë¢°ë„ ì ìˆ˜(confidence)ëŠ” í•´ë‹¹ í´ë¦½ì˜ í”„ë ˆì„ë³„ í‰ê·  í™•ë¥ :
    $$
    conf(clipi)=\frac1N \sum_j^N= N p_jâ€‹
$$
---

## 33. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ë°©ì‹ (Confidence Score Calculation)

> We test two methods to calculate frame-level confidence: softmax-based and temperature scaling.  
> **í”„ë ˆì„ ìˆ˜ì¤€ì˜ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ softmax ê¸°ë°˜ ë°©ë²•ê³¼ temperature scaling ë°©ë²• ë‘ ê°€ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ì˜€ë‹¤.**

- **Softmax ê¸°ë°˜**:
    $$
    p_j^{SM} = \max_k \sigma(z_j)_kpj
    $$â€‹
    - ê°€ì¥ ë†’ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
    - ì¼ë°˜ì ìœ¼ë¡œ **ê³¼ì‹ (overconfident)** ê²½í–¥ì´ ìˆìŒ
        
- **Temperature scaling**:
    $$
  p_j^{TS} = \max k * \sigma\left(\frac{z_j}{T}\right)_k
    $$â€‹
    - TëŠ” ê²€ì¦ ì„¸íŠ¸ì—ì„œ ìµœì í™”ë¨
        
    - ê³¼ì‹ ë„ ê°ì†Œ íš¨ê³¼
        

---

## 34. ì‹ ë¢°ë„ ê¸°ë°˜ ë¦¬ë·° (Confidence-Based Review)

> The system reviews clips with the lowest confidence scores first.  
> **ì‹œìŠ¤í…œì€ ì‹ ë¢°ë„ê°€ ê°€ì¥ ë‚®ì€ í´ë¦½ë¶€í„° ìš°ì„ ì ìœ¼ë¡œ ê²€í† í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.**

- ì „ì²´ ì‹ ë¢°ë„ ê¸°ë°˜ ì •í™•ë„:
    $$
    conf(Dunlabeledâ€‹)=âˆ‘jâ€‹âˆ£clipjâ€‹âˆ£âˆ‘iâ€‹conf(clipiâ€‹)â‹…âˆ£clipiâ€‹âˆ£â€‹
    $$
    - ì´ ê°’ì´ ì „ì²´ ë¯¸ë¼ë²¨ë§ ë°ì´í„°ì˜ í‰ê·  ì •í™•ë„ ì¶”ì •ì¹˜ê°€ ëœë‹¤.
        

---

## 35. ë¦¬ë·° íš¨ìœ¨ì„± í‰ê°€ (Evaluating Review Efficiency)

> We simulate a scenario where a user only reviews a portion of the test set clips.  
> **ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ í´ë¦½ ì¤‘ ì¼ë¶€ë§Œ ê²€í† í•  ë•Œì˜ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•œë‹¤.**

- **ì •ì˜**:
    
    - ë¦¬ë·° í›„ ì •í™•ë„:
        $$
        acc(D_k) = \frac{|\text{reviewed}| + acc(\text{unreviewed}) \cdot |\text{unreviewed}|}{|D|}
        $$
    - **Review Efficiency**:
        $$
       {Efficiency} = \frac{\text{Improvement (ì‹ ë¢°ë„ ê¸°ë°˜)}}{\text{Improvement (ì´ë¡ ì  ìµœëŒ€)}}
        $$â€‹
