## **초록 (Abstract)** 
DeepEthogram은 원시 비디오 픽셀을 사용하여 관심 행동의 목록인 '행동상(ethogram)'을 생성하는 지도 기계 학습 소프트웨어입니다. 이 도구는 수동 행동 채점의 시간 소모성, 제한된 행동 수, 연구자 간의 변동성 문제를 해결하기 위해 고안되었습니다. 컨볼루션 신경망을 활용하여 움직임을 계산하고, 이미지 및 움직임에서 특징을 추출하며, 이를 행동으로 분류합니다. 쥐와 파리 비디오의 단일 프레임에서 90% 이상의 정확도를 달성하여 전문가 수준의 인간 성능과 일치합니다. 또한, DeepEthogram은 희귀 행동을 정확하게 예측하며, 적은 훈련 데이터만으로도 높은 일반화 능력을 보입니다. 그래픽 인터페이스(GUI)를 제공하여 사용자가 프로그래밍 없이 분석을 시작부터 끝까지 수행할 수 있도록 합니다.

## **도입 (Introduction)** 
동물 행동 분석은 신경과학 및 질병 모델 연구에서 중요한 접근 방식입니다. 전통적인 수동 행동 채점은 시간이 많이 걸리고, 연구할 수 있는 행동의 수가 제한되며, 연구자마다 결과가 달라지는 단점이 있습니다. 컴퓨터 비전은 이러한 문제를 해결할 잠재력을 가지고 있으며, DeepEthogram은 원시 픽셀에서 직접 행동을 분류하여 파이프라인을 단순화합니다. 기존의 지도 기계 학습 방법(JAABA, SimBA, MARS 등)은 주로 키포인트 추적을 통해 행동을 분류하지만 , DeepEthogram은 원시 픽셀을 직접 사용함으로써 전처리 과정이 필요 없고 다양한 종과 환경에 더 일반적으로 적용될 수 있습니다. 이 소프트웨어는 연구자들이 정의한 행동을 자동적이고 재현 가능하게 분류함으로써 행동 분석을 가속화하고 향상시킬 것을 목표로 합니다.

## 방법론

### 구현
|구분|도구/라이브러리|인용|버전/사양|용도|
|---|---|---|---|---|
|프로그래밍 언어|Python|Rossum et al., 2010|3.7 이상|구현 언어|
|딥러닝 프레임워크|PyTorch|Paszke, 2018|1.4.0 이상|모델 구현|
|훈련 프레임워크|PyTorch Lightning|Falcon, 2019|—|훈련 관리|
|이미지 I/O|OpenCV|Bradski, 2008|—|이미지 읽기/쓰기|
|GPU 기반 이미지 증강|Kornia|Riba et al., 2019|—|이미지 증강|
|평가 지표|scikit-learn|Pedregosa, 2021|—|메트릭 계산|
|사용자 정의 평가 코드|Python|—|—|추가 메트릭 구현|
|CNN 다이어그램 생성|PlotNeuralNet|Iqbal, 2018|—|그림 1 다이어그램 생성|
|기타 그림 생성|Matplotlib|Caswell, 2021|—|시각화|
|GPU 하드웨어|Nvidia GPU|—|GeForce 1080Ti, Titan RTX, Quadro RTX6000/8000|모델 훈련|
|추론 속도 평가 환경|시스템 사양|—|Ubuntu 18.04AMD Ryzen Threadripper 2950XSamsung 970 Evo HDD128 GB DDR4|속도 평가|

## 파이프라인 
- **흐름 생성기 (Flow Generator):** 비디오 프레임에서 움직임(광학 흐름)을 추정합니다. TinyMotionNet, MotionNet, 그리고 새로운 TinyMotionNet3D 아키텍처가 사용되며, Kinetics700 데이터셋으로 사전 훈련됩니다.
- **특징 추출기 (Feature Extractor):** 광학 흐름 및 단일 이미지에서 저차원 특징을 압축합니다. ResNet18, ResNet50, 3D ResNet34 모델이 사용되며, 이 또한 사전 훈련됩니다.
- **시퀀스 모델 (Sequence Model):** 압축된 특징의 시퀀스를 사용하여 각 프레임에서 행동의 확률을 예측합니다. Temporal Gaussian Mixture (TGM) 네트워크를 사용하며, 이는 긴 시간 스케일 정보를 활용합니다.
### 모델
- 모델 훈련은 흐름 생성기, 특징 추출기, 시퀀스 모델 순서로 순차적으로 진행되며, 이는 대규모 모델의 과적합 및 계산 자원 한계 문제를 완화하기 위함입니다. 모델의 견고성을 높이기 위해 이미지 밝기, 대비, 회전, 뒤집기 등 다양한 증강(augmentation) 기법을 적용합니다. 희귀 행동을 정확하게 식별하기 위해 초점 이진 손실(focal binary loss)과 클래스 불균형에 대한 바이어스 초기화를 사용했습니다. 훈련 및 추론 속도는 GPU 하드웨어와 입력 해상도에 따라 달라지지만, DeepEthogram-fast는 256×256 해상도 비디오에 대해 초당 약 150프레임의 추론 속도를 달성합니다.
- 모델의 성능은 전체 정확도, F1 점수, AUROC(수신기 작동 특성 곡선 아래 면적)를 사용하여 평가되었습니다. 인간 레이블러 간의 일치도와 비교하여 DeepEthogram의 성능을 벤치마킹했으며 , 키포인트 기반 방법(DeepLabCut 기반 분류) 및 비지도 분류 방법(B-SoID)과도 비교했습니다


## 결과 (Results)

#### 모델링 접근 방식
저희는 영상 프레임을 입력으로 받아 각 프레임에서 관심 행동이 발생할 확률을 예측하는 것을 목표로 DeepEthogram을 개발했습니다. 이 작업은 행동이 적은 프레임에서 발생할 때의 정확도 문제, 적은 훈련 데이터 요구 사항, 동물의 위치 및 방향 불변성, 낮은 계산 자원 요구 사항이라는 과제를 안고 있습니다. 저희의 솔루션은 비디오 클립에서 모션(광학 흐름)을 추정하고, 이를 저차원 특징으로 압축한 다음, 시퀀스 모델을 사용하여 행동 확률을 추정합니다. 이 아키텍처는 딥 컨볼루션 신경망(CNN)을 사용하여 구현되었습니다.
DeepEthogram은 세 가지 버전(DeepEthogram-fast, DeepEthogram-medium, DeepEthogram-slow)으로 제공되며, 이는 속도와 정확도 사이의 절충을 반영합니다. 모든 버전은 Kinetics700 비디오 데이터셋에 사전 훈련된 흐름 생성기(TinyMotionNet, MotionNet, TinyMotionNet3D)와 특징 추출기(ResNet18, ResNet50, 3D-ResNet34)를 사용합니다. 시퀀스 모델로는 TGM 네트워크(Temporal Gaussian Mixture networks)를 활용합니다. 모델 훈련은 흐름 생성기, 특징 추출기, 시퀀스 모델 순으로 순차적으로 이루어집니다.
#### DeepEthogram을 테스트하기 위한 다양한 데이터 세트
모델 성능을 테스트하기 위해 두 종(쥐와 파리)에 걸쳐 다양한 시각적 및 행동적 과제를 제시하는 9가지 신경과학 연구 데이터셋을 사용했습니다. 데이터셋에는 Mouse-Ventral1/2 (바닥 시야), Mouse-Openfield (상단 시야), Mouse-Homecage (홈 케이지), Mouse-Social (사회적 상호작용), 그리고 Sturman et al., 2020의 EPM, FST, OFT 데이터셋, 마지막으로 Fly 데이터셋이 포함됩니다. 이 데이터셋들은 다양한 시야각, 조명, 해상도, 비디오 품질을 포함하며, 피험자가 적은 픽셀을 차지하거나 복잡한 배경, 물체 가림, 낮은 대비, 적은 훈련 예시와 같은 컴퓨터 비전 문제를 제시합니다. 모든 비디오는 다른 개별 동물에서 기록되었으므로, 모델의 일반화 능력을 평가할 수 있었습니다.

#### deepEthogram은 전문가 수준의 인간 성능에 근접하는 높은 성능을 달성합니다
DeepEthogram은 모든 데이터셋의 테스트 데이터에서 85% 이상의 전체 정확도를 달성했습니다. 전체 F1 점수와 AUROC 측정에서도 높은 성능을 보였습니다. 개별 행동에 대해서도 F1 점수가 0.7 이상이었고, 일부는 0.9 이상에 도달했습니다. 이는 무작위 수준을 크게 초과하는 성능입니다. 특히, 드물게 발생하는 행동(비디오 프레임의 10% 미만)에 대해서도 상대적으로 높은 성능을 보였습니다. DeepEthogram의 성능은 DeepEthogram-slow에서 가장 높고 DeepEthogram-fast에서 가장 낮은 경향이 있었지만, 그 차이는 일반적으로 작았습니다. 저희는 모델 성능을 전문가 인간 레이블러 간의 일치도와 비교하여 벤치마킹했습니다. DeepEthogram의 전체 정확도, F1 점수, 정밀도 및 재현율은 전문가 인간 레이블러의 수준에 근접했으며, 많은 경우 통계적으로 구별할 수 없었습니다. DeepEthogram의 성능이 가장 낮았던 행동은 인간 레이블러 간의 일치도가 낮았던 행동인 경향이 있었습니다. 이는 모델과 인간 성능 사이에 강한 상관관계가 있음을 보여줍니다.

#### DeepEthogram은 행동 발작 통계를 정확하게 예측합니다
DeepEthogram은 개별 프레임에 대한 예측을 생성하므로, 행동 발작의 횟수, 지속 시간 및 행동 간의 전환 확률과 같은 통계적 분석이 가능합니다. Mouse-Ventral1 데이터셋에 대한 분석 결과, 모델 예측과 인간 레이블 간에 행동에 소요된 시간에서 통계적으로 구별할 수 없는 강력한 일치를 보였습니다. 발작 횟수와 발작 지속 시간에서도 모델은 훈련에 사용된 인간 레이블과 통계적으로 구별할 수 없었으며, 다른 인간 레이블러와의 차이는 인간 레이블러 간의 차이 범위 내에 있었습니다.

#### 키포인트 추적 기반 기존 방법과의 비교
저희는 DeepEthogram과 키포인트 기반 행동 분류 방법(Sturman et al., 2020 방식의 사용자 정의 구현)을 Mouse-Openfield 데이터셋에서 비교했습니다. DeepLabCut을 사용하여 코, 앞발, 뒷발, 꼬리 밑동, 꼬리 끝 등 7개의 키포인트를 식별했습니다. DeepEthogram의 정확도와 F1 점수는 일반적으로 이러한 키포인트 특징을 기반으로 하는 분류기보다 우수했습니다. 발작 통계의 경우 두 방법 모두 유사하게 잘 수행되었습니다. 낮은 해상도 비디오와 신체 부위 가림은 키포인트 기반 방법의 성능에 영향을 미쳤을 수 있습니다.
또한, 비지도 행동 분류 방법인 B-SoID(Hsu and Yttri, 2019)와 DeepEthogram을 비교했습니다. B-SoID는 11개의 행동 클러스터를 식별했지만, B-SoID 클러스터와 인간 레이블 간에는 거의 일치하지 않았습니다. 인간 레이블을 B-SoID 클러스터에 사후 할당한 후에도, DeepEthogram이 대부분의 행동에서 이 대체 파이프라인보다 더 나은 성능을 보였습니다. 이는 인간이 정의한 행동의 자동 레이블링이 목표일 때 지도 학습 접근 방식이 더 나은 선택임을 시사합니다.

#### DeepEthogram은 높은 성능을 달성하는 데 적은 훈련 데이터만 필요합니다
모델이 신뢰할 수 있는 성능을 달성하기 위해 필요한 레이블링 데이터 양을 평가했습니다. DeepEthogram-fast 모델은 훈련 세트에 1개의 레이블링된 비디오만 있어도 대부분의 행동에서 높은 수준의 성능을 보였습니다. 모든 행동에 걸쳐 성능은 약 12개의 비디오로 훈련한 후 거의 점근적인 수준에 도달했습니다. 특정 행동에 대한 긍정적 예시 프레임 수 기준으로, 80개 프레임만으로도 90% 이상의 정확도를, 100개 프레임으로 95% 이상의 정확도를 달성했습니다. F1 점수는 9000개 긍정적 예시 프레임(약 5분 분량)으로 0.7에 도달했습니다. 시퀀스 모델을 사용하면 특징 추출기 단독 사용보다 모델 성능이 더 높았고, 더 적은 훈련 데이터가 필요했습니다.

#### DeepEthogram은 빠른 추론 시간을 허용합니다
DeepEthogram의 각 버전은 일반적인 실험 파이프라인에서 기능하기에 충분히 빠릅니다. 흐름 생성기 및 특징 추출기는 약 24시간 내에 훈련될 수 있으며, 대부분의 경우 한 번만 훈련하면 됩니다. 추론은 DeepEthogram-fast의 경우 256×256 해상도 비디오에서 초당 약 150프레임, DeepEthogram-medium은 초당 80프레임, DeepEthogram-slow는 초당 13프레임으로 수행됩니다. 이는 표준 30분 비디오의 경우 DeepEthogram-fast로 12분, DeepEthogram-slow로 2시간 내에 완료될 수 있음을 의미합니다. 모델 훈련과 추론에는 사용자 시간이 전혀 필요하지 않습니다.

## 토의 (Discussion)
**주요 특징 및 장점:**

- 원시 픽셀 기반 분석: 전처리가 필요 없으며, 동물의 절대적인 공간 위치와 관계없이 행동을 식별할 수 있습니다.
- 높은 성능 : 인간 레이블러와 일치하는 높은 정확도를 보이며, 회귀 행동도 정확하게 예측합니다.
- 사용자 친화적 : GUI를 통해 프로그래밍 지식 없이도 비디오 레이블링, 모델 훈련, 예측 생성이 가능
- 효율적인 시간 관리 : 초기 훈련 비디오 레이블링에만 사용자 시간이 소요되며, 이후 대규모 분석에는  거의 시간이 안듬 

```bash

```


JAABA(Kabra et al., 2013), SimBA(Nilsson, 2020), MARS(Segalin, 2020), Live Mouse Tracker(de Chaumont et al., 2019) 및 기타(Segalin, 2020; Dankert et al., 2009; Sturman et al., 2020)를 포함한 선구적인 작업은 행동의 지도 분류 목표를 향한 중요한 진전을 이루었습니다. 
이러한 방법은 동물의 신체 특정 특징을 추적하고 이러한 특징의 시계열을 사용하여 주어진 시점에 행동이 존재하는지 여부를 분류합니다.
컴퓨터 비전에서 이것은 '골격 기반 액션 감지'로 알려져 있습니다. JAABA에서는 동물의 신체 윤곽에 타원을 맞추고, 이 타원을 사용하여 행동을 분류합니다. SimBA는 사지 관절과 같은 동물의 신체 '키포인트'의 위치를 기반으로 행동을 분류합니다. MARS는 사회적 행동에 초점을 맞춘 유사한 접근 방식을 취합니다(Segalin, 2020). DeepLabCut(Mathis, 2018; Nath, 2019; Lauer, 2021) 및 유사 알고리즘(Pereira, 2018a; Graving et al., 2019)을 포함한 최근 자세 추정 방법으로 이러한 접근 방식이 더 쉬워졌습니다. 
따라서 이러한 접근 방식은 두 가지 주요 단계로 구성된 파이프라인을 활용합니다. 첫째, 연구자들은 자세 추정 소프트웨어를 사용하여 영상을 사용자 정의 관심 특징(예: 사지 위치) 세트로 줄입니다. 둘째, 이러한 자세 추정치는 관심 행동을 식별하는 분류기의 입력으로 사용됩니다. 이 접근 방식은 주어진 시점에 관심 행동이 존재하는지 여부 이상의 정보를 제공한다는 장점이 있습니다. 행동에 기여하는 동물의 신체 부위가 추적되기 때문에, 움직임과 이러한 움직임이 관심 행동에 어떻게 기여하는지에 대한 상세한 분석을 수행할 수 있습니다.
