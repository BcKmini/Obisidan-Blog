동영상을 통해 프레임별로 동물의 행동을 분석하는 접근을 시작하기 전에, 비교적 데이터 양이 풍부한 고양이와 개를 대상으로 한 행동분석 논문들이 많다는 조언을 받아 관련 선행 연구를 먼저 살펴보게 되었다. 그 과정에서 2021년 9월에 발표된 Ridi Ferdiana, Wiliam Fafar Dicka, 그리고 Boediman이 작성한 **"Cat Sounds Classification with Convolutional Neural Network"** 논문을 접하게 되었다.

이 논문은 고양이의 다양한 보컬리제이션(예: purr, meow, hiss 등)을 자동으로 분류하기 위해, 오디오 데이터를 MFCC로 변환한 후 합성곱 신경망(CNN)을 적용하는 방법을 제시한다. 풍부한 데이터셋을 기반으로 고양이의 음향적 특징을 추출하고 분석하는 방식은, 향후 동영상 프레임 기반의 동물 행동 분석 연구를 진행할 때 참고할 만한 좋은 선례가 될 것으로 판단되었다.

즉, 동영상에서 프레임을 추출하여 동물(예: 고니 또는 고라니)의 행동을 분석하기 전에, 데이터가 풍부한 반려동물(고양이, 개)을 대상으로 한 행동분석 연구들을 먼저 검토함으로써 기법과 모델 구성을 이해하고, 이를 기반으로 보다 세밀한 분석 방법을 마련하라는 조언에 따라 이 논문을 읽게 되었다.


---
## 1. Introduction

### 1.1 연구 배경 및 문제의식

- **고양이 보컬리제이션의 복잡성**  
    고양이는 감정과 행동 상태를 전달하는 다양한 소리를 생성한다. 그러나 이러한 소리는 인간의 청각으로는 미묘한 차이가 존재하며, 전통적인 수작업 분류 방법이나 기초 신호 처리 기법으로는 정확한 자동 분류에 한계가 있었다.
    
- **딥러닝의 도입 필요성**  
    최근 이미지 분류 분야에서 CNN이 탁월한 성능을 보임에 따라, 오디오 데이터를 2차원 스펙트로그램이나 MFCC(멜 주파수 켑스트럼 계수) 등으로 변환하여 CNN에 입력함으로써 고양이 소리의 특징을 효과적으로 추출하고 분류할 수 있다는 가능성이 제시되었다.
    

### 1.2 논문의 기여

- 고양이의 다양한 소리 데이터를 MFCC 변환을 통해 2차원 특성 공간으로 전환하고, 이를 이용한 CNN 기반 분류 모델을 제안.
    
- 소리 신호의 시간 및 주파수 특징을 효과적으로 추출하여 분류 정확도를 높이고, 자동화된 고양이 소리 모니터링 시스템의 가능성을 입증하는 데 기여.
    

---

## 2. Methodology

논문에서는 전체적인 데이터 수집부터 전처리, 모델 구성 및 학습, 평가까지의 과정을 상세하게 기술한다.

### 2.1 데이터 수집 및 전처리

- **데이터 소스**:  
    고양이의 여러 보컬리제이션(예, purr, meow, growl, hiss, chirp 등) 데이터셋은 공개 음원 데이터셋(예: Audio set)이나 온라인 동영상 자료에서 수집된다.
    
- **전처리 과정**:
    
    1. **노이즈 제거 및 정규화**: 원시 오디오 신호에서 배경 잡음을 줄이고, 소리의 음량을 정규화한다.
        
    2. **프레임 분할**: 오디오 신호를 일정 시간 간격(예, 약 25~50ms)으로 나눈다.
        
    3. **MFCC 변환**: 각 프레임에 대해 멜-주파수 켑스트럼 계수를 계산하여 2차원 행렬 형태(계수×프레임)로 변환한다.
        

### 2.2 MFCC Feature Extraction 및 수식 설명

- **MFCC 변환의 목적**:  
    원시 오디오 신호의 주파수 및 시간적 특징을 인간의 청각 시스템과 유사한 멜 스케일로 압축하여, CNN이 입력으로 활용하기 용이한 형태(일종의 “이미지”)로 만든다.
    
- **주요 수식 (DCT-II 사용)**:  
    MFCC 계수는 일반적으로 멜 스펙트로그램의 로그 스펙트럼에 대해 이산 코사인 변환(Discrete Cosine Transform, DCT) 타입 II를 적용하여 계산된다.  
    수식은 다음과 같이 표현할 수 있다.
    $$
X_k = \sum_{n=0}^{N-1} x_n \cos\left[\frac{\pi}{N}\left(n+\frac{1}{2}\right) k\right]
$$

    - Xn​: 멜 스케일로 변환된, 로그 스펙트럼의 n번째 값
        
    - N: 프레임 내 샘플 수
        
    - k: 출력 MFCC 계수 인덱스 (일반적으로 상위 12~40개 계수를 사용)
        
    
    **설명**: 이 수식을 통해, 오디오 신호의 주파수 분포를 압축하여 각 프레임 당 대표적인 2차원 특징 벡터를 산출한다. 이러한 특징은 CNN 입력으로서 공간적 패턴 학습에 도움을 준다.
    

### 2.3 CNN 모델 구성

- **입력 데이터**:  
    전처리 과정을 통해 얻은 MFCC 행렬(예: 40×T40 \times T40×T; 여기서 40은 MFCC 계수의 수, TTT는 프레임 수)이 CNN의 입력으로 사용된다.
    
- **모델 아키텍처**:
    
    1. **합성곱 계층 (Convolutional Layers)**:
        - 여러 개의 합성곱 계층을 사용하여 지역적 특징(예, 특정 음향 패턴)을 자동으로 추출한다.
            
        - 각 계층에서는 필터(커널)를 적용하여 입력 특징 맵과의 합성곱 연산을 수행한다.
            
    2. **풀링 계층 (Pooling Layers)**:
        
        - 공간적 차원 축소 및 노이즈 제거를 위해 최대 풀링 또는 평균 풀링 기법을 사용한다.
            
    3. **Dropout**:
        
        - 과적합을 방지하기 위해 각 계층 후 Dropout 기법을 적용한다.
            
    4. **완전연결 계층 (Fully Connected/Dense Layers)**:
        
        - 최종적으로 추출된 특징 벡터를 기반으로 다중 클래스 분류를 수행한다.
            
- **학습**:
    
    - **손실 함수**: 주로 다중 클래스 분류에 적합한 categorical cross-entropy 사용.
        
    - **최적화 알고리즘**: Adam, SGD 등으로 학습률을 조정하며 최적화.
        
    - **하이퍼파라미터**: 배치 크기, 에폭 수 등을 조정하여 성능 최적화를 진행.
        

### 2.4 실험 설정 및 평가 방법

- **데이터 분할**:  
    수집한 데이터를 훈련 세트와 검증 세트로 분할하여 모델 학습 및 평가를 진행한다.
    
- **평가지표**:  
    – 분류 정확도(accuracy)  
    – 혼동 행렬(confusion matrix)  
    – Precision, Recall, F1-score 등 세부 지표를 통해 각 클래스별 성능 분석
    

---

## 3. Results

### 3.1 분류 성능

- **주요 결과**:  
    CNN 모델은 MFCC 기반의 입력 데이터를 통해 고양이 소리를 5개 이상의 클래스(예: purr, meow, growl, hiss, chirp 등)로 효과적으로 분류하였으며, 전체 분류 정확도가 85% 이상을 달성한 것으로 보고되었다.
    
- **혼동 행렬 분석**:  
    – 클래스 간 유사성이 높은 경우(예, 고요한 purr와 낮은 볼륨의 meow)에는 일부 혼동이 발생했으나, 전체적으로 각 소리의 특징을 잘 구분해내었다.
    

### 3.2 학습 곡선 및 수렴 속도

- 모델의 학습 곡선은 빠른 수렴을 보였으며, 과적합 방지를 위해 적용한 Dropout 및 데이터 증강 기법의 효과가 확인되었다.
    

---

## 4. Discussion

### 4.1 결과 해석

- **특징 추출의 유효성**:  
    MFCC를 통한 특징 변환이 오디오 신호의 시간 및 주파수 정보를 효과적으로 요약하였고, CNN이 지역적 특징을 학습하는 데 큰 도움이 됨을 확인하였다.
    
- **CNN 모델의 강점**:  
    합성곱 및 풀링 계층을 통해 소리의 다양한 패턴(예, 음의 강약, 시간적 변화)을 자동으로 학습하여, 기존의 수작업 특징 추출 방식보다 높은 분류 성능을 보였다.
    

### 4.2 한계점 및 개선 방안

- **데이터의 양 및 다양성**:  
    연구에서 사용된 데이터셋의 규모가 상대적으로 제한적이어서, 보다 다양한 환경 및 고양이 개체의 데이터를 수집하여 일반화 성능을 높일 필요가 있다.
    
- **모델 확장**:  
    현재의 CNN 모델 외에도 RNN, Transformer 등 시계열 데이터를 효과적으로 처리할 수 있는 모델과의 결합 가능성이 논의되었다.
    
- **수식 및 파라미터 최적화**:  
    MFCC 변환에서 사용된 DCT 수식의 파라미터(예, 계수 수, 윈도우 크기 등)를 추가적으로 최적화할 여지가 있으며, 이는 모델 성능에 직접적인 영향을 미칠 수 있다.
    

---

## 5. Conclusion

### 5.1 주요 결론

- 본 연구에서는 고양이의 다양한 보컬리제이션을 분류하기 위해 MFCC 기반의 오디오 전처리와 CNN을 결합한 모델을 제안하였다.
    
- 실험 결과, 제안된 모델은 높은 분류 정확도와 효과적인 특징 추출 능력을 보여주었으며, 고양이 소리의 미세한 차이를 잘 포착할 수 있었다.
    

### 5.2 연구의 의의 및 미래 연구 방향

- **자동화된 동물 소리 분류**:  
    본 논문은 반려동물 모니터링, 수의학적 진단, 동물 행동 연구 등 다양한 분야에서 활용 가능한 자동 소리 분류 시스템의 기초를 마련하였다.
    
- **향후 개선 사항**:  
    – 데이터셋 확대 및 다양한 보컬리제이션 유형 추가  
    – MFCC 변환 및 CNN 구조에 대한 하이퍼파라미터 최적화  
    – 다중모달(multimodal) 접근(예, 소리와 영상 데이터의 융합) 연구 등
    

---

## 수식 요약 및 설명

- **MFCC 변환에 사용된 DCT-II 수식**:
$$
X_k = \sum_{n=0}^{N-1} x_n \cos\left[\frac{\pi}{N}\left(n+\frac{1}{2}\right) k\right]
$$

- xnx_nxn​ : 멜 스케일로 변환된 로그 스펙트럼의 nnn번째 샘플
	
- NNN : 프레임 내 총 샘플 수
	
- kkk : 출력 MFCC 계수 인덱스 (보통 상위 12~40개 계수를 선택)
	

**설명**: 이 수식은 각 프레임의 오디오 데이터에서 주파수 분포를 코사인 함수의 가중 합으로 표현하여, 중요한 음향 특징을 압축하는 역할을 한다. 이를 통해 CNN이 입력으로 활용할 수 있는 2차원 특징 맵이 생성된다.


---

## 종합 정리

본 논문은 고양이 보컬리제이션 분류 문제에 딥러닝 CNN 모델을 적용한 대표적인 사례로,

1. 서론에서는 고양이 소리의 복잡성과 자동 분류의 필요성을 제시하고,
    
2. 방법론에서는 데이터 전처리(노이즈 제거, 프레임 분할, MFCC 변환), CNN 모델 설계 및 학습 과정을 상세히 기술하며, MFCC 수식을 포함하여 수학적 배경을 제공한다.
    
3. 실험 결과에서는 높은 분류 정확도와 모델 성능을 입증하고,
    
4. 논의에서는 특징 추출의 유효성, 모델 한계 및 개선 방안, 그리고
    
5. 결론에서는 연구 결과의 의의 및 향후 방향을 제시한다.
    
