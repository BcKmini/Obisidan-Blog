기계학습 기말고사 - 9주차에서 15주차 내용에서 문제는 나옵니다. 우리가 다룬 내용에서 빠지는 것은 없습니다.

0은 짝수입니다 :)기계학습 기말고사 - 9주차에서 15주차 내용에서 문제는 나옵니다. 우리가 다룬 내용에서 빠지는 것은 없습니다.

---
# 코드 스니펫

## 1. 비지도 학습 (Unsupervised Learning)

### k-means 클러스터링 (Iris 데이터)
```python
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

samples, _ = load_iris(return_X_y=True)            # (150, 4) 배열 로드
model = KMeans(n_clusters=3, random_state=42)      # random_state 추가 → 재현성 확보
model.fit(samples)                                 # 중심 계산
labels = model.predict(samples)                    # 각 샘플 군집 레이블
print(labels)                                      # 예: [0, 1, 2, …]
```

### 새로운 샘플 클러스터 할당
```python
new_samples = [
    [5.7, 4.4, 1.5, 0.4],
    [6.5, 3.0, 5.5, 1.8],
    [5.8, 2.7, 5.1, 1.9]
]
new_labels = model.predict(new_samples)
print(new_labels)  # 예: [0, 2, 1]
```

### 관성(Inertia) 측정
```python
model = KMeans(n_clusters=3, random_state=42)
model.fit(samples)
print(model.inertia_)  # 관성 값 출력 → 클러스터 품질 지표
```

### 표준화 (StandardScaler)
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(samples)                         # 평균=0, 분산=1 모델 학습
samples_scaled = scaler.transform(samples)  # 이후 클러스터링 시 samples→samples_scaled 사용
```

### 파이프라인 결합 (StandardScaler + KMeans)
```python
from sklearn.pipeline import make_pipeline

scaler = StandardScaler()
kmeans = KMeans(n_clusters=3, random_state=42)
pipeline = make_pipeline(scaler, kmeans)
pipeline.fit(samples)                       # scaler→kmeans 순차 수행
labels_pipe = pipeline.predict(samples)
print(labels_pipe)
```

### 계층적 군집화 (Hierarchical Clustering)
```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

mergings = linkage(samples, method='complete')
dendrogram(
    mergings,
    labels=['샘플'+str(i) for i in range(len(samples))],  # 실제 레이블 리스트로 변경
    leaf_rotation=90,
    leaf_font_size=6
)
plt.show()

labels_hier = fcluster(mergings, 15, criterion='distance')
print(labels_hier)  # 임계 거리 15 기준 라벨
```

### t-SNE 시각화
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

model_tsne = TSNE(learning_rate=100, random_state=42)
transformed = model_tsne.fit_transform(samples)
xs, ys = transformed[:, 0], transformed[:, 1]
plt.scatter(xs, ys, c=[0,1,2]*50)  # species 리스트로 대체 필요
plt.show()
```

### NMF (Non-negative Matrix Factorization)
```python
from sklearn.decomposition import NMF

model_nmf = NMF(n_components=2, random_state=42)
nmf_features = model_nmf.fit_transform(samples)
print(model_nmf.components_)  # W 행렬(특성 × 컴포넌트)
```

### NMF 기반 추천 + 코사인 유사도
```python
from sklearn.decomposition import NMF
from sklearn.preprocessing import normalize

nmf = NMF(n_components=6, random_state=42)
nmf_features = nmf.fit_transform(articles)   # articles: 단어-빈도 배열
norm_features = normalize(nmf_features)      # 단위 벡터 정규화
current_article = norm_features[23]
similarities = norm_features.dot(current_article)
print(similarities)  # 각 문서와의 유사도 점수
```

## 2. 의사결정나무 / 앙상블 (Decision Tree & Ensemble)

### CART 분류기 (DecisionTreeClassifier)
```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(
    max_depth=3,        # 과적합 방지용 최대 깊이
    random_state=42     # 재현성 확보
)
model.fit(X_train, y_train)
preds = model.predict(X_test)
```

### 트리 구조 내보내기 (export_graphviz)
```python
from sklearn.tree import export_graphviz

export_graphviz(
    model,
    out_file='tree.dot',
    feature_names=feature_names,  # 실제 특성명 리스트로 변경
    class_names=target_names,     # 실제 클래스명 리스트로 변경
    filled=True                   # 노드 색 채우기
)
# DOT → Graphviz 도구로 시각화 필요
```

### RandomForestClassifier
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
rf.fit(X_train, y_train)
importances = rf.feature_importances_
print(importances)  # 특성 중요도 배열
```

### Bagging (BaggingClassifier)
```python
from sklearn.ensemble import BaggingClassifier

bag = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=50,
    random_state=42
)
bag.fit(X_train, y_train)
pred_bag = bag.predict(X_test)
```

### AdaBoostClassifier
```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
ada.fit(X_train, y_train)
pred_ada = ada.predict(X_test)
```

### GradientBoostingClassifier
```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb.fit(X_train, y_train)
pred_gb = gb.predict(X_test)
```

### 부분 샘플링된 GB (subsample)
```python
s_gb = GradientBoostingClassifier(
    n_estimators=100,
    subsample=0.5,      # 각 트리 학습에 데이터 절반만 사용
    random_state=42
)
s_gb.fit(X_train, y_train)
pred_sgb = s_gb.predict(X_test)
```

### 하이퍼파라미터 튜닝 (GridSearchCV)
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5, 7]
}
grid = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5
)
grid.fit(X_train, y_train)
print(grid.best_params_)  # 최적 파라미터
```

## 3. MLOps 파이프라인

### MLflow 실험 추적
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

with mlflow.start_run():
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    mlflow.log_param("n_estimators", model.n_estimators)
    rmse = mean_squared_error(y_test, model.predict(X_test), squared=False)
    mlflow.log_metric("rmse", rmse)
    mlflow.sklearn.log_model(model, "model")
```

### Dockerfile 예제
```dockerfile
FROM python:3.8-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
```

### Airflow DAG
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def train_model():
    # 데이터 로드→전처리→모델 학습
    pass

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'ml_pipeline',
    default_args=default_args,
    schedule_interval='@daily'
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag
)
```

### Kubeflow Pipeline
```python
import kfp
from kfp.components import create_component_from_func

def preprocess_op():
    # 전처리 단계
    pass

def train_op():
    # 학습 단계
    pass

@kfp.dsl.pipeline(
    name='ML Training Pipeline',
    description='샘플 MLOps 파이프라인'
)
def ml_pipeline():
    p1 = preprocess_op()
    p2 = train_op().after(p1)

if __name__ == '__main__':
    kfp.compiler.Compiler().compile(ml_pipeline, 'ml_pipeline.yaml')
```

### GitHub Actions CI/CD
```yaml
name: CI

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'
      - name: Install deps
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build Docker Image
        run: docker build -t myregistry/ml-model:latest .
      - name: Push to Registry
        run: docker push myregistry/ml-model:latest
```

## 4. 기타 실습 예제

### K-Nearest Neighbors (KNN)
```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)  # 최적 k값 검증 필요
knn.fit(X_train, y_train)
preds_knn = knn.predict(X_test)
```

### Support Vector Machine (SVM)
```python
from sklearn.svm import SVC

svc = SVC(kernel='rbf', C=1.0, random_state=42)
svc.fit(X_train, y_train)
preds_svc = svc.predict(X_test)
```
