### **English Presentation Script (Revised)**

**(Slide 1: Title)**

Hello, I'm Kim Kyung-min from Konkuk University's Glocal Campus. Today's presentation topic is the core of multi-classification, and we will briefly look at how to classify it in multiple classes


## 2
"So, what problem are we solving? In multi-class classification, our main goal is to teach a model to learn **decision boundaries** 
"The **GIF animation** on the left   the problem we're solving. You can see the process of a model learning and dynamically adjusting the **decision boundaries** to best separate the different groups of data.

And theimg on the right shows _how_ this learning happens. It's the mechanism—from raw scores to a final loss value"


**(Slide 2: Softmax → Probability)**

"First, we need to turn the model's raw scores into probabilities. We use the **Softmax function** for this.

Looking at the formula, Pi​=∑j​ezj​ezi​​, it might seem complex, but it's quite simple.

- The top part, the **numerator (**ezi​**)**, takes the score for each class and makes it a positive number. A higher score results in a much bigger positive number.
    
- The bottom part, the **denominator (**∑j​ezj​**)**, is just the sum of all these positive numbers from every class.
    
So, when we pass our example logits—2.0, 1.0, and 0.1—**through this Softmax function**, they are transformed into clear probabilities: approximately 66%, 24%, and 10%. This now tells us exactly how confident the model is for each class.

**(Slide 3: Probability → Cross-Entropy)**

"Next, how do we measure if the model is wrong? We use **Cross-Entropy**.

The formula is Loss=−log(py​). Let's break it down:

- py​ is the probability the model predicted for the **correct answer**.
    
- The **log** function is a special calculator. When the probability for the correct answer (py​) is high , the log value is very small. When the probability is low (close to 0), the log value becomes a large negative number.
    
- We add a **minus sign** in front to make that large negative number positive.
    

So, if the model is confident and correct, the loss is small. If the model is not confident or wrong, the loss becomes very large. This large loss score tells the model it made a big mistake and needs a major correction."

**(Slide 4: Why This Combination is the Standard)**

"So, why do we always use Softmax with Cross-Entropy?

First, it's based on proven math. It turns scores into real probabilities, which is statistically sound.

Second, it's very practical. This combination makes the math for training the model simple and fast. This helps the model learn steadily without getting stuck.

This basic knowledge is crucial for the Physical AI systems I want to build at DGIST.

Thank you."

### **한국어 발표 스크립트 (수정본)**

**(슬라이드 1: 제목)**

"안녕하십니까, 교수님. 오늘 'Multi-class classification의 핵심'에 대한 발표를 맡게 된 김경민입니다.

제가 이 주제를 선택한 이유는, 기본에 대한 이해가 DGIST에서 제가 하고 싶은 연구의 핵심이기 때문입니다. 저는 Vision, Language, Action을 결합하는 'Physical AI'를 연구하고 싶습니다. 이 모든 복잡한 기술들은 '분류'라는 간단한 핵심 아이디어에 기반합니다. 이번 발표를 통해 제가 앞으로의 연구를 위해 얼마나 튼튼한 기초를 가지고 있는지 보여드리겠습니다."

**(슬라이드 2: Softmax → 확률)**

"먼저, 모델의 점수를 '확률'로 바꿔야 합니다. 이때 **Softmax 함수**를 사용합니다.

수식 $P_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$를 보시면 복잡해 보이지만 아주 간단합니다.

- 위에 있는 **분자(ezi​)**는 각 클래스의 점수를 일단 전부 양수로 만들어주는 역할을 합니다. 점수가 클수록 훨씬 더 큰 양수가 됩니다.
    
- 아래에 있는 **분모(∑j​ezj​)**는 이렇게 만들어진 모든 클래스의 양수 값들을 전부 더한 총합입니다.
    

결국 분자를 분모로 나눈다는 것은 '전체 점수 총합 중에서 해당 클래스의 점수가 차지하는 비율'을 계산하는 것과 같습니다. 이렇게 점수 2.0, 1.0, 0.1이 각각 66%, 24%, 10% 같은 명확한 확률이 되는 것입니다. 이제 모델이 각 클래스를 얼마나 확신하는지 알 수 있습니다."

**(슬라이드 3: Probability → Cross-Entropy)**

"다음으로, 모델이 얼마나 틀렸는지 어떻게 측정할까요? **Cross-Entropy**를 사용합니다.

수식은 Loss=−log(py​) 입니다. 이걸 쉽게 풀어보면 이렇습니다.

- **py​**는 모델이 '**실제 정답**'에 대해 예측한 확률입니다.
    
- **log**는 특별한 계산기인데, 정답 확률(py​)이 1에 가까워 높을수록 아주 작은 값을 내뱉고, 0에 가까워 낮을수록 아주 큰 마이너스 값을 내뱉습니다.
    
- 앞에 붙은 **마이너스 기호**는 이 큰 마이너스 값을 다시 플러스로 바꿔주는 역할을 합니다.
    

그래서 모델이 정답을 높은 확률로 맞추면 손실(Loss)이 아주 작아지고, 정답을 낮은 확률로 예측하면 손실이 엄청나게 커지는 것 입니다. 이 큰 손실 점수는 모델에게 '너 크게 틀렸으니 많이 수정해야 해' 라고 알려주는 신호가 됩니다."

**(슬라이드 4: 왜 이 조합이 표준인가?)**

"그렇다면 왜 항상 Softmax와 Cross-Entropy를 함께 사용할까요?

첫째, 검증된 수학에 기반하기 때문입니다. 점수를 통계적으로 의미 있는 실제 확률로 바꿔줍니다.

둘째, 매우 실용적입니다. 이 조합은 모델을 학습시키는 계산을 간단하고 빠르게 만들어 줍니다. 그래서 모델이 중간에 멈추지 않고 꾸준히 똑똑해질 수 있도록 돕습니다.

이러한 기본 지식은 제가 DGIST에서 만들고 싶은 Physical AI 시스템에 필수적입니다.

감사합니다."