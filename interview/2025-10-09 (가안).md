### **PPT 슬라이드 구성 (표지 제외, 내용 4장)**

#### **[Page 1] 문제 정의: 왜 점수를 손실로 바꿔야 할까?**

- **제목:** 문제 정의: 왜 점수를 손실(Loss)로 바꿔야 할까?
    
- **핵심 메시지:** 모델이 내놓은 ‘점수’는 학습을 위한 ‘지도’가 될 수 없습니다.
    
- **내용 구성:**
    
    1. **목표:** 사진(x)을 보고 정답(y)을 맞추는 분류기 만들기
        
    2. **현실:** 모델은 각 클래스에 대한 **점수(Logit)**만 출력
        
    3. **해결 흐름:** 이 점수를 학습 가능한 신호, 즉 **손실(Loss)** 값으로 변환해야 함
        
        - **점수(z) → [Softmax] → 확률(p) → [Cross-Entropy] → 손실(L)**
            
- **시각화 아이디어:**
    
    - 중앙에 위 '해결 흐름'을 명확한 순서도(Flowchart)로 배치하여 발표의 전체 로드맵을 제시합니다.
        

> **(발표 멘트)** “분류 문제에서 모델은 처음엔 각 클래스에 대한 원시 점수만 출력합니다. 이 점수만으로는 학습 방향을 잡기 어렵습니다. 따라서 저희는 소프트맥스와 크로스엔트로피라는 두 단계를 거쳐 이 점수를 학습 가능한 하나의 숫자, 즉 손실 값으로 바꾸는 여정을 살펴볼 것입니다.”

---

#### **[Page 2] Softmax: 점수를 경쟁적인 확률로 변환**

- **제목:** Step 1. Softmax: 점수를 ‘경쟁적인 확률’로 변환하기
    
- **핵심 메시지:** 지수 함수로 1등을 확실히 밀어주고, 전체 합으로 나눠 공정하게 만듭니다.
    
- **내용 구성:**
    
    1. **정의 (Definition):**
        
        - 수식을 중앙에 크게 배치:
            
            pi​=∑j​ezj​ezi​​
            
    2. **주요 특징 (Key Features):**
        
        - **확률적 해석:** 모든 값의 합은 1이며, 0과 1 사이의 값을 가짐
            
        - **차이 극대화:** 가장 큰 점수에 확률이 크게 쏠리도록 설계
            
        - **실무 팁:** `max(z)`를 빼주는 트릭으로 계산 안정성 확보
            
- **시각화 아이디어:**
    
    - 왼쪽에는 입력 점수 `[2.0, 1.0, 0.1]` 막대그래프, 오른쪽에는 소프트맥스를 통과한 확률 `[0.66, 0.24, 0.10]` 막대그래프를 나란히 배치하여 ‘쏠림’ 효과를 시각적으로 보여줍니다.
        

> **(발표 멘트)** “첫 단계는 소프트맥스입니다. 이 함수는 지수 연산을 통해 점수들 간의 차이를 극대화한 뒤, 전체 합으로 나누어 총합이 1인 확률 분포로 만들어 줍니다. 보시는 것처럼, 가장 높았던 2.0 점수에 확률이 66%까지 쏠리는 것을 확인할 수 있습니다.”

---

#### **[Page 3] Cross-Entropy: 예측과 정답의 거리를 측정**

- **제목:** Step 2. Cross-Entropy: 예측 확률에 ‘벌점’ 부여하기
    
- **핵심 메시지:** 정답 클래스의 확률이 낮을수록 강력한 페널티를 부과합니다.
    
- **내용 구성:**
    
    1. **정의 (Definition):**
        
        - 수식을 중앙에 크게 배치:
            
            L=−log(py​)
            
        - (py​ = 모델이 정답 클래스에 부여한 확률)
            
    2. **핵심 직관 (Gradient):**
        
        - **학습 신호:**
            
            ∂zi​∂L​=pi​−yi​
            
        - **의미:** (나의 예측 - 실제 정답) → 이처럼 간단하고 명확한 신호가 학습을 이끕니다.
            
- **시각화 아이디어:**
    
    - `-log(x)` 그래프를 크게 그리고, x축을 ‘정답 확률’, y축을 ‘손실(벌점)’로 명시합니다.
        
    - **확률이 1에 가까울 때** 손실이 0에 수렴하는 지점과, **확률이 0에 가까울 때** 손실이 무한대로 치솟는 지점을 강조하여 보여줍니다.
        

> **(발표 멘트)** “이제 확률이 나왔으니 정답과 비교해 벌점을 매길 차례입니다. 크로스엔트로피는 정답 클래스의 확률에 마이너스 로그를 취합니다. 그래프처럼 정답 확률이 1에 가까우면 벌점이 0이지만, 0에 가까워질수록 벌점이 기하급수적으로 커집니다. 특히 이 방식은 미분했을 때 ‘예측-정답’이라는 매우 깔끔한 학습 신호를 만들어냅니다.”

---

#### **[Page 4] 예시로 확인하는 최종 정리 및 실전 팁**

- **제목:** 최종 정리: 예시로 확인하고 실전 팁까지
    
- **핵심 메시지:** CE는 확률적 타당성과 안정적 학습을 모두 잡은 분류 문제의 표준입니다.
    
- **내용 구성 (상하 2단 구성):**
    
    - **[상단] 예시로 확인하기**
        
        - **Probabilities:** `[0.659, 0.242, 0.099]`
            
        - **정답이 0번일 때 Loss:** -log(0.659) ≈ **0.42** (정답을 잘 맞춤 → 낮은 벌점)
            
        - **정답이 2번일 때 Loss:** -log(0.099) ≈ **2.32** (완전히 틀림 → 높은 벌점)
            
    - **[하단] 핵심 요약 및 실전 팁**
        
        - **왜 CE인가?**
            
            - **확률적 타당성:** 최대우도추정(MLE)과 동일하여 해석이 명확함
                
            - **안정적 학습:** `p-y` 형태의 깨끗한 그라디언트로 빠른 수렴 유도
                
        - **실전 팁:**
            
            - **Label Smoothing:** 모델의 과신 방지
                
            - **Class Weights:** 데이터 불균형 문제 대처
                

> **(발표 멘트)** “앞선 예시의 확률값으로 실제 손실을 계산해보면, 정답일 확률이 0.66일 땐 손실이 0.42에 불과하지만, 0.1로 낮아지자 손실이 2.32까지 치솟는 것을 볼 수 있습니다. 이처럼 크로스엔트로피는 통계적으로 타당하고 안정적인 학습 신호를 제공하기에 분류 문제의 표준으로 사용됩니다. 실전에서는 데이터 불균형이나 모델의 과신을 막기 위해 클래스 가중치나 라벨 스무딩 같은 기법을 함께 활용하기도 합니다. 이상으로 발표를 마치겠습니다. 감사합니다.”