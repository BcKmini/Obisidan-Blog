### A안. Convolution & Receptive Field (CNN 기본 개념)

- **S1 Title**: _“What Is Convolution? From Local Patterns to Hierarchies”_  
    키워드: task(vision), need(local pattern), solution(convolution)
    
- **S2 개념**: 합성곱 연산(커널, stride, padding), 파라미터 공유/희소연결  
    영어포인트: “Sparse connections and weight sharing reduce overfitting and computation.”
    
- **S3 수용영역(Receptive Field)**: 층이 깊어질수록 글로벌 문맥 포착 예시(엣지→텍스처→오브젝트)  
    영어포인트: “Deeper layers aggregate context: edges → motifs → parts → objects.”
    
- **S4 예시**: 간단한 1D/2D 예제(커널 3×3, stride 2 계산 표), 파라미터 수 비교(FC vs Conv)  
    영어포인트: “A 3×3 kernel uses 9 weights per channel, independent of input size.”
    
- **S5 한계·확장**: dilation, depthwise separable, attention과의 비교(언제 Conv? 언제 Attention?)  
    영어포인트: “Conv excels at locality and efficiency; attention shines for long-range dependencies.”

```


[발표 키 메시지(암기 3줄)]
- 합성곱은 작은 커널을 슬라이딩하며 가중치를 **공유**해, 이미지의 **국소 패턴**을 효율적으로 추출한다.
- 층을 쌓거나 스트라이드/딜레이션을 쓰면 **수용영역(RF)**이 넓어져 더 큰 문맥을 본다.
- Conv는 **희소 연결 + 파라미터 공유**로 연산/메모리 효율이 뛰어나고, 이미지 구조(공간적 인접성)에 잘 맞는다.

────────────────────────────────────────────────────────

[1] 직관과 연산(완전 기본기)
- 직관: 이미지는 모서리·선·텍스처 같은 **지역 패턴**의 조합. 합성곱은 작은 창(K×K)으로 그 패턴을 훑는다 → **희소 연결** + **가중치 공유**.
- 수식(한 줄):  y[i,j,c_out] = Σ_{u,v,c_in} W[u,v,c_in,c_out]·x[i+u,j+v,c_in] + b[c_out]
- 출력 크기: H_out = ⌊(H + 2P − K)/S⌋ + 1,  W_out 동일.
- 파라미터 수: K×K×C_in×C_out (+ bias C_out). 3×3, Cin=64, Cout=32 → 약 18K.

[2] 하이퍼파라미터 4종(면접 단골)
- K(커널): 3×3이 표준, 5×5는 3×3 두 번으로 대체 가능(파라미터·FLOPs 절약).
- S(스트라이드): S↑ → 공간 크기 감소(다운샘플링).
- P(패딩): SAME(크기 보전) / VALID(축소).
- Cout(필터 수): 표현력↑ ↔ 비용↑ (메모·연산 주의).

[3] 이동 ‘등가성’ vs ‘불변성’
- 합성곱은 **이동 등가성**(입력이 이동하면 특징도 같이 이동)을 갖는다. **불변성**은 풀링/데이터증강/아키텍처로 강화한다.

[4] 수용영역(Receptive Field, RF)
- 정의: 특정 출력 위치가 입력의 어느 범위를 보는가.
- 단순 누적(Stride=1): RF_L = RF_{L-1} + (K−1), 시작 RF_1=K → 예: 3×3을 3층: 3→5→7.
- Stride/Dilation 포함: RF는 (K_eff−1)에 이전 층들의 stride 곱이 누적되어 커진다. Dilation D면 K_eff = K + (K−1)(D−1).  (딜레이션은 해상도 유지한 채 RF만 키운다). 〔딜레이션을 이용해 해상도 손실 없이 문맥을 늘려 세그멘테이션 성능을 올린 고전 연구 참고. :contentReference[oaicite:0]{index=0}

[5] Conv가 FC보다 유리한 이유(숫자 감각)
- 같은 입력을 전결합(FC)로 연결하면 파라미터가 입력 해상도에 선형 이상으로 폭증. Conv는 **공간 독립적 파라미터(커널)**로 입력 크기에 덜 민감.
- 효율성 덕분에 대규모 이미지(예: ImageNet)에서도 실용적 성능 달성(AlexNet이 전환점). :contentReference[oaicite:1]{index=1}

────────────────────────────────────────────────────────

[6] 발전사(아주 핵심만 연표식)
- LeNet-5(1998): 문자 인식에 CNN 적용. 현대 CNN의 시조격. :contentReference[oaicite:2]{index=2}
- AlexNet(2012): ReLU, 드롭아웃, GPU 학습로 ImageNet 혁신(에러율 급감). Conv 르네상스 촉발. :contentReference[oaicite:3]{index=3}
- VGG(2014): **3×3 소커널 깊게 쌓기**로 성능상승, 구조적 단순성. :contentReference[oaicite:4]{index=4}
- ResNet(2015): **잔차 연결**로 초심층 학습 안정화(ResNet-152, ILSVRC’15 우승). :contentReference[oaicite:5]{index=5}
- Dilated Conv(2015): 해상도 유지하며 RF 확장 → 세그멘테이션 등 **dense prediction**에 유리. :contentReference[oaicite:6]{index=6}
- Xception/MobileNet(2017): **Depthwise Separable**로 경량·모바일 환경 최적화. :contentReference[oaicite:7]{index=7}
- ViT(2020): Conv 없이 **순수 Transformer**로 대규모 데이터에서 SOTA. Conv/Attention 비교의 기준점. :contentReference[oaicite:8]{index=8}
- ConvNeXt(2022): 현대 레시피로 **순수 Conv**를 업데이트, ViT급 성능 회복. :contentReference[oaicite:9]{index=9}
- RepLKNet(2022): **대형 커널(최대 31×31)** 설계로 전역 문맥을 Conv로 흡수. :contentReference[oaicite:10]{index=10}
- ConvNeXt V2(2023): **FCMAE(컨볼루션형 MAE)** + **GRN**으로 Conv를 다시 강화. :contentReference[oaicite:11]{index=11}

[암기형 한 줄 요약]
- “VGG는 **작은 커널을 깊게**, ResNet은 **잔차**, MobileNet/Xception은 **채널별-점별 분해**, Dilated는 **해상도 유지 RF↑**, ConvNeXt/RepLKNet은 **현대화·대형커널**.”

────────────────────────────────────────────────────────

[7] 최근 동향(2022–2025 포인트 업)
1) **Conv의 현대화 & 재부상**  
   - ConvNeXt: 스테이지 비율, 패치화 스템, 활성/정규화 교체 등 ‘Transformer식 레시피’를 Conv에 이식 → COCO/ ADE20K 등에서 Swin과 경쟁. :contentReference[oaicite:12]{index=12}
   - ConvNeXt V2: Conv 전용 **마스킹 오토인코더(FCMAE)** + **GRN 레이어**로 자기지도 사전학습 효과 극대화. 효율형(Atto)부터 Huge까지 폭넓게 개선. :contentReference[oaicite:13]{index=13}
2) **대형 커널 트렌드**  
   - 소커널 누적 대신 **직접 큰 커널**로 장거리 문맥을 포착(RepLKNet). ViT의 전역 수용력에 일부 근접하면서 지연(latency) 이점. :contentReference[oaicite:14]{index=14}
3) **하이브리드 CNN–ViT**  
   - CoAtNet 등: **Conv(일반화·수렴 안정)** + **Attention(용량·전역성)**의 장점을 수직 스택으로 결합, 데이터 크기 전 구간에서 우수. 최근 서베이도 시너지 정리. :contentReference[oaicite:15]{index=15}
4) **Dense Prediction에서의 딜레이션 재평가**  
   - 해상도 유지 문맥 확장이 여전히 유효(세그멘테이션/디텍션 백본). 현대 레시피와 병행되는 구성요소로 자리. :contentReference[oaicite:16]{index=16}

[면접에서 “요즘 뭐가 핫?”에 대한 15초 답]
- “최근은 **Conv 현대화(ConvNeXt V2)**와 **대형 커널(RepLKNet)**, 그리고 **CNN–ViT 하이브리드(CoAtNet)**가 핵심 축입니다. 작업 성격과 데이터 규모에 맞춰 **지역 효율(Conv)**과 **전역 상호작용(Attention)**을 적절히 섞는 게 트렌드예요.”  :contentReference[oaicite:17]{index=17}

────────────────────────────────────────────────────────

[8] 발표용 5장 구조(문장 2개 규칙, 한국어 버전)
S1 제목: “Convolution: 지역 패턴에서 계층 표현까지”
  - “작은 커널 + 가중치 공유로 지역 패턴을 효율적으로 추출합니다.”
  - “층을 쌓아 수용영역을 넓히며 문맥을 포착합니다.”

S2 연산과 하이퍼파라미터
  - “수식 한 줄로: ‘주변 K×K의 가중합 + 편향’.”
  - “K,S,P,Cout이 출력 크기·연산량을 좌우합니다.”

S3 수용영역(RF)과 직관
  - “3×3을 3층: 3→5→7. 딜레이션은 해상도 유지하며 RF만 확대.”
  - “전역 문맥을 단계적으로 취합합니다.”  :contentReference[oaicite:18]{index=18}

S4 왜 Conv인가(수치 1개)
  - “예: 64→32, 3×3 Conv 파라미터 ≈ 18K(FC는 수백만).”
  - “효율성과 일반화에 유리합니다.”  (AlexNet 전환점 참고) :contentReference[oaicite:19]{index=19}

S5 확장과 한계
  - “Depthwise separable/Group/1×1/Dilated/Residual/ConvNeXt/대형커널.”
  - “장거리 의존성은 Attention·하이브리드가 적합.”  :contentReference[oaicite:20]{index=20}

────────────────────────────────────────────────────────

[9] 예상 질문 10문항(암기형 키워드 답안)
1) Weight sharing의 핵심 이점? → “**파라미터 감소** + **이동 등가성** 유지.”
2) Stride/패딩이 출력 크기에 미치는 영향? → “S↑ 축소, P로 경계 보존.”
3) 3×3 Conv 파라미터 공식? → “Cin×3×3×Cout(+bias).”
4) Conv가 이미지에 적합한 이유? → “**지역성·희소성·공유**가 이미지 통계와 정합.”
5) RF는 어떻게 커지나? → “(K−1) 누적, Stride/Dilation 곱 영향.”
6) Dilation vs Pooling? → “Dilation은 **해상도 유지 RF↑**, Pooling은 **다운샘플링**.”
7) 언제 Attention/하이브리드가 유리? → “**전역 상호작용/장거리 의존**이 지배적일 때.” :contentReference[oaicite:21]{index=21}
8) Depthwise Separable의 장점? → “FLOPs·파라미터 **대폭 감소**.” :contentReference[oaicite:22]{index=22}
9) 대형 커널이 왜 각광? → “소커널 누적 대신 **직접 전역 문맥** 확보, 지연 이점.” :contentReference[oaicite:23]{index=23}
10) ConvNeXt V2의 핵심? → “**FCMAE + GRN**으로 Conv의 자기지도·표현력 강화.” :contentReference[oaicite:24]{index=24}

────────────────────────────────────────────────────────

[10] 짧은 퀴즈(연습용)
Q1) 입력 128×128, K=3, S=2, P=1 → H_out?  
A) ⌊(128+2−3)/2⌋+1 = ⌊127/2⌋+1 = 63+1 = **64**

Q2) 3×3(stride 1) 두 층 뒤에 3×3(stride 2) 한 층이면 마지막 RF?  
- 앞 두 층: 3→5→7, 마지막 층은 (K−1)=2에 이전 stride 곱(=1×1)과 현재 stride(2)가 반영되어 대략 **11** 근방(직관식). (정밀 계산 시 경계/패딩 설정에 따라 ±1 오차 가능)

────────────────────────────────────────────────────────

[11] 최신 면접용 ‘짧은 주제 4개’(5문장 토크로 확장 가능)
A. “**ConvNeXt V2가 왜 다시 Conv를 세운 걸까?**”  
   - FCMAE(Conv형 MAE)로 자기지도 사전학습을 Conv에 맞게 재설계, GRN으로 채널 경쟁 강화 → ImageNet/COCO/ADE20K에서 성능↑.  
   - 메시지: “**레시피와 사전학습 설계를 맞추면 Conv도 ViT급으로 간다**.” :contentReference[oaicite:25]{index=25}

B. “**대형 커널 CNN은 Attention을 대체할까?**”  
   - RepLKNet은 31×31 같은 큰 커널로 전역 문맥을 직접 포착, Swin 대비 경쟁적 성능·낮은 지연 보고.  
   - 메시지: “**전역성을 커널로 흡수하는 새로운 축**.” :contentReference[oaicite:26]{index=26}

C. “**하이브리드 CNN–ViT, 언제 이득?**”  
   - Conv(강한 귀납편향, 데이터작을수록 유리) + Attention(용량·전역성, 대규모 데이터 유리) → CoAtNet처럼 수직 스택이 효과적.  
   - 메시지: “**데이터 규모/과제 특성에 따라 혼합이 상한을 끌어올림**.” :contentReference[oaicite:27]{index=27}

D. “**딜레이션이 세그멘테이션에서 여전히 쓰이는 이유**”  
   - 해상도 손실 없이 RF만 확장 → 경계 보존과 문맥 통합을 동시에.  
   - 메시지: “**Dense prediction에서 해상도와 문맥을 동시에 잡는 고전이자 현역**.” :contentReference[oaicite:28]{index=28}

────────────────────────────────────────────────────────

[12] 한-영 전환용 2문장(암기)
- KR: “합성곱은 작은 커널을 공유해 지역 패턴을 효율적으로 추출하고, 층을 쌓아 수용영역을 넓혀 더 큰 문맥을 포착합니다.”
- EN: “Convolution uses small shared kernels to extract local patterns efficiently, and stacking layers widens the receptive field to capture broader context.”

────────────────────────────────────────────────────────

[참고문헌(핵심 원전)]
- LeNet-5(1998) 〔LeCun et al.〕 :contentReference[oaicite:29]{index=29}
- AlexNet(2012) 〔Krizhevsky et al., NeurIPS〕 :contentReference[oaicite:30]{index=30}
- VGG(2014) 〔Simonyan & Zisserman〕 :contentReference[oaicite:31]{index=31}
- ResNet(2015) 〔He et al.〕 :contentReference[oaicite:32]{index=32}
- Dilated Conv(2015) 〔Yu & Koltun〕 :contentReference[oaicite:33]{index=33}
- Xception(2017) / MobileNetV1(2017) 〔Chollet / Howard et al.〕 :contentReference[oaicite:34]{index=34}
- ViT(2020) 〔Dosovitskiy et al.〕 :contentReference[oaicite:35]{index=35}
- ConvNeXt(2022) 〔Liu et al.〕 :contentReference[oaicite:36]{index=36}
- RepLKNet(2022) 〔Ding et al., CVPR〕 :contentReference[oaicite:37]{index=37}
- ConvNeXt V2(2023) 〔Woo et al., CVPR〕 :contentReference[oaicite:38]{index=38}
-cS
```


